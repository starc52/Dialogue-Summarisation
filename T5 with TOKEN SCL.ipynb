{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPemym8VXSSVWeTFlg8Eiy+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"20f7eb82ac15406d8f79616709d113ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_35281e39556142a9bf1e970500c416e9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4441745f41b9433abee786db9173662c","IPY_MODEL_6eadf212ef7742cdb0be5e80f145e663","IPY_MODEL_88b56bea54074d7785707e16dcbdd449"]}},"35281e39556142a9bf1e970500c416e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4441745f41b9433abee786db9173662c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_19a597704b2e44b387891cdaced9cbc7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a17ec66ddd1345f18592b7e68108a36e"}},"6eadf212ef7742cdb0be5e80f145e663":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f872be6117074bc8aba2aad4b3dd3ca8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e207dadddb7e49fb8970745ccb0836e5"}},"88b56bea54074d7785707e16dcbdd449":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_71c786eede4e4aed84b1a2369a947848","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/3 [00:00&lt;00:00, 94.16it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_471a1ad39fce49d2849954ee96b6c347"}},"19a597704b2e44b387891cdaced9cbc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a17ec66ddd1345f18592b7e68108a36e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f872be6117074bc8aba2aad4b3dd3ca8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e207dadddb7e49fb8970745ccb0836e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"71c786eede4e4aed84b1a2369a947848":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"471a1ad39fce49d2849954ee96b6c347":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d3e7b5e803d4e83b6fe910075c8b83b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c19f165d22154d17baab3037d4b02d01","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_46dc815d6df84a3381b95f9cf1d9528b","IPY_MODEL_be3fee3a3e8e48fb8c4e2af8b7c7146f","IPY_MODEL_1b670cd6cb934ee6bc8d39630c100034"]}},"c19f165d22154d17baab3037d4b02d01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46dc815d6df84a3381b95f9cf1d9528b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73c055a964ac42fe963543e2f1461027","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8f1119acba54c42bdecb627202c3d51"}},"be3fee3a3e8e48fb8c4e2af8b7c7146f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f78d3a55930e4b40a0eb33c739af6bfc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1199,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1199,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_63afa40361ce4ee1b6b25af1d1a51c12"}},"1b670cd6cb934ee6bc8d39630c100034":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d974dc174ae84d9abec1d6166c5be9a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20k/1.20k [00:00&lt;00:00, 38.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dbcbda5171a2448f8b66dfa7e1a4d38d"}},"73c055a964ac42fe963543e2f1461027":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b8f1119acba54c42bdecb627202c3d51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f78d3a55930e4b40a0eb33c739af6bfc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"63afa40361ce4ee1b6b25af1d1a51c12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d974dc174ae84d9abec1d6166c5be9a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dbcbda5171a2448f8b66dfa7e1a4d38d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"95d3a2f8dfe843759c8c58b9d660dd74":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e0db229cc23f41d1a1bff64a0e77c467","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3accf7929b3d43f4ae03854e52a4b882","IPY_MODEL_4410587f7c56447d9e9634c4fc62794f","IPY_MODEL_7eb53687304f40d89c1d9c9a361d85a9"]}},"e0db229cc23f41d1a1bff64a0e77c467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3accf7929b3d43f4ae03854e52a4b882":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5b2ac1da083d4f52a3eb3841f8b6db58","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fc242c6eb5324868966dacd028c28b92"}},"4410587f7c56447d9e9634c4fc62794f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f293916b50e04d1e88fa329534764ec3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0d2dac5392b2413cba7c5429159dce49"}},"7eb53687304f40d89c1d9c9a361d85a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b9ba84662764b91a7a647b8b5b1ff96","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 792k/792k [00:00&lt;00:00, 1.63MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bb63da76d68a46cfa8f38d2f919c11b2"}},"5b2ac1da083d4f52a3eb3841f8b6db58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fc242c6eb5324868966dacd028c28b92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f293916b50e04d1e88fa329534764ec3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0d2dac5392b2413cba7c5429159dce49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b9ba84662764b91a7a647b8b5b1ff96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bb63da76d68a46cfa8f38d2f919c11b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59df14d0fd1e4099a03aca7152463495":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4cc8a06f045e44afa06ebe24dcc28ca0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b95252f8dd1d44129e6c846ba00157a5","IPY_MODEL_055f67b4f1484a2282672842aeee9c71","IPY_MODEL_36d7caae02634e5dbe95e6951ab018d6"]}},"4cc8a06f045e44afa06ebe24dcc28ca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b95252f8dd1d44129e6c846ba00157a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_65cfd9cb59454b55a6a7840e23d9b061","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ede9b5ccb5c7442985cbd43f69c9290a"}},"055f67b4f1484a2282672842aeee9c71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6222c4d589e84bfaa269f7a65d697228","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1389353,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1389353,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4056b4b8235d46a99825bde6a0b45a67"}},"36d7caae02634e5dbe95e6951ab018d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9226ce3d264e415e9d75b340a56d712d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.39M/1.39M [00:00&lt;00:00, 1.69MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc86722c8d4e4bd7958bd068ef71d1b9"}},"65cfd9cb59454b55a6a7840e23d9b061":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ede9b5ccb5c7442985cbd43f69c9290a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6222c4d589e84bfaa269f7a65d697228":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4056b4b8235d46a99825bde6a0b45a67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9226ce3d264e415e9d75b340a56d712d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dc86722c8d4e4bd7958bd068ef71d1b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c174d87f42a748dab753397400e23b55":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3cce4a6783fe46d18f6c05531381b89f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8377dbb8fe4e4d1fb3abeeec74028ca9","IPY_MODEL_bcc4574c93f54258be34d38dc7b15abf","IPY_MODEL_2c0ac6769d85469bb1115ef3c147b439"]}},"3cce4a6783fe46d18f6c05531381b89f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8377dbb8fe4e4d1fb3abeeec74028ca9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_af943730f96c4c1ab9ecea9c130d0bb5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  0%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6617ddada5ab499baebe87c2c0af8e05"}},"bcc4574c93f54258be34d38dc7b15abf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5706a0ab78d74893aa6ebdcc85ee7969","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":15,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0218ef503cb64b1789f06cf50d3b5235"}},"2c0ac6769d85469bb1115ef3c147b439":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_49e4d5eb0be049239e1144823566f134","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/15 [00:00&lt;?, ?ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d49fd45101ff46d3a9beff9a63405632"}},"af943730f96c4c1ab9ecea9c130d0bb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6617ddada5ab499baebe87c2c0af8e05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5706a0ab78d74893aa6ebdcc85ee7969":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0218ef503cb64b1789f06cf50d3b5235":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49e4d5eb0be049239e1144823566f134":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d49fd45101ff46d3a9beff9a63405632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"gMzbM4naD2g9","executionInfo":{"status":"ok","timestamp":1670087171358,"user_tz":300,"elapsed":291,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install wandb\n","!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHu9oO2Vqiud","executionInfo":{"status":"ok","timestamp":1670087235900,"user_tz":300,"elapsed":62967,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"outputId":"00b5816a-82bb-41e1-cf55-7d54365e975c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s \n","\u001b[?25hCollecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 72.0 MB/s \n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 93.1 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 87.2 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 57.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 96.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 59.6 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 75.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 75.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 76.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 71.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 88.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 90.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 92.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 94.9 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 76.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 71.5 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=efd83c44286c0d106b929850c0e7cce71fe252af3630888907009017aa1976bc\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["import wandb\n","\n","wandb.init(project=\"t5-token\", entity=\"akatsuki_leaf\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"id":"xwFTxxJiqwvI","executionInfo":{"status":"ok","timestamp":1670087250808,"user_tz":300,"elapsed":3454,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"outputId":"660af53a-db97-460a-b3c9-729a819ac153"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhiteshwarjmu\u001b[0m (\u001b[33makatsuki_leaf\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20221203_170730-3h5eq3p3</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/akatsuki_leaf/t5-token/runs/3h5eq3p3\" target=\"_blank\">honest-universe-1</a></strong> to <a href=\"https://wandb.ai/akatsuki_leaf/t5-token\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f366483f5d0>"],"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/akatsuki_leaf/t5-token/runs/3h5eq3p3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    \n","    print('Memory Usage:',round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On8STYHmSKtA","executionInfo":{"status":"ok","timestamp":1670087263218,"user_tz":300,"elapsed":1614,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"outputId":"1d70d65c-0d97-4799-f6c2-d0c73c3fcc59"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","Tesla T4\n","Memory Usage: 14.8 GB\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  FutureWarning)\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import random\n","\n","def set_random_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","set_random_seed(0)"],"metadata":{"id":"faaqh5xzw8pI","executionInfo":{"status":"ok","timestamp":1670087265937,"user_tz":300,"elapsed":89,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOsHUjgdIrIW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"432db3fd-f530-4690-efb1-2250da5aec15","executionInfo":{"status":"ok","timestamp":1670087270571,"user_tz":300,"elapsed":3408,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"source":["! pip install datasets transformers rouge-score nltk py7zr"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.7.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.25.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: py7zr in /usr/local/lib/python3.7/dist-packages (0.20.2)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.10.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n","Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.1)\n","Requirement already satisfied: texttable in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.6.7)\n","Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.15.3)\n","Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from py7zr) (3.16.0)\n","Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.9)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from py7zr) (5.4.8)\n","Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.2.3)\n","Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.0)\n","Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"e-v6Mjk_JvG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %cd /content/drive/MyDrive/NLP Project with SCL"],"metadata":{"id":"z5nOd8wBL-BO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","metadata":{"id":"IreSlFmlIrIm","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["20f7eb82ac15406d8f79616709d113ea","35281e39556142a9bf1e970500c416e9","4441745f41b9433abee786db9173662c","6eadf212ef7742cdb0be5e80f145e663","88b56bea54074d7785707e16dcbdd449","19a597704b2e44b387891cdaced9cbc7","a17ec66ddd1345f18592b7e68108a36e","f872be6117074bc8aba2aad4b3dd3ca8","e207dadddb7e49fb8970745ccb0836e5","71c786eede4e4aed84b1a2369a947848","471a1ad39fce49d2849954ee96b6c347"]},"outputId":"dc56b45b-6102-4335-d060-516f4e041cf3","executionInfo":{"status":"ok","timestamp":1670087274304,"user_tz":300,"elapsed":2437,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"source":["from datasets import load_dataset, load_metric\n","\n","raw_datasets = load_dataset(\"samsum\")\n","\n","metric = load_metric(\"rouge\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20f7eb82ac15406d8f79616709d113ea","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  \"\"\"\n"]}]},{"cell_type":"markdown","source":["## T5"],"metadata":{"id":"X23VZ_cqSQJ5"}},{"cell_type":"markdown","metadata":{"id":"EGgU3K1eSZYo"},"source":["### Preprocessing the data"]},{"cell_type":"code","metadata":{"id":"y1It_TQ_SZYo","executionInfo":{"status":"ok","timestamp":1670087490194,"user_tz":300,"elapsed":94,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"source":["model_checkpoint = \"t5-base\""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMAHEIXVSZYo","colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["1d3e7b5e803d4e83b6fe910075c8b83b","c19f165d22154d17baab3037d4b02d01","46dc815d6df84a3381b95f9cf1d9528b","be3fee3a3e8e48fb8c4e2af8b7c7146f","1b670cd6cb934ee6bc8d39630c100034","73c055a964ac42fe963543e2f1461027","b8f1119acba54c42bdecb627202c3d51","f78d3a55930e4b40a0eb33c739af6bfc","63afa40361ce4ee1b6b25af1d1a51c12","d974dc174ae84d9abec1d6166c5be9a4","dbcbda5171a2448f8b66dfa7e1a4d38d","95d3a2f8dfe843759c8c58b9d660dd74","e0db229cc23f41d1a1bff64a0e77c467","3accf7929b3d43f4ae03854e52a4b882","4410587f7c56447d9e9634c4fc62794f","7eb53687304f40d89c1d9c9a361d85a9","5b2ac1da083d4f52a3eb3841f8b6db58","fc242c6eb5324868966dacd028c28b92","f293916b50e04d1e88fa329534764ec3","0d2dac5392b2413cba7c5429159dce49","7b9ba84662764b91a7a647b8b5b1ff96","bb63da76d68a46cfa8f38d2f919c11b2","59df14d0fd1e4099a03aca7152463495","4cc8a06f045e44afa06ebe24dcc28ca0","b95252f8dd1d44129e6c846ba00157a5","055f67b4f1484a2282672842aeee9c71","36d7caae02634e5dbe95e6951ab018d6","65cfd9cb59454b55a6a7840e23d9b061","ede9b5ccb5c7442985cbd43f69c9290a","6222c4d589e84bfaa269f7a65d697228","4056b4b8235d46a99825bde6a0b45a67","9226ce3d264e415e9d75b340a56d712d","dc86722c8d4e4bd7958bd068ef71d1b9"]},"executionInfo":{"status":"ok","timestamp":1670087496270,"user_tz":300,"elapsed":5654,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"outputId":"11fb964a-6c83-4f47-ac2a-254891d5d432"},"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d3e7b5e803d4e83b6fe910075c8b83b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95d3a2f8dfe843759c8c58b9d660dd74","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59df14d0fd1e4099a03aca7152463495","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:165: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["tokenizer([\"Amanda: bla bla\\r\\nGrey: toyot\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXXAc4FOAw5N","executionInfo":{"status":"ok","timestamp":1670088245507,"user_tz":300,"elapsed":92,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"outputId":"66f1097b-6d50-4838-af95-1cf4a499c73a"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[21542, 10, 3, 4605, 3, 4605, 12630, 10, 12, 63, 32, 17, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def check_token_length(dataset):\n","    ids=[]\n","    for i in range(len(dataset['dialogue'])):\n","        if len(tokenizer(dataset['dialogue'][i])['input_ids'])>1000:\n","            ids.append(i)\n","    print(ids)\n","    return ids\n","def remove_idx(list_idx, dataset):\n","    return dataset.select((\n","          i for i in range(len(dataset)) \n","          if i not in set(list_idx)))\n","    \n","train_ids=check_token_length(raw_datasets['train'])\n","validation_ids=check_token_length(raw_datasets['validation'])\n","test_ids = check_token_length(raw_datasets['test'])\n","changed_datasets_train=remove_idx(train_ids, raw_datasets['train'])\n","changed_datasets_val = remove_idx(validation_ids, raw_datasets['validation'])\n","changed_datasets_test = remove_idx(test_ids, raw_datasets['test'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9hiJeaPFqEK","executionInfo":{"status":"ok","timestamp":1670087736148,"user_tz":300,"elapsed":238926,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"outputId":"41cc0519-8e24-49c6-d00e-63124571f622"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["[4269, 9491]\n","[]\n"]},{"output_type":"stream","name":"stderr","text":["Parameter 'indices'=<generator object remove_idx.<locals>.<genexpr> at 0x7f34b21eabd0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"code","metadata":{"id":"vc0BSBLIIrJQ","executionInfo":{"status":"ok","timestamp":1670088138888,"user_tz":300,"elapsed":196,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"source":["max_input_length = 1024\n","max_target_length = 128\n","\n","def make_one_hot_sequence(input_ids, sequence_ids):\n","    changed_sequence_id=[0]\n","    token_to_speaker_id={}\n","    uniq_id = 1\n","    for dic in sequence_ids:\n","        if str(input_ids[dic['spk'][0]:dic['spk'][1]]) in token_to_speaker_id:\n","            speaker_id = token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])]\n","        else:\n","            token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])] = uniq_id\n","            speaker_id = uniq_id\n","            uniq_id+=1\n","        for _ in range(dic['spk'][0], dic['spk'][1]):\n","            changed_sequence_id.append(speaker_id)\n","        for _ in range(dic['utt'][0], dic['utt'][1]):\n","            changed_sequence_id.append(-1)\n","    changed_sequence_id.append(0)\n","    return changed_sequence_id \n","\n","\n","def preprocess_function(examples): ## hit gold here. change this preprocess function to include speaker and turn information. \n","    slash_n = tokenizer([\"\\r\\n\"])['input_ids'][0][1:-1]\n","    slash_n_mask = tokenizer([\"\\r\\n\"])['attention_mask'][0][1:-1]\n","    inputs_list=[]\n","    masks_list=[]\n","    pos_list=[]\n","    for index in range(len(examples['dialogue'])):\n","        # breaking the dialogue for spk:utt info\n","        broken=[]\n","        for utt in examples['dialogue'][index].split(\"\\r\\n\"):\n","            first_ind = utt.find(':')\n","            broken.append(utt[:first_ind])\n","            broken.append(utt[first_ind:])\n","        \n","        tokenized_broken = tokenizer(broken)['input_ids']\n","        attention_broken = tokenizer(broken)['attention_mask']\n","        print(len(tokenized_broken))\n","        # adding \\r\\n tokens\n","        for i in range(1, len(tokenized_broken)-1, 2):\n","            print(slash_n[0])\n","\n","            tokenized_broken[i].insert(-1, slash_n[0])\n","            tokenized_broken[i].insert(-1, slash_n[1])\n","            attention_broken[i].insert(-1, slash_n_mask[0])\n","            attention_broken[i].insert(-1, slash_n_mask[1])\n","            print(\"second\",tokenized_broken[i])\n","\n","        joined = tokenized_broken[0]\n","\n","        # annotating for spk_utt_pos\n","        assoc_dict={}\n","        assoc_dict['spk'] = [1, len(tokenized_broken[0])-1] # the range is actually exclusive of the last index. \n","        odd_bool = True\n","        running_length = len(tokenized_broken[0])\n","        sequence_ids=[]\n","        for inner in tokenized_broken[1:]:\n","            if odd_bool==True:\n","                assoc_dict['utt']=[running_length-1, running_length+len(inner)-3]\n","                odd_bool=False\n","                sequence_ids.append(assoc_dict)\n","                assoc_dict={}\n","            else:\n","                assoc_dict['spk']=[running_length-1, running_length+len(inner)-3]\n","                odd_bool=True\n","            joined = joined[:-1]+inner[1:]\n","            running_length += (len(inner)-2)\n","        \n","        # test for CUDA assert error\n","        if(len(joined)>1024):\n","            print(\"input tokens list length greater than 1024, skipping example\", end=' ')\n","            print(\"equal to\", len(joined))\n","            print(tokenizer.decode(joined))\n","        \n","        # creating inputs list\n","        inputs_list.append(joined)\n","        pos_list.append(make_one_hot_sequence(joined, sequence_ids))\n","        \n","        # creating new mask\n","        joined_mask = attention_broken[0]\n","        for inner_attention in attention_broken[1:]:\n","            joined_mask = joined_mask[:-1]+inner_attention[1:]\n","        masks_list.append(joined_mask)\n","    \n","    # overriding normal model_inputs\n","    inputs = [doc for doc in examples[\"dialogue\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","    model_inputs['input_ids'] = inputs_list\n","    model_inputs['attention_mask'] = masks_list\n","    model_inputs['spk_utt_pos'] = pos_list\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDtsaJeVIrJT","executionInfo":{"status":"error","timestamp":1670088141756,"user_tz":300,"elapsed":230,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"colab":{"base_uri":"https://localhost:8080/","height":430,"referenced_widgets":["c174d87f42a748dab753397400e23b55","3cce4a6783fe46d18f6c05531381b89f","8377dbb8fe4e4d1fb3abeeec74028ca9","bcc4574c93f54258be34d38dc7b15abf","2c0ac6769d85469bb1115ef3c147b439","af943730f96c4c1ab9ecea9c130d0bb5","6617ddada5ab499baebe87c2c0af8e05","5706a0ab78d74893aa6ebdcc85ee7969","0218ef503cb64b1789f06cf50d3b5235","49e4d5eb0be049239e1144823566f134","d49fd45101ff46d3a9beff9a63405632"]},"outputId":"0bb53a4d-b4ad-44b5-dc82-cc235aa4bdec"},"source":["tokenized_datasets_train_o = changed_datasets_train.map(preprocess_function, batched=True)\n","tokenized_datasets_val_o = changed_datasets_val.map(preprocess_function, batched=True)\n","tokenized_datasets_test_o = changed_datasets_test.map(preprocess_function, batched=True)\n","\n","# tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets_train = tokenized_datasets_train_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_val = tokenized_datasets_val_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_test = tokenized_datasets_test_o.remove_columns(['id', 'dialogue', 'summary'])"],"execution_count":26,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c174d87f42a748dab753397400e23b55","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/15 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["6\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-22fc162377cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_datasets_train_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchanged_datasets_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenized_datasets_val_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchanged_datasets_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_datasets_test_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchanged_datasets_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2601\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m                 \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2603\u001b[0;31m                 \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2604\u001b[0m             )\n\u001b[1;32m   2605\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m         }\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2984\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m                                 \u001b[0mcheck_same_num_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m                                 \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m                             )\n\u001b[1;32m   2988\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mNumExamplesMismatchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2863\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2865\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2543\u001b[0m                 )\n\u001b[1;32m   2544\u001b[0m                 \u001b[0;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2545\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2546\u001b[0m                 \u001b[0;31m# Return a standard dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-6eaae4782aa7>\u001b[0m in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# adding \\r\\n tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_broken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslash_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtokenized_broken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslash_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","\n","\n","class CustomTrainer(Seq2SeqTrainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # How the loss is computed by Trainer. By default, all models return the loss in the first element.\n","        # Subclass and override for custom behavior.\n","        # print(inputs)\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","\n","        # Save past state if it exists\n","        # TODO: this needs to be fixed and mselfade cleaner later.\n","\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n","                loss = self.label_smoother(outputs, labels, shift_labels=True)\n","            else:\n","                loss = self.label_smoother(outputs, labels)\n","        else:\n","            if isinstance(outputs, dict) and \"loss\" not in outputs:\n","                raise ValueError(\n","                    \"The model did not return a loss from the inputs, only the following keys: \"\n","                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n","                )\n","            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","        return (loss, outputs) if return_outputs else loss\n"],"metadata":{"id":"eXTmGGJbtFhr","executionInfo":{"status":"aborted","timestamp":1670087736851,"user_tz":300,"elapsed":10,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import PreTrainedTokenizerBase\n","from transformers.utils import PaddingStrategy\n","from transformers import DataCollatorForSeq2Seq\n","from typing import Optional, Any, Union\n","import numpy as np\n","\n","\n","class CustomCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n","    r\"\"\"\n","    Data collator that will dynamically pad the inputs received, as well as the labels.\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        model ([`PreTrainedModel`]):\n","            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n","            prepare the *decoder_input_ids*\n","            This is useful when using *label_smoothing* to avoid calculating loss twice.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n","              is provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n","              lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","        label_pad_token_id (`int`, *optional*, defaults to -100):\n","            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    model: Optional[Any] = None\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    label_pad_token_id: int = -100\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features, return_tensors=None):\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n","        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n","        # same length to return tensors.\n","        if labels is not None:\n","            max_label_length = max(len(l) for l in labels)\n","            if self.pad_to_multiple_of is not None:\n","                max_label_length = (\n","                        (max_label_length + self.pad_to_multiple_of - 1)\n","                        // self.pad_to_multiple_of\n","                        * self.pad_to_multiple_of\n","                )\n","\n","            padding_side = self.tokenizer.padding_side\n","            for feature in features:\n","                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n","                if isinstance(feature[\"labels\"], list):\n","                    feature[\"labels\"] = (\n","                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n","                    )\n","                elif padding_side == \"right\":\n","                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n","                else:\n","                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n","        # added here\n","        spk_utt_pos = [feature[\"spk_utt_pos\"] for feature in features]\n","        max_spk_utt_pos_length = max(len(l) for l in spk_utt_pos)\n","\n","        if self.pad_to_multiple_of is not None:\n","            max_spk_utt_pos_length = (\n","                    (max_spk_utt_pos_length + self.pad_to_multiple_of - 1)\n","                    // self.pad_to_multiple_of\n","                    * self.pad_to_multiple_of\n","            )\n","\n","        padding_side = self.tokenizer.padding_side\n","        for feature in features:\n","            remainder = [0] * (max_spk_utt_pos_length - len(feature[\"spk_utt_pos\"]))\n","            if isinstance(feature[\"spk_utt_pos\"], list):\n","                feature[\"spk_utt_pos\"] = (\n","                    feature[\"spk_utt_pos\"] + remainder if padding_side == \"right\" else remainder + feature[\n","                        \"spk_utt_pos\"]\n","                )\n","            elif padding_side == \"right\":\n","                feature[\"spk_utt_pos\"] = np.concatenate([feature[\"spk_utt_pos\"], remainder]).astype(np.int64)\n","            else:\n","                feature[\"spk_utt_pos\"] = np.concatenate([remainder, feature[\"spk_utt_pos\"]]).astype(np.int64)\n","\n","        features = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=return_tensors,\n","        )\n","\n","        # prepare decoder_input_ids\n","        if (\n","                labels is not None\n","                and self.model is not None\n","                and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n","        ):\n","            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n","            features[\"decoder_input_ids\"] = decoder_input_ids\n","\n","        return features\n"],"metadata":{"id":"HEZHf93mtTx2","executionInfo":{"status":"aborted","timestamp":1670087736852,"user_tz":300,"elapsed":10,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","from transformers import BartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","\n","from transformers.models.bart.modeling_bart import BartConfig\n","import torch\n","from typing import *\n","from transformers.modeling_outputs import Seq2SeqLMOutput\n","from transformers.models.bart.modeling_bart import shift_tokens_right\n","import random\n","from tqdm import tqdm\n","import gc\n","import itertools\n","class T5WithSCL(AutoModelForSeq2SeqLM):\n","    def __init__(self, config: BartConfig):\n","        super().__init__(config)\n","\n","    def set_losses_list(self, SCLossesList=['token']):\n","        self.SCLossesList = SCLossesList\n","    def set_scl_coeff(self, scl_coeff=1e-1):\n","        self.scl_coeff=scl_coeff\n","    def token_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Token Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        spk_utt_states[spk].append(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]])\n","            \n","            \n","            #---------- hitesh------------------------------\n","            # positive samples\n","            # L_pos = 0\n","            # L_neg = 0 \n","\n","            # sampled_spk_utt_states = []           \n","\n","            # for spk in uniq_spks:\n","            #     utts = len(spk_utt_states[spk])\n","            #     spk_utt = []\n","            #     if utts > 1:\n","            #         # ids = random.sample(list(range(len(spk_utt_states[spk]))), random.randint(1, utts))\n","            #         ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","            #         for i in ids:\n","            #           spk_utt.append(spk_utt_states[spk][i])\n","            #     sampled_spk_utt_states.append(spk_utt)\n","\n","            # for instance in sampled_spk_utt_states:\n","            #   for i in range(len(instance)):\n","            #     for j in range(len(instance)):\n","            #       mat_mul = torch.einsum('ij, kj->ik', instance[i], instance[j])\n","            #       sigm = torch.sigmoid(mat_mul)\n","            #       log = torch.log(sigm)\n","            #       L_pos += torch.sum(-1 * log)\n","            # # print(\"L_pos\", L_pos)\n","\n","            # #negative loss\n","            # for i in range(0,len(sampled_spk_utt_states)):\n","            #   instance = sampled_spk_utt_states[i]\n","\n","            #   neg_instances = sampled_spk_utt_states[:i]+sampled_spk_utt_states[i+1:]\n","            #   neg_instances = list(itertools.chain(*neg_instances))\n","            #   # neg_instances = random.choices(neg_instances,k = random.randint(1, len(neg_instances)))\n","            #   if len(neg_instances)>0:\n","            #     # print(len(neg_instances))\n","            #     # print(\"-------------------------\")\n","            #     # print(sampled_spk_utt_states)\n","            #     neg_instances = random.choices(neg_instances,k = 2)\n","            #     for i in range(len(instance)):\n","            #       for j in range(len(neg_instances)):\n","            #         mat_mul = torch.einsum('ij, kj->ik', instance[i], neg_instances[j])\n","            #         sigm = torch.sigmoid(mat_mul)\n","            #         log = torch.log(1 - sigm+1e-5)\n","            #         L_neg += torch.sum(-1 * log)\n","            #---------- hitesh------------------------------\n","            \n","            \n","            # positive samples\n","            L_pos = 0\n","            \n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk][id1])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            \n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            wandb.log({\"batch_scl-token\": batch_scl})\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def turn_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk][id1])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    # L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                # L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            wandb.log({\"batch_scl-turn\": batch_scl})\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def global_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.choice(list(range(len(spk_utt_states[spk]))))\n","                    \n","                    spk_mean_exc = torch.mean(torch.vstack([spk_utt_states[spk][temp] for temp in range(len(spk_utt_states[spk])) if temp != ids]), 0)\n","                    \n","                    pos_mat_mul = torch.einsum('i, j->', spk_utt_states[spk][ids], spk_mean_exc)\n","                    pos_sigm = torch.sigmoid(pos_mat_mul)\n","                    pos_log = torch.log(pos_sigm)\n","                    L_pos += torch.sum(-1 * pos_log)\n","\n","                    # negative sample\n","\n","                    new_uniq_spks = uniq_spks.copy()\n","                    new_uniq_spks.remove(spk)\n","                    \n","                    spk2 = random.choice(new_uniq_spks)\n","                    id_neg = random.choice(list(range(len(spk_utt_states[spk2]))))\n","                    neg_mat_mul = torch.einsum('i, j->', spk_utt_states[spk2][id_neg], spk_mean_exc)\n","                    neg_sigm = torch.sigmoid(neg_mat_mul)\n","                    neg_log = torch.log(1 - neg_sigm+1e-5)\n","                    L_neg += torch.sum(-1 * neg_log)\n","                \n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            wandb.log({\"batch_scl-global\": batch_scl})\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","\n","    def forward(\n","            self,\n","            input_ids: torch.LongTensor = None,\n","            attention_mask: Optional[torch.Tensor] = None,\n","            spk_utt_pos: Optional[torch.Tensor] = None, ##changed here\n","            decoder_input_ids: Optional[torch.LongTensor] = None,\n","            decoder_attention_mask: Optional[torch.LongTensor] = None,\n","            head_mask: Optional[torch.Tensor] = None,\n","            decoder_head_mask: Optional[torch.Tensor] = None,\n","            cross_attn_head_mask: Optional[torch.Tensor] = None,\n","            encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n","            past_key_values: Optional[List[torch.FloatTensor]] = None,\n","            inputs_embeds: Optional[torch.FloatTensor] = None,\n","            decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","            labels: Optional[torch.LongTensor] = None,\n","            use_cache: Optional[bool] = None,\n","            output_attentions: Optional[bool] = None,\n","            output_hidden_states: Optional[bool] = None,\n","            return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, Seq2SeqLMOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n","            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n","            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n","        Returns:\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if labels is not None:\n","            if use_cache:\n","                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n","            use_cache = False\n","            if decoder_input_ids is None and decoder_inputs_embeds is None:\n","                decoder_input_ids = shift_tokens_right(\n","                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n","                )\n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            encoder_outputs=encoder_outputs,\n","            decoder_attention_mask=decoder_attention_mask,\n","            head_mask=head_mask,\n","            decoder_head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            past_key_values=past_key_values,\n","            inputs_embeds=inputs_embeds,\n","            decoder_inputs_embeds=decoder_inputs_embeds,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        if encoder_outputs is None:\n","            encoder = self.get_encoder()\n","            # TODO: mask the speaker names from the input IDs using the speaker pos info\n","            turn_attention_mask=None\n","            token_encoder_outputs=None\n","            tog_encoder_outputs=None\n","            \n","            if 'token' in self.SCLossesList:\n","                token_encoder_outputs = encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","\n","            if 'turn' in self.SCLossesList or 'global' in self.SCLossesList:\n","                tog_attention_mask = torch.where(spk_utt_pos>0, 0, attention_mask)\n","                tog_encoder_outputs = encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=tog_attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","        # if 'hidden_states' in encoder_outputs:\n","        #     print(\"encoder_outputs['last_hidden_state'].size(), encoder_outputs['hidden_states'].size()\",\n","        #     encoder_outputs['last_hidden_state'].size(), encoder_outputs['hidden_states'].size())\n","        # else:\n","        #     print(\"encoder_outputs['last_hidden_state'].size()\", encoder_outputs['last_hidden_state'].size())\n","\n","        lm_logits = self.lm_head(outputs[0])\n","        lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = torch.nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n","        # added here\n","        sc_loss = 0\n","        if 'token' in self.SCLossesList and labels is not None:\n","            sc_loss += self.token_scl(last_hidden_state=token_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","            # print(sc_loss)\n","        if 'turn' in self.SCLossesList and labels is not None:\n","            sc_loss += self.turn_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if 'global' in self.SCLossesList and labels is not None:\n","            sc_loss += self.global_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if not return_dict:\n","            output = (lm_logits,) + outputs[1:]\n","            return ((masked_lm_loss+(self.scl_coeff*sc_loss),) + output) if masked_lm_loss is not None else output\n","        loss = None\n","        if masked_lm_loss is None:\n","            loss = None\n","        else:\n","            loss = masked_lm_loss+(self.scl_coeff*sc_loss)\n","        wandb.log({\"batch_scl-loss\": loss})\n","        \n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=outputs.past_key_values,\n","            decoder_hidden_states=outputs.decoder_hidden_states,\n","            decoder_attentions=outputs.decoder_attentions,\n","            cross_attentions=outputs.cross_attentions,\n","            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n","            encoder_hidden_states=outputs.encoder_hidden_states,\n","            encoder_attentions=outputs.encoder_attentions,\n","        )\n"],"metadata":{"id":"nEvSIc03tW7d","executionInfo":{"status":"aborted","timestamp":1670087736853,"user_tz":300,"elapsed":11,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = T5WithSCL.from_pretrained(model_checkpoint)\n","model.set_losses_list(['token'])\n","model.set_scl_coeff(0.1)\n"],"metadata":{"id":"0K6zHZKL7Tle","executionInfo":{"status":"aborted","timestamp":1670087736854,"user_tz":300,"elapsed":12,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZNKwHmTpjxX"},"source":["### Fine-tuning the model"]},{"cell_type":"code","source":["# Parameters\\\n","batch_size=4\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"t5-token-b4c0.1\",\n","    num_train_epochs=5,\n","    do_train=True,\n","    do_eval=True,\n","    evaluation_strategy = \"epoch\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    learning_rate=1e-4,\n","    warmup_steps=500,\n","    weight_decay=0.1,\n","    # label_smoothing_factor=0.1, ## causes to throw an error\n","    predict_with_generate=True,\n","    # logging_dir=\"logs\",\n","    logging_steps=10,\n","    # save_total_limit=3,\n",")\n","\n","\n","data_collator = CustomCollatorForSeq2Seq(tokenizer, model=model)\n"],"metadata":{"id":"37a4Gih6lB8t","executionInfo":{"status":"aborted","timestamp":1670087736854,"user_tz":300,"elapsed":11,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmvbnJ9JIrJd","executionInfo":{"status":"aborted","timestamp":1670087736855,"user_tz":300,"elapsed":12,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"source":["import nltk\n","import numpy as np\n","import torch\n","torch.cuda.empty_cache()\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    for i in range(0,50):\n","      # print(tokenized_datasets_val[\"dialogue\"][i])\n","      print(\"-----------\",i,\"--------------\")\n","      print(\"------>Predictions by Model\")\n","      print(decoded_preds[i])\n","      print(\"----->Predictions Original\")\n","      print(decoded_labels[i])\n","      print(\"**************************\")\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets_train,\n","    eval_dataset=tokenized_datasets_val,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"fUlQNc7i7ziP","executionInfo":{"status":"aborted","timestamp":1670087736856,"user_tz":300,"elapsed":13,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xixI4gdbuoe","executionInfo":{"status":"aborted","timestamp":1670087736857,"user_tz":300,"elapsed":14,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"l24RBo5J75kp","executionInfo":{"status":"aborted","timestamp":1670087736857,"user_tz":300,"elapsed":13,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate before training for comparison\n","trainer.evaluate()"],"metadata":{"id":"5RY4ZQIDrmWr","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1669407741389,"user_tz":300,"elapsed":135974,"user":{"displayName":"Hiteshwar Singh","userId":"09663286059140448498"}},"outputId":"69d76c56-61e3-421a-84d1-614f1901f2cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, id, dialogue. If summary, id, dialogue are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 8\n","You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [103/103 02:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["-------------------------\n","------>Predictions by Model\n","a hamster named lemmy is named after his dead hamster.\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","x-mas advent calendars are filled with various things.\n","i love the idea\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Madison is pregnant Iggy: ok, she's probably worrying about it Iggy:\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","a teen girl's underwear was found under her bed.\n","it was probably\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","m8: i have to buy guitar cable Fred: file_other>\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","a drawer next to the fridge has cereal and milk.\n","a false alarm, we\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Holly: I'm surprised!\n","Holly: 'i'm surprised!'\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Luis: a bit outside the city Adam: where exactly?\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","vegano resto: sorry, there is no room at 20:00. but he\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","a Texan drawl is a Texan drawl, girl!\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Jamie: you're sure you need a new printer?\n","Laura: you can buy\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","yoghurt is a yoghurt made with coconut milk.\n","it'\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Wendy: ok, am buying something.\n","be right out!\n","Wendy: ok, am\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Cecil: \"it's always windy here\" but he took some nice pics\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Sophie: Checked all pockets and handbags.\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","b-movies like toxic avenger and poultrygeist are among your favorites\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Julia: I've had this dream for some time now, to become a voice actor\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Alice: I'm in the same shape as you!\n","What a long day!\n","Alice\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","caron: i'm out from 12 Sash: will be before Sash:\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","he's a little strange Giuseppe: why is she so strange?\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Ramzi: Mom is calling for supper Jannette: Ok Im coming Ramzi\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","ravioli are a real treasure.\n","nobody makes them better than your family.\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Madison: Okay.\n","but make it quick.\n","Lawrence: Okay.\n","I will get back to you\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","hahahahahahahahahahahahahahahahahahaha\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Chloe: i need to go to the dentist quickly after work.\n","sure\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Olivia: I am currently involved with a new restaurant in the city centre.\n","Olivia:\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","he's back from surgery, but the doctors don't know if he\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Skyler: i know, he cheated on her with numerous girls.\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","i'm pregnant!\n","i'm so proud of myself!\n","i'm\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Isabel: I haven’t had any!\n","(;)(;)(\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","theo: we're going to the italian Alpes, but we'll be back\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Brandon hasn't called to say he'd be late.\n","he'\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Olga: I'm sick again... and I'm caughing like crazy\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","awww :3 Kate: yes, I'm terrified.\n","awww\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Andrew: i can never guess the weather right you know?\n","he says it's\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","'Millionaires' on tvn Sam: ok, fingers crossed then\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Kelly: Thanks :*\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Sophie: he's already 40 min late Sophie: i hate this!\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Sue: i'm going downstairs now Daniel: 'i'm on my way\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","stuffed squash with salmon?\n","Probably.\n","stuffed squash with salmon?\n","i'\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","-------------------------\n","------>Predictions by Model\n","endomondo doesn't work properly I think or maybe there's something wrong with\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Ivan: ok, you'll come next time Ivan: ok, you\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Ashley's nude fb page is a hit with her students.\n","she\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","a letter says that the papers won't be sent in time and need another month\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Maria: ok, why not?\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","sus: don't wanna work :D Val: I know :D\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","we're at the Guggenheim right now, so it will take a bit\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Cathy: i left my sunglasses at your house Broke: Yeah, they are waiting\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Frederica: ok, see you tomorrow!\n","Bradley: Great!\n","you?\n","Freder\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","-------------------------\n","------>Predictions by Model\n","Adrian: Did you receive the money?\n","Camilla: Yes, let me know when you have\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 2.411588191986084,\n"," 'eval_rouge1': 24.5734,\n"," 'eval_rouge2': 7.6639,\n"," 'eval_rougeL': 20.7835,\n"," 'eval_rougeLsum': 22.5135,\n"," 'eval_gen_len': 18.3191,\n"," 'eval_runtime': 135.4583,\n"," 'eval_samples_per_second': 6.039,\n"," 'eval_steps_per_second': 0.76}"]},"metadata":{},"execution_count":29}]}]}