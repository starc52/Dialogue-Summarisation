{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LSGuhaPN4Syw"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a029a1f51884c2685a6a1821dd13e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94808883277e4fcdacc20506d5ce9bc7",
              "IPY_MODEL_0c9bb5a4fb13440392df6c51ebebd310",
              "IPY_MODEL_b619f7f7f2734138b7d1dd0b9fae58c5"
            ],
            "layout": "IPY_MODEL_fa2e8b24f1e54af38a87af5bc8b86430"
          }
        },
        "94808883277e4fcdacc20506d5ce9bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f16ed8746774ec7ad9ffb04c8295d1b",
            "placeholder": "​",
            "style": "IPY_MODEL_d531a8a8a10143c1ac84d87beddf8f39",
            "value": "100%"
          }
        },
        "0c9bb5a4fb13440392df6c51ebebd310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019c5439289040a0bf80b61e5ff19922",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02f998e832de4468b30c41b1bad82e6a",
            "value": 3
          }
        },
        "b619f7f7f2734138b7d1dd0b9fae58c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe63a92ff02945fdb3d526f8e21ec778",
            "placeholder": "​",
            "style": "IPY_MODEL_a4981f121b9e4060aa03e90955ae051b",
            "value": " 3/3 [00:00&lt;00:00, 105.57it/s]"
          }
        },
        "fa2e8b24f1e54af38a87af5bc8b86430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f16ed8746774ec7ad9ffb04c8295d1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d531a8a8a10143c1ac84d87beddf8f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "019c5439289040a0bf80b61e5ff19922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02f998e832de4468b30c41b1bad82e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe63a92ff02945fdb3d526f8e21ec778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4981f121b9e4060aa03e90955ae051b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7e1e2df2f7e4c09b8d2940588f1144c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28274ac53aa647049fe38b0bf83c43dc",
              "IPY_MODEL_1429c5ede33d48e8ad210b3cd7908647",
              "IPY_MODEL_2dbb864acf9542cfa5f2dad34bc7bf3c"
            ],
            "layout": "IPY_MODEL_4b0e997f43cc49cebba270b9d2861a35"
          }
        },
        "28274ac53aa647049fe38b0bf83c43dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6772cee92c4f4936ab9daa64fdffc844",
            "placeholder": "​",
            "style": "IPY_MODEL_1fe24a6c14bb4f7c9ecef3f9816c2085",
            "value": "100%"
          }
        },
        "1429c5ede33d48e8ad210b3cd7908647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f185a6b300c64b72819ba10a87798434",
            "max": 14731,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a85165734d241de83ba29e4418159ab",
            "value": 14731
          }
        },
        "2dbb864acf9542cfa5f2dad34bc7bf3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_160c9e6889a444a5a463b983253605b8",
            "placeholder": "​",
            "style": "IPY_MODEL_a0e193a9312445ab84c87f3ff5265add",
            "value": " 14731/14731 [00:02&lt;00:00, 5415.59ex/s]"
          }
        },
        "4b0e997f43cc49cebba270b9d2861a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6772cee92c4f4936ab9daa64fdffc844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe24a6c14bb4f7c9ecef3f9816c2085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f185a6b300c64b72819ba10a87798434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a85165734d241de83ba29e4418159ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "160c9e6889a444a5a463b983253605b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e193a9312445ab84c87f3ff5265add": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc8c97708f294973bcc58b42f32c2ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59800ad4f99d4638862cd759d5a54a9b",
              "IPY_MODEL_3da7c03fa0a6455fbc78997b42df7d1e",
              "IPY_MODEL_0efd944077da4da691bf2ffa5e0a5f6e"
            ],
            "layout": "IPY_MODEL_dc1e641336a646b3961441c7e95571bd"
          }
        },
        "59800ad4f99d4638862cd759d5a54a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab73871eb35404eb0daff146425c3fe",
            "placeholder": "​",
            "style": "IPY_MODEL_eb7c7da187734c0bb0f2be6cda6f8000",
            "value": "100%"
          }
        },
        "3da7c03fa0a6455fbc78997b42df7d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aca43c6b0a6e40119ae7f4544d04d631",
            "max": 819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_847dec1df87d443aae67c9e2530ef1b6",
            "value": 819
          }
        },
        "0efd944077da4da691bf2ffa5e0a5f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6207a07ec89c4532964bad4a240b9850",
            "placeholder": "​",
            "style": "IPY_MODEL_26a50f56accd474a939ea282e1381bc4",
            "value": " 819/819 [00:00&lt;00:00, 8864.77ex/s]"
          }
        },
        "dc1e641336a646b3961441c7e95571bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab73871eb35404eb0daff146425c3fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7c7da187734c0bb0f2be6cda6f8000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aca43c6b0a6e40119ae7f4544d04d631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847dec1df87d443aae67c9e2530ef1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6207a07ec89c4532964bad4a240b9850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a50f56accd474a939ea282e1381bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c15f016a9ba843d1ba366b92ca170a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f197b73f3644495a97cee13f85246ad",
              "IPY_MODEL_5a889f5da40f4ee0acec57ae5b06c81c",
              "IPY_MODEL_304f58fe83264c2dae86557663955cef"
            ],
            "layout": "IPY_MODEL_06c1b4ad54c747f0b63053136e1d5ab9"
          }
        },
        "5f197b73f3644495a97cee13f85246ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4496710adfab404995633675f1fafffc",
            "placeholder": "​",
            "style": "IPY_MODEL_72327ca87dcc47278e34c600f7a0c2a8",
            "value": "100%"
          }
        },
        "5a889f5da40f4ee0acec57ae5b06c81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de906135707f4453a3504cd81749f229",
            "max": 818,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_537aba1aec51464ea941d6bc9a11924a",
            "value": 818
          }
        },
        "304f58fe83264c2dae86557663955cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3f84f0349ad4081bd44c7d550074fca",
            "placeholder": "​",
            "style": "IPY_MODEL_0e4ad84fdb7044ed800bd0dc3f6b4a40",
            "value": " 818/818 [00:00&lt;00:00, 7566.26ex/s]"
          }
        },
        "06c1b4ad54c747f0b63053136e1d5ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4496710adfab404995633675f1fafffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72327ca87dcc47278e34c600f7a0c2a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de906135707f4453a3504cd81749f229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537aba1aec51464ea941d6bc9a11924a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3f84f0349ad4081bd44c7d550074fca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e4ad84fdb7044ed800bd0dc3f6b4a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "!pip install datasets transformers rouge-score nltk py7zr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e-v6Mjk_JvG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c72d69-8b9b-49b4-8f4b-70d4290fe4e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP Project"
      ],
      "metadata": {
        "id": "z5nOd8wBL-BO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1310d7b7-6cd8-4537-fb60-b2278cde4079"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need to replace these code with modified version for this notebook to run properly\n",
        "!cp \"/content/drive/MyDrive/NLP Project/tokenization_utils_base.py\" /usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\n",
        "!cp \"/content/drive/MyDrive/NLP Project/trainer_seq2seq.py\" /usr/local/lib/python3.8/dist-packages/transformers/trainer_seq2seq.py"
      ],
      "metadata": {
        "id": "k0eiGcjV89P-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a summarization task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers.modeling_utils import unwrap_model\n",
        "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, load_metric\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from modeling_bart import BartForConditionalGeneration  # Custom coref bart\n",
        "from modeling_t5 import T5ForConditionalGeneration    # Custom coref t5\n"
      ],
      "metadata": {
        "id": "ZetvZ8yuGtkE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreSlFmlIrIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "8a029a1f51884c2685a6a1821dd13e19",
            "94808883277e4fcdacc20506d5ce9bc7",
            "0c9bb5a4fb13440392df6c51ebebd310",
            "b619f7f7f2734138b7d1dd0b9fae58c5",
            "fa2e8b24f1e54af38a87af5bc8b86430",
            "3f16ed8746774ec7ad9ffb04c8295d1b",
            "d531a8a8a10143c1ac84d87beddf8f39",
            "019c5439289040a0bf80b61e5ff19922",
            "02f998e832de4468b30c41b1bad82e6a",
            "fe63a92ff02945fdb3d526f8e21ec778",
            "a4981f121b9e4060aa03e90955ae051b"
          ]
        },
        "outputId": "45b550b9-75da-4f6b-fe30-70336a8cf974"
      },
      "source": [
        "raw_datasets = load_dataset(\"samsum\")\n",
        "raw_datasets = raw_datasets.filter(lambda data:data['dialogue'] != \"\")\n",
        "metric = load_metric(\"rouge\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a029a1f51884c2685a6a1821dd13e19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-d1fc38f963a75d34.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-0f4686307978eada.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-31d8585b85287fcf.arrow\n",
            "<ipython-input-7-1fa46ba57893>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"rouge\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class CustomTrainer(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "        # Subclass and override for custom behavior.\n",
        "        if self.label_smoother is not None and \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Save past state if it exists\n",
        "        # TODO: this needs to be fixed and mselfade cleaner later.\n",
        "\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = outputs[self.args.past_index]\n",
        "\n",
        "        if labels is not None:\n",
        "            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
        "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
        "            else:\n",
        "                loss = self.label_smoother(outputs, labels)\n",
        "        else:\n",
        "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
        "                raise ValueError(\n",
        "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
        "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
        "                )\n",
        "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
        "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "eIefc8iBq_zm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coreference functions"
      ],
      "metadata": {
        "id": "1YftpcArAVVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess functions"
      ],
      "metadata": {
        "id": "JNaJb-yv4ZZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_processed_coref_dataset(prefix, datasets):\n",
        "    \"\"\"\n",
        "    :param prefix: prefix arbitrary name\n",
        "    :param datasets: the datasets need to save.\n",
        "    :return: directly write a file.\n",
        "    \"\"\"\n",
        "    task_names = ['train', 'test', 'validation']\n",
        "    for name in task_names:\n",
        "        output_fp = open(\"data/\" + prefix + '-' + name + '.source', 'w', encoding='utf-8')\n",
        "        current_dataset = datasets[name]\n",
        "        for data in current_dataset:\n",
        "            output_fp.write(data['id'] + '#' + str(data['input_ids']) + '#' \n",
        "                + str(data['coref_information'][0]) + '#' + str(data['coref_information'][1]) + '\\n')\n",
        "        output_fp.close()\n",
        "\n",
        "\n",
        "def convert_str_list_to_list(str_list):\n",
        "    tmp = str_list.strip().replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
        "    tmp = [int(i.strip()) for i in tmp if len(i.strip()) > 0]\n",
        "    return tmp\n",
        "\n",
        "\n",
        "def load_processed_coref_dataset(prefix):\n",
        "    \"\"\"\n",
        "    :param prefix: prefix arbitrary name\n",
        "    :return: input_ids and coref_information\n",
        "    \"\"\"\n",
        "    task_names = ['train', 'test', 'validation']\n",
        "    datasets = {}\n",
        "    for name in task_names:\n",
        "        input_ids_list = []\n",
        "        coref_information_list = []\n",
        "        result_dict = {}\n",
        "        input_fp = open(\"data/\" + prefix + '-' + name + '.source', 'r', encoding='utf-8')\n",
        "        for line in input_fp:\n",
        "            tmp = line.split('#')\n",
        "            input_ids_list.append(convert_str_list_to_list(tmp[1]))\n",
        "            coref_information_list.append((convert_str_list_to_list(tmp[2]), convert_str_list_to_list(tmp[3])))\n",
        "        result_dict['input_ids'] = input_ids_list\n",
        "        result_dict['coref_information'] = coref_information_list\n",
        "        datasets[name] = result_dict\n",
        "    \n",
        "    return datasets\n",
        "\n",
        "\n",
        "# preprocess fucntions\n",
        "def add_preprocessed_data_train(x, indice):\n",
        "    x['input_ids'] = coref_datasets['train']['input_ids'][indice]\n",
        "    x['attention_mask'] = [1] * len(coref_datasets['train']['input_ids'][indice])\n",
        "    x['coref_information'] = coref_datasets['train']['coref_information'][indice]\n",
        "    return x\n",
        "\n",
        "\n",
        "def add_preprocessed_data_test(x, indice):\n",
        "    x['input_ids'] = coref_datasets['test']['input_ids'][indice]\n",
        "    x['attention_mask'] = [1] * len(coref_datasets['test']['input_ids'][indice])\n",
        "    x['coref_information'] = coref_datasets['test']['coref_information'][indice]\n",
        "    return x\n",
        "\n",
        "\n",
        "def add_preprocessed_data_validation(x, indice):\n",
        "    x['input_ids'] = coref_datasets['validation']['input_ids'][indice]\n",
        "    x['attention_mask'] = [1] * len(coref_datasets['validation']['input_ids'][indice])\n",
        "    x['coref_information'] = coref_datasets['validation']['coref_information'][indice]\n",
        "    return x\n",
        "\n",
        "\n",
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = examples\n",
        "    \n",
        "    # Setup the tokenizer for targets\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def preprocess_function_t5(examples):\n",
        "    task_prefix = \"summarize: \"\n",
        "    first_word = \" #\"  # replace the first token of original input ids\n",
        "    first_token = tokenizer(first_word)\n",
        "    tokenized_prefix = tokenizer(task_prefix)\n",
        "\n",
        "    model_inputs = examples\n",
        "\n",
        "    # replace the original input_ids\n",
        "    input_ids = [tokenized_prefix['input_ids'][:2] + first_token['input_ids'][:1] + i[1:] for i in model_inputs['input_ids']]\n",
        "    model_inputs['input_ids'] = input_ids\n",
        "\n",
        "    # update attention mask\n",
        "    mask = [[1, 1] + i for i in model_inputs['attention_mask']]\n",
        "    model_inputs['attention_mask'] = mask\n",
        "\n",
        "    new_coref_information = []\n",
        "    # update coreference information\n",
        "    for source_list, target_list in model_inputs['coref_information']:\n",
        "        temp_list = []\n",
        "        new_source_list = [i+2 for i in source_list]\n",
        "        new_target_list = [i+2 for i in target_list]\n",
        "        temp_list.append(new_source_list)\n",
        "        temp_list.append(new_target_list)\n",
        "        new_coref_information.append(temp_list)\n",
        "    \n",
        "    model_inputs['coref_information'] = new_coref_information\n",
        "\n",
        "    # setup the tokenizer for targets\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# save function\n",
        "def save_result(result, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as fp:\n",
        "        for i in result:\n",
        "            fp.write(i.strip() + \"\\n\")"
      ],
      "metadata": {
        "id": "cfusKMjaJWis"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coreference information extraction functions"
      ],
      "metadata": {
        "id": "LSGuhaPN4Syw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coreference information extraction takes a lot of time. Thus, we only extracted once and saved the result for future usage."
      ],
      "metadata": {
        "id": "UV1M5hwaUs8y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip instal allennlp allennlp_models"
      ],
      "metadata": {
        "id": "RU2pnAQGwRY4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # code from https://github.com/seq-to-mind/coref_dial_summ modified.\n",
        "# import re\n",
        "# import pickle\n",
        "# from tqdm import tqdm\n",
        "# from allennlp.predictors.predictor import Predictor\n",
        "# import allennlp_models.coref\n",
        "# from allennlp_models import pretrained\n",
        "\n",
        "\n",
        "# class NeuralCoreferenceProcessing:\n",
        "#     def __init__(self, gpu_id=-1):\n",
        "#         \"\"\" download and indicate the path of pre-trained coref-spanbert model \"\"\"\n",
        "#         # self.predictor = Predictor.from_path(\"/content/drive/MyDrive/coref-spanbert-large-2021.03.10.tar.gz.tar\", cuda_device=gpu_id)\n",
        "#         self.predictor = pretrained.load_predictor(\"coref-spanbert\", cuda_device=gpu_id)\n",
        "\n",
        "#     def process(self, input_list, batch_size=4):\n",
        "#         # output_list = {\"dot\": [], \"sharp\": [], \"newline\": [], \"semicolon\": []}\n",
        "#         output_list = []\n",
        "#         dataloader = DataLoader(input_list, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#         # for tmp_content in tqdm(input_list):\n",
        "#         # for tmp_content in input_list:\n",
        "#         for tmp_content in dataloader:\n",
        "#             tmp_content = [dialogue.replace(\"#\", \" \").replace(\"\\r\\n\", \" # \").replace(\"\\n\", \" \").replace(\"🙂\", \" \") \n",
        "#                             for dialogue in tmp_content]\n",
        "#             tmp_content = [re.sub(\"\\s+\", \" \", dialogue).strip() for dialogue in tmp_content]\n",
        "\n",
        "#             \"\"\" here we replace the sentence segmenter, to obtain multiple coreference resolution outputs \"\"\"\n",
        "#             tmp_res_with_dot_batch = self.predictor.predict_batch_json([{\"document\": dialogue.replace(\"#\", \".\")} for dialogue in tmp_content])\n",
        "#             tmp_res_with_sharp_batch = self.predictor.predict_batch_json({\"document\": dialogue} for dialogue in tmp_content)\n",
        "#             tmp_res_with_newline_batch = self.predictor.predict_batch_json([{\"document\": dialogue.replace(\"#\", \"\\n\")} for dialogue in tmp_content])\n",
        "#             tmp_res_with_semicolon_batch = self.predictor.predict_batch_json([{\"document\": dialogue.replace(\"#\", \";\")} for dialogue in tmp_content])\n",
        "\n",
        "#             # # \"\"\" check the length of multiple coreference resolution outputs are the same \"\"\"\n",
        "#             # assert len(tmp_res_with_dot['document']) == len(tmp_res_with_sharp['document'])\n",
        "#             # assert len(tmp_res_with_newline['document']) == len(tmp_res_with_sharp['document'])\n",
        "#             # assert len(tmp_res_with_semicolon['document']) == len(tmp_res_with_sharp['document'])\n",
        "\n",
        "#             for (dialogue, tmp_res_with_dot, tmp_res_with_sharp, tmp_res_with_newline, tmp_res_with_semicolon) in \\\n",
        "#                 zip(tmp_content, tmp_res_with_dot_batch, tmp_res_with_sharp_batch, tmp_res_with_newline_batch, tmp_res_with_semicolon_batch):\n",
        "#                 # \"\"\" check the length of multiple coreference resolution outputs are the same \"\"\"\n",
        "#                 assert len(tmp_res_with_dot['document']) == len(tmp_res_with_sharp['document'])\n",
        "#                 assert len(tmp_res_with_newline['document']) == len(tmp_res_with_sharp['document'])\n",
        "#                 assert len(tmp_res_with_semicolon['document']) == len(tmp_res_with_sharp['document'])\n",
        "                \n",
        "#                 tmp_res_with_dot['document'] = tmp_res_with_sharp['document']\n",
        "#                 tmp_res_with_newline['document'] = tmp_res_with_sharp['document']\n",
        "#                 tmp_res_with_semicolon['document'] = tmp_res_with_sharp['document']\n",
        "\n",
        "#                 \"\"\" ensemble multiple coreference resolution outputs \"\"\"\n",
        "#                 output_list.append({\"dot\": (dialogue, tmp_res_with_dot),\n",
        "#                                     \"sharp\": (dialogue, tmp_res_with_sharp),\n",
        "#                                     \"newline\": (dialogue, tmp_res_with_newline),\n",
        "#                                     \"semicolon\": (dialogue, tmp_res_with_semicolon)}, )\n",
        "#             # output_list[\"dot\"].extend(tmp_res_with_dot_batch)\n",
        "#             # output_list[\"sharp\"].extend(tmp_res_with_sharp_batch)\n",
        "#             # output_list[\"newline\"].extend(tmp_res_with_newline_batch)\n",
        "#             # output_list[\"semicolon\"].extend(tmp_res_with_semicolon_batch)\n",
        "\n",
        "#         return output_list"
      ],
      "metadata": {
        "id": "C4zMAwu794rV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import copy\n",
        "# from tqdm import tqdm\n",
        "# import re\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# def Prev_Coreference_Matrix(token_length, src_list, tgt_list):\n",
        "#     \"\"\" build the prev-linked coreference matrix \"\"\"\n",
        "#     coref_matrix = np.zeros([token_length, token_length], dtype=float)\n",
        "#     assert len(src_list) == len(tgt_list)\n",
        "#     for i in range(len(src_list)):\n",
        "#         coref_matrix[src_list[i]][tgt_list[i]] = 1\n",
        "#     for i in range(token_length):\n",
        "#         if sum(coref_matrix[i]) == 0:\n",
        "#             coref_matrix[i][i] = 1\n",
        "#     return coref_matrix\n",
        "\n",
        "\n",
        "# class BuildSampleWithCoreferenceInfo:\n",
        "#     def __init__(self, tokenizer):\n",
        "#         \"\"\" Here we use the tokenizer from BART \"\"\"\n",
        "#         self.global_tokenizer = tokenizer\n",
        "\n",
        "#     def build_sample_with_coref_to_file(self, input_list, aux_condition_name_file=None, conditional_file_path=None, debug=False):\n",
        "#         \"\"\"\n",
        "#         :param aux_condition_name_file: each row will contain the speaker roles / personal named entities\n",
        "#         :param input_list: the list of conversations\n",
        "#         :param conditional_file_path: the list of conversations with conditional planning\n",
        "#         :param debug: for debug print\n",
        "#         :return: directly write a file.\n",
        "#         \"\"\"\n",
        "\n",
        "#         if conditional_file_path is not None:\n",
        "#             conditional_line_list = open(conditional_file_path, encoding=\"utf-8\").readlines()\n",
        "#             assert len(conditional_line_list) == len(input_list)\n",
        "\n",
        "#         output_list = {'input_ids': [], 'coref_information': []}\n",
        "\n",
        "#         if aux_condition_name_file is not None:\n",
        "#             aux_name_list = open(aux_condition_name_file, encoding=\"utf-8\").readlines()\n",
        "#             assert len(aux_name_list) == len(input_list)\n",
        "#         else:\n",
        "#             aux_name_list = None\n",
        "\n",
        "#         tmp_line_idx = 0\n",
        "#         # for tmp_k, tmp_dict_node in tqdm(enumerate(input_list)):\n",
        "#         for tmp_k, tmp_dict_node in enumerate(input_list):\n",
        "#             \"\"\" we use the multiple coreference resolution outputs \"\"\"\n",
        "#             for coref_idx, coref_type in enumerate(['newline', 'dot', 'sharp', 'semicolon']):\n",
        "#                 tmp_i = tmp_dict_node[coref_type]\n",
        "#                 tmp_token_list = tmp_i[1][\"document\"]\n",
        "#                 tmp_clusters = tmp_i[1][\"clusters\"]\n",
        "\n",
        "#                 # print(tmp_i[0])\n",
        "#                 raw_coref_cluster_info = []\n",
        "#                 for k, i in enumerate(tmp_clusters):\n",
        "#                     one_list = [\" \".join(tmp_token_list[j[0]:j[1] + 1]) for j in i]\n",
        "#                     raw_coref_cluster_info.append((k, one_list))\n",
        "\n",
        "#                 \"\"\" Tackle the issue that some speaker names are not included in coreference chains \"\"\"\n",
        "#                 tmp_new_coref_cluster_info = copy.deepcopy(raw_coref_cluster_info)\n",
        "\n",
        "#                 tmp_titled_speakers = set([i[:-1] for i in tmp_i[0].split() if i[-1] == \":\" and i.istitle()])\n",
        "#                 tmp_speaker_label_dict = {}\n",
        "#                 for k, v in enumerate(tmp_titled_speakers):\n",
        "#                     tmp_cluster_res = [j[0] for j in tmp_new_coref_cluster_info if v in j[1]]\n",
        "#                     if len(tmp_cluster_res) < 1 and v not in tmp_speaker_label_dict.keys():\n",
        "#                         tmp_speaker_label_dict[v] = len(tmp_new_coref_cluster_info) + 30\n",
        "#                         tmp_new_coref_cluster_info.append((len(tmp_new_coref_cluster_info) + 30, [v]))\n",
        "#                     else:\n",
        "#                         if len(tmp_cluster_res) == 1:\n",
        "#                             tmp_speaker_label_dict[v] = tmp_cluster_res[0]\n",
        "#                         if len(tmp_cluster_res) > 1:\n",
        "#                             \"\"\" Here we select the first found token as the cluster label \"\"\"\n",
        "#                             q_list = [(q, tmp_clusters[q][tmp_new_coref_cluster_info[q][1].index(v)][0]) for q in tmp_cluster_res]\n",
        "#                             q_list = sorted(q_list, key=lambda x: x[1])\n",
        "#                             tmp_speaker_label_dict[v] = q_list[0][0]\n",
        "\n",
        "#                 if aux_name_list is not None:\n",
        "#                     assert len(re.findall(\"\\}\\s+\\#\", aux_name_list[tmp_k])) == 1\n",
        "#                     aux_one_cond_name_set = set(re.sub(\"[\\#\\.\\|\\{\\}]]\", \" \", aux_name_list[tmp_k].split(\"}\")[0]).split())\n",
        "#                 else:\n",
        "#                     aux_one_cond_name_set = set()\n",
        "\n",
        "#                 continue_flag = False\n",
        "#                 for tmp_item in tmp_new_coref_cluster_info:\n",
        "#                     tmp_small_set = set([i.strip().split()[0] for i in tmp_item[1]])\n",
        "#                     intersection = tmp_small_set & (set(tmp_titled_speakers) | set(aux_one_cond_name_set))\n",
        "#                     if len(intersection) > 1:\n",
        "#                         if len(set([i[:2] for i in intersection])) > 1:\n",
        "#                             if coref_idx == 3:\n",
        "#                                 print(\"one plausible coreference chain.\")\n",
        "#                             continue_flag = True\n",
        "\n",
        "#                 if continue_flag is False:\n",
        "#                     break\n",
        "\n",
        "#             \"\"\" Further add titled words to increase coverage \"\"\"\n",
        "#             tmp_titled_other_tokens = set([i for i in tmp_i[1][\"document\"] if len(i) > 2 and i.istitle()])\n",
        "#             for k, v in enumerate(tmp_titled_other_tokens):\n",
        "#                 tmp_cluster_res = [j[0] for j in tmp_new_coref_cluster_info if v in j[1]]\n",
        "#                 if len(tmp_cluster_res) < 1 and v not in tmp_speaker_label_dict.keys():\n",
        "#                     tmp_cluster_res = [(j[0], j[1].count(v)) for j in tmp_new_coref_cluster_info if v in \" \".join(j[1]).split()]\n",
        "#                     tmp_cluster_res = sorted(tmp_cluster_res, key=lambda x: x[1], reverse=True)\n",
        "#                     if len(tmp_cluster_res) > 0:\n",
        "#                         tmp_speaker_label_dict[v] = tmp_cluster_res[0][0]\n",
        "#                     else:\n",
        "#                         tmp_speaker_label_dict[v] = len(tmp_new_coref_cluster_info) + 100\n",
        "#                         tmp_new_coref_cluster_info.append((len(tmp_new_coref_cluster_info) + 100, [v]))\n",
        "\n",
        "#             \"\"\" Adding spaces in tokenized list, to recover the same tokenization via BART \"\"\"\n",
        "#             tmp_doc = copy.deepcopy(\" \" + tmp_i[0])\n",
        "\n",
        "#             tmp_token_list_with_space = []\n",
        "#             for k, v in enumerate(tmp_token_list):\n",
        "#                 find_idx = str(tmp_doc).index(v)\n",
        "#                 if find_idx > 0 and tmp_doc[find_idx - 1] == \" \":\n",
        "#                     tmp_token_list_with_space.append([\" \" + v, -1])\n",
        "#                 else:\n",
        "#                     tmp_token_list_with_space.append([v, -1])\n",
        "#                 tmp_doc = tmp_doc[find_idx + len(v):]\n",
        "\n",
        "#             \"\"\" Labeling the token list with the coreference cluster labels \"\"\"\n",
        "#             \"\"\" From the longer spans to shorter spans, to avoiding labels to be re-changed \"\"\"\n",
        "#             tmp_span_len_list = []\n",
        "#             for i in tmp_clusters:\n",
        "#                 tmp_span_len_list.extend([j[-1] + 1 - j[0] for j in i])\n",
        "\n",
        "#             tmp_span_len_list = list(set(tmp_span_len_list))\n",
        "#             tmp_span_len_list = sorted(tmp_span_len_list, reverse=True)\n",
        "\n",
        "#             for one_len in tmp_span_len_list:\n",
        "#                 for i in range(len(tmp_clusters)):\n",
        "#                     for j in tmp_clusters[i]:\n",
        "#                         if (j[1] + 1 - j[0]) == one_len:\n",
        "#                             for e in j:\n",
        "#                                 tmp_token_list_with_space[e][1] = i\n",
        "\n",
        "#             \"\"\" Tackle the issue that speaker names do not have coreference \"\"\"\n",
        "#             assert len(tmp_token_list_with_space) == len(tmp_token_list)\n",
        "#             for k in range(len(tmp_token_list_with_space)):\n",
        "#                 if (k == len(tmp_token_list_with_space) - 1 or tmp_token_list_with_space[k + 1][0].strip() == \":\") \\\n",
        "#                         and tmp_token_list_with_space[k][0].strip() in tmp_speaker_label_dict.keys() \\\n",
        "#                         and tmp_token_list_with_space[k][1] == -1:\n",
        "#                     tmp_token_list_with_space[k][1] = tmp_speaker_label_dict[tmp_token_list_with_space[k][0].strip()]\n",
        "#                     # print(tmp_token_list_with_space)\n",
        "\n",
        "#             \"\"\" Merge the token list with the same coreference cluster \"\"\"\n",
        "#             merged_tmp_token_list_with_space = []\n",
        "#             current_merge_set = []\n",
        "#             current_cluster_id_to_merge = -999\n",
        "#             for i in range(len(tmp_token_list_with_space)):\n",
        "#                 if tmp_token_list_with_space[i][1] == current_cluster_id_to_merge:\n",
        "#                     current_merge_set.append(tmp_token_list_with_space[i])\n",
        "#                     current_cluster_id_to_merge = tmp_token_list_with_space[i][1]\n",
        "#                 else:\n",
        "#                     if len(current_merge_set) > 0:\n",
        "#                         merged_tmp_token_list_with_space.append([[j[0] for j in current_merge_set], current_merge_set[0][1]])\n",
        "#                         current_merge_set = []\n",
        "#                     current_merge_set.append(tmp_token_list_with_space[i])\n",
        "#                     current_cluster_id_to_merge = tmp_token_list_with_space[i][1]\n",
        "#                 if i == len(tmp_token_list_with_space) - 1 and len(current_merge_set) > 0:\n",
        "#                     merged_tmp_token_list_with_space.append([[j[0] for j in current_merge_set], current_merge_set[0][1]])\n",
        "#                     current_merge_set = []\n",
        "\n",
        "#             # print(merged_tmp_token_list_with_space)\n",
        "\n",
        "#             \"\"\" Using the BART tokenizer to process the new token list \"\"\"\n",
        "#             for i in range(len(merged_tmp_token_list_with_space)):\n",
        "#                 merged_tmp_token_list_with_space[i][0] = self.global_tokenizer.tokenize(\"\".join(merged_tmp_token_list_with_space[i][0]))\n",
        "\n",
        "#             \"\"\" V2 only point to the first token of spans \"\"\"\n",
        "#             tmp_token_list_with_cluster_ids = []\n",
        "#             for i in merged_tmp_token_list_with_space:\n",
        "#                 for j in range(len(i[0])):\n",
        "#                     if j == 0:\n",
        "#                         tmp_token_list_with_cluster_ids.append([i[0][j], i[1]])\n",
        "#                     else:\n",
        "#                         tmp_token_list_with_cluster_ids.append([i[0][j], -1])\n",
        "\n",
        "#             if debug:\n",
        "#                 tmp_t = \" \".join(self.global_tokenizer.tokenize(tmp_i[0])).strip()\n",
        "#                 tmp_c = \" \".join([j[0] for j in tmp_token_list_with_cluster_ids]).strip()\n",
        "#                 print(\"\\n\", tmp_t, \"\\n\", tmp_c)\n",
        "\n",
        "#             \"\"\" Adding coreference of the conditional personal names \"\"\"\n",
        "#             if conditional_file_path is not None:\n",
        "#                 assert len(re.findall(\"\\}\\s+\\#\", conditional_line_list[tmp_k])) == 1\n",
        "#                 conditional_names = (conditional_line_list[tmp_k].split(\"}\")[0] + \"} #\").split()\n",
        "#                 conditional_names = [[i.strip(), -1] for i in conditional_names]\n",
        "\n",
        "#                 for k, v in enumerate(conditional_names):\n",
        "#                     if v[0] not in [\"{\", \"}\", \"#\", \"|\"]:\n",
        "#                         tmp_cluster_res = [(j[0], j[1].count(v[0])) for j in tmp_new_coref_cluster_info if v[0] in j[1]]\n",
        "#                         if len(tmp_cluster_res) > 0:\n",
        "#                             conditional_names[k][1] = tmp_cluster_res[0][0]\n",
        "#                             # if len(tmp_cluster_res) > 1:\n",
        "#                             #     print(tmp_cluster_res)\n",
        "#                         else:\n",
        "#                             \"\"\" splitting every name in the cluster keys, then find more names \"\"\"\n",
        "#                             tmp_cluster_res = [(j[0], j[1].count(v[0])) for j in tmp_new_coref_cluster_info if v[0] in \" \".join(j[1]).split()]\n",
        "\n",
        "#                             if len(tmp_cluster_res) > 0:\n",
        "#                                 conditional_names[k][1] = tmp_cluster_res[0][0]\n",
        "#                             else:\n",
        "#                                 \"\"\" To tackle the exception of names are not included \"\"\"\n",
        "#                                 tmp_new_coref_cluster_info.append((len(tmp_new_coref_cluster_info) + 50, [v[0]]))\n",
        "#                                 for n, i in enumerate(tmp_token_list_with_cluster_ids):\n",
        "#                                     if i[0][1:] == v[0] and i[1] == -1:\n",
        "#                                         tmp_token_list_with_cluster_ids[n][1] = tmp_new_coref_cluster_info[-1][0]\n",
        "#                                         conditional_names[k][1] = tmp_new_coref_cluster_info[-1][0]\n",
        "#                                 print(\"\\n\\n\\n\")\n",
        "#                                 print(tmp_i[0])\n",
        "#                                 print(v[0])\n",
        "#                                 print(tmp_token_list_with_cluster_ids)\n",
        "#                                 pass\n",
        "\n",
        "#                 print(conditional_names)\n",
        "#                 condition_prefix = conditional_names\n",
        "#                 tmp_prefix = []\n",
        "#                 for i in condition_prefix:\n",
        "#                     tmp_t = self.global_tokenizer.tokenize(\" \" + i[0])\n",
        "#                     for j in range(len(tmp_t)):\n",
        "#                         if j == 0:\n",
        "#                             tmp_prefix.append((tmp_t[j], i[1]))\n",
        "#                         else:\n",
        "#                             tmp_prefix.append((tmp_t[j], -1))\n",
        "\n",
        "#                 tmp_token_list_with_cluster_ids = tmp_prefix + tmp_token_list_with_cluster_ids\n",
        "\n",
        "#             else:\n",
        "#                 tmp_token_list_with_cluster_ids = [['#', -1]] + tmp_token_list_with_cluster_ids\n",
        "\n",
        "#             \"\"\" Build the src list ang tgt list for DGL GNN implementation \"\"\"\n",
        "#             src_list = []\n",
        "#             tgt_list = []\n",
        "#             text_input_list = []\n",
        "#             for k, v in enumerate(tmp_token_list_with_cluster_ids):\n",
        "#                 text_input_list.append(v[0])\n",
        "#                 if v[1] != -1:\n",
        "#                     find_precedent = [j for j in range(k) if tmp_token_list_with_cluster_ids[j][1] == v[1]]\n",
        "#                     if len(find_precedent) > 0:\n",
        "#                         src_list.append(k)\n",
        "#                         tgt_list.append(max(find_precedent))\n",
        "\n",
        "#             assert len(src_list) == len(tgt_list)\n",
        "\n",
        "#             tmp_line_idx += 1\n",
        "\n",
        "#             \"\"\" Truncating the lengthy samples \"\"\"\n",
        "#             if len(text_input_list) > 1023:\n",
        "#                 print(\"Truncate the lengthy sample:\", len(text_input_list))\n",
        "#                 cut_num = len([i for i in src_list if i > 1022])\n",
        "#                 print(cut_num)\n",
        "#                 src_list = src_list[:-cut_num]\n",
        "#                 tgt_list = tgt_list[:-cut_num]\n",
        "#                 print(src_list)\n",
        "#                 print(tgt_list)\n",
        "#                 assert len(src_list) == len(tgt_list)\n",
        "\n",
        "#             \"\"\" We write all information as a text file \"\"\"\n",
        "#             text_input_list = text_input_list[:1023]\n",
        "#             # output_fp.write(\" \".join(text_input_list) + \" ##### \" + str(self.global_tokenizer.convert_tokens_to_ids(text_input_list)) + \\\n",
        "#             #                 \" ##### \" + str(src_list) + \" ##### \" + str(tgt_list) + \" ##### \" + str(len(text_input_list)) + \"\\n\")\n",
        "#             output_list[\"input_ids\"].append(self.global_tokenizer.convert_tokens_to_ids(text_input_list))\n",
        "#             output_list[\"coref_information\"].append((src_list, tgt_list))\n",
        "\n",
        "#             assert len(text_input_list) == len(self.global_tokenizer.convert_tokens_to_ids(text_input_list))\n",
        "#             assert len(self.global_tokenizer.convert_tokens_to_ids(text_input_list)) < 1024\n",
        "\n",
        "#             if debug:\n",
        "#                 for k, v in enumerate(text_input_list):\n",
        "#                     if k in src_list:\n",
        "#                         print(\">>>>>>>> \", v.replace(\"Ġ\", \"\"), k, tgt_list[src_list.index(k)])\n",
        "#                     else:\n",
        "#                         print(v.replace(\"Ġ\", \"\"), k, \"X\")\n",
        "#                 tmp_matrix = Prev_Coreference_Matrix(len(text_input_list), src_list, tgt_list)\n",
        "#                 print(tmp_matrix)\n",
        "\n",
        "#         return output_list"
      ],
      "metadata": {
        "id": "Ef1gw6jMIpVp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_target_length = 128\n",
        "# def preprocess_function(examples):\n",
        "#     # model_inputs = tokenizer(examples[\"dialogue\"], max_length=max_input_length, truncation=True)\n",
        "#     result = coref_model.process(examples[\"dialogue\"], batch_size=4)\n",
        "#     model_inputs = coref_build.build_sample_with_coref_to_file(result)\n",
        "\n",
        "#     # Setup the tokenizer for targets\n",
        "#     labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs"
      ],
      "metadata": {
        "id": "LRjZuNhcEkKr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processed_dataset = raw_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "CQxnCxAEEem6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # save coreference dataset, prefix: ['bart', 't5-base']\n",
        "# prefix = 'bart'\n",
        "# coref_datasets = save_processed_coref_dataset(prefix, processed_datasets)"
      ],
      "metadata": {
        "id": "zLGQCAn-VMz-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bart"
      ],
      "metadata": {
        "id": "lb64T7l6aZ62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load saved coreference dataset and preprocess"
      ],
      "metadata": {
        "id": "HZ5sOE9lc0Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load coreference dataset, prefix: ['bart', 't5-base']\n",
        "prefix = 'bart'\n",
        "coref_datasets = load_processed_coref_dataset(prefix)\n",
        "\n",
        "model_checkpoint = \"facebook/bart-base\""
      ],
      "metadata": {
        "id": "DQH1BDp1OFGQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "v5KShfXKaSUs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_dataset = raw_datasets['train'].map(add_preprocessed_data_train, with_indices=True)\n",
        "processed_test_dataset = raw_datasets['test'].map(add_preprocessed_data_test, with_indices=True)\n",
        "processed_validation_dataset = raw_datasets['validation'].map(add_preprocessed_data_validation, with_indices=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "d7e1e2df2f7e4c09b8d2940588f1144c",
            "28274ac53aa647049fe38b0bf83c43dc",
            "1429c5ede33d48e8ad210b3cd7908647",
            "2dbb864acf9542cfa5f2dad34bc7bf3c",
            "4b0e997f43cc49cebba270b9d2861a35",
            "6772cee92c4f4936ab9daa64fdffc844",
            "1fe24a6c14bb4f7c9ecef3f9816c2085",
            "f185a6b300c64b72819ba10a87798434",
            "9a85165734d241de83ba29e4418159ab",
            "160c9e6889a444a5a463b983253605b8",
            "a0e193a9312445ab84c87f3ff5265add",
            "fc8c97708f294973bcc58b42f32c2ded",
            "59800ad4f99d4638862cd759d5a54a9b",
            "3da7c03fa0a6455fbc78997b42df7d1e",
            "0efd944077da4da691bf2ffa5e0a5f6e",
            "dc1e641336a646b3961441c7e95571bd",
            "6ab73871eb35404eb0daff146425c3fe",
            "eb7c7da187734c0bb0f2be6cda6f8000",
            "aca43c6b0a6e40119ae7f4544d04d631",
            "847dec1df87d443aae67c9e2530ef1b6",
            "6207a07ec89c4532964bad4a240b9850",
            "26a50f56accd474a939ea282e1381bc4",
            "c15f016a9ba843d1ba366b92ca170a97",
            "5f197b73f3644495a97cee13f85246ad",
            "5a889f5da40f4ee0acec57ae5b06c81c",
            "304f58fe83264c2dae86557663955cef",
            "06c1b4ad54c747f0b63053136e1d5ab9",
            "4496710adfab404995633675f1fafffc",
            "72327ca87dcc47278e34c600f7a0c2a8",
            "de906135707f4453a3504cd81749f229",
            "537aba1aec51464ea941d6bc9a11924a",
            "d3f84f0349ad4081bd44c7d550074fca",
            "0e4ad84fdb7044ed800bd0dc3f6b4a40"
          ]
        },
        "id": "wSd-AtnzJ9OF",
        "outputId": "26704c6f-ecea-4023-838e-526cb12ce476"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.fingerprint:Parameter 'function'=<function add_preprocessed_data_train at 0x7f833c4d89d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/14731 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7e1e2df2f7e4c09b8d2940588f1144c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function tqdm.__del__ at 0x7f8465529ee0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tqdm/std.py\", line 1161, in __del__\n",
            "    def __del__(self):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/819 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc8c97708f294973bcc58b42f32c2ded"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/818 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c15f016a9ba843d1ba366b92ca170a97"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "processed_datasets = copy.deepcopy(raw_datasets)"
      ],
      "metadata": {
        "id": "FNFQlrH1QQCE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_datasets['train'] = processed_train_dataset\n",
        "processed_datasets['test'] = processed_test_dataset\n",
        "processed_datasets['validation'] = processed_validation_dataset"
      ],
      "metadata": {
        "id": "M0TuYBrNK9zN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_coref_datasets = processed_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "sOn8cBijbUTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "4JdkaXHYaPGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(model_checkpoint, output_hidden_states=False)"
      ],
      "metadata": {
        "id": "onJ19HiKZPEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7olpPP6acJ8"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    \"16-coref-bart-dialogue-summarization\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    # gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    # save_total_limit=2,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps = 10, ## added\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    generation_max_length=max_target_length,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtDg-_hqacJ9"
      },
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_coref_datasets_train = tokenized_coref_datasets['train']\n",
        "tokenized_coref_dataset_val = tokenized_coref_datasets['validation']"
      ],
      "metadata": {
        "id": "pTikd6_UdPfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieroGHrLacJ9"
      },
      "source": [
        "trainer = CustomTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_coref_datasets_train,\n",
        "    eval_dataset=tokenized_coref_dataset_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmF4susRacJ9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4RKBztwacJ9"
      },
      "source": [
        "trainer.evaluate() #before training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MGB4InuUOlmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "75kctJToPZmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save result"
      ],
      "metadata": {
        "id": "MkX0amRL4kt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = trainer.predict(tokenized_coref_dataset_val, max_length=128)"
      ],
      "metadata": {
        "id": "V9C7HJhMp3Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"result/bart-coref-16-generation.txt\"\n",
        "batch_result = tokenizer.batch_decode(prediction.predictions, skip_special_tokens=True)\n",
        "save_result(batch_result, path)"
      ],
      "metadata": {
        "id": "XhTUA4oeo0Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "QXWd9U-Kaf19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load saved coreference dataset and preprocess"
      ],
      "metadata": {
        "id": "4ncnXECAc977"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load coreference dataset, prefix: ['bart', 't5-base']\n",
        "prefix = 't5-base'\n",
        "coref_datasets = load_processed_coref_dataset(prefix)\n",
        "\n",
        "model_checkpoint = \"t5-base\""
      ],
      "metadata": {
        "id": "8vta7nfVc977"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_lenght=max_input_length)"
      ],
      "metadata": {
        "id": "uerm1MCac978"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_dataset = raw_datasets['train'].map(add_preprocessed_data_train, with_indices=True)\n",
        "processed_test_dataset = raw_datasets['test'].map(add_preprocessed_data_test, with_indices=True)\n",
        "processed_validation_dataset = raw_datasets['validation'].map(add_preprocessed_data_validation, with_indices=True)"
      ],
      "metadata": {
        "id": "48vqP4XTc977"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "processed_datasets = copy.deepcopy(raw_datasets)"
      ],
      "metadata": {
        "id": "i6vlvemCc978"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_datasets['train'] = processed_train_dataset\n",
        "processed_datasets['test'] = processed_test_dataset\n",
        "processed_datasets['validation'] = processed_validation_dataset"
      ],
      "metadata": {
        "id": "q24CcCd9c978"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_coref_datasets = processed_datasets.map(preprocess_function_t5, batched=True)"
      ],
      "metadata": {
        "id": "re8AtVUsc978"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "WQ2MdnXpc979"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint, output_hidden_states=False)"
      ],
      "metadata": {
        "id": "SswKqs42c979"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDAsIvhHc979"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    \"16-coref-t5-dialogue-summarization\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    # gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    # save_total_limit=2,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps = 10, ## added\n",
        "    predict_with_generate=True,\n",
        "    # fp16=True,\n",
        "    report_to=\"none\",\n",
        "    generation_max_length=max_target_length,\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlTqZZf2c979"
      },
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_coref_datasets_train = tokenized_coref_datasets['train']\n",
        "tokenized_coref_dataset_val = tokenized_coref_datasets['validation']"
      ],
      "metadata": {
        "id": "ZOv5dBplc979"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL8ikXD0c97-"
      },
      "source": [
        "trainer = CustomTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_coref_datasets_train,\n",
        "    eval_dataset=tokenized_coref_dataset_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd909fa-cbcb-4ef1-891e-a5c195561fd8",
        "id": "AV5jLUOEc97-"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br3p05uqc97-"
      },
      "source": [
        "trainer.evaluate() #before training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3139bf0f-c787-4382-c9e2-951a03079566",
        "id": "pGvA0gcRc97-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 14731\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 9210\n",
            "  Number of trainable parameters = 222903552\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9120' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9120/9210 52:44 < 00:31, 2.88 it/s, Epoch 4.95/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.626800</td>\n",
              "      <td>1.470832</td>\n",
              "      <td>47.730900</td>\n",
              "      <td>23.026400</td>\n",
              "      <td>39.567700</td>\n",
              "      <td>43.875300</td>\n",
              "      <td>23.199300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.569900</td>\n",
              "      <td>1.432313</td>\n",
              "      <td>49.059500</td>\n",
              "      <td>24.073100</td>\n",
              "      <td>40.414100</td>\n",
              "      <td>44.968900</td>\n",
              "      <td>23.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.593000</td>\n",
              "      <td>1.416316</td>\n",
              "      <td>49.780900</td>\n",
              "      <td>24.694000</td>\n",
              "      <td>40.911400</td>\n",
              "      <td>45.703500</td>\n",
              "      <td>24.231100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.450300</td>\n",
              "      <td>1.410689</td>\n",
              "      <td>49.947000</td>\n",
              "      <td>24.986000</td>\n",
              "      <td>41.241400</td>\n",
              "      <td>46.048100</td>\n",
              "      <td>24.677300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-500/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-1000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-1000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-1000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-1500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-1500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-1500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-2000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-2000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-2000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-2500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-2500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-2500/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-3000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-3000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-3000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-3500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-3500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-3500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-4000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-4000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-4000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-4500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-4500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-4500/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-5000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-5000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-5000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-5500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-5500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-5500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-6000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-6000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-6000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-6500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-6500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-6500/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-7000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-7000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-7000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-7500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-7500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-7500/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-8000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-8000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-8000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-8500\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-8500/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-8500/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-t5-dialogue-summarization/checkpoint-9000\n",
            "Configuration saved in 16-coref-t5-dialogue-summarization/checkpoint-9000/config.json\n",
            "Model weights saved in 16-coref-t5-dialogue-summarization/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-t5-dialogue-summarization/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-t5-dialogue-summarization/checkpoint-9000/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "rCgqlbsqc97-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save result"
      ],
      "metadata": {
        "id": "i-gbT7NFc97-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = trainer.predict(tokenized_coref_dataset_val, max_length=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "2aa210a2-f4da-41de-dae0-7eda82d3a6ec",
        "id": "ZHawSTv2c97-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 819\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='412' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [103/103 28:36]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"result/t5-generation.txt\"\n",
        "batch_result = tokenizer.batch_decode(prediction.predictions, skip_special_tokens=True)\n",
        "save_result(batch_result, path)"
      ],
      "metadata": {
        "id": "TYv7vIPvc97-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}