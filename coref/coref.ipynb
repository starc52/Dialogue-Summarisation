{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "545PP3o8IrJV",
        "EGgU3K1eSZYo",
        "fZNKwHmTpjxX"
      ],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63a5a682c1ab4196a24eef4013ef7e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15ca160474454247baa35cc858a40375",
              "IPY_MODEL_abf314fef2294f55a08c8c743ffe5e14",
              "IPY_MODEL_14860c08cf4b4fbeb471e88616f230a4"
            ],
            "layout": "IPY_MODEL_87af2e08c3484275a96bf8dc6066c78b"
          }
        },
        "15ca160474454247baa35cc858a40375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54376cc1520c4861aafb48bc067dbfd5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4e499d1818d74ca880271e234f1b7973",
            "value": "100%"
          }
        },
        "abf314fef2294f55a08c8c743ffe5e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ea45dcabeff4a0d803ef11517d85063",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6edd8a65b76a4e668bc892cfd0907885",
            "value": 3
          }
        },
        "14860c08cf4b4fbeb471e88616f230a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c61756c9e004978b06c86a165d7a26a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_33ef762db09d4aaeb62eeaa812bf196c",
            "value": " 3/3 [00:00&lt;00:00, 109.64it/s]"
          }
        },
        "87af2e08c3484275a96bf8dc6066c78b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54376cc1520c4861aafb48bc067dbfd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e499d1818d74ca880271e234f1b7973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ea45dcabeff4a0d803ef11517d85063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6edd8a65b76a4e668bc892cfd0907885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c61756c9e004978b06c86a165d7a26a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ef762db09d4aaeb62eeaa812bf196c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RVtm1XK0BxsO",
        "outputId": "963db205-f777-4b84-ea74-b22b9c61958b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  4 22:29:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    53W / 400W |   9134MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e38390f-78ea-46df-b21d-bd29ad068b5f"
      },
      "source": [
        "! pip install datasets transformers rouge-score nltk py7zr allennlp allennlp_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.20.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.8/dist-packages (0.20.2)\n",
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.8/dist-packages (2.10.1)\n",
            "Requirement already satisfied: allennlp_models in /usr/local/lib/python3.8/dist-packages (2.10.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.6.7)\n",
            "Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.8/dist-packages (from py7zr) (0.15.3)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.8/dist-packages (from py7zr) (0.2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from py7zr) (5.4.8)\n",
            "Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.0.9)\n",
            "Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from py7zr) (0.3.1)\n",
            "Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.0.1)\n",
            "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.0.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from py7zr) (3.16.0)\n",
            "Requirement already satisfied: lmdb>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.3.0)\n",
            "Requirement already satisfied: spacy<3.4,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.3.1)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.12.21)\n",
            "Requirement already satisfied: fairscale==0.4.6 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.4.6)\n",
            "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.1.6)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.7.0)\n",
            "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (5.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.0.53)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.8/dist-packages (from allennlp) (2.5.1)\n",
            "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.1.97)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.0.2)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.4.2)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.19.6)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.19.1)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.8/dist-packages (from allennlp) (7.2.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.13.1+cu113)\n",
            "Requirement already satisfied: base58>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (2.1.1)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.7.3)\n",
            "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (9.0.0)\n",
            "Requirement already satisfied: rich<13.0,>=12.1 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (12.6.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.5.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.26.22)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.22 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.29.22)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.22->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.57.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.2.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (1.1.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (1.0.4)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (0.9.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.8.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (8.0.17)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp) (5.2.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.29)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.11)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.9.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (6.1.1)\n",
            "Requirement already satisfied: conllu==4.4.2 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (4.4.2)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (1.1)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->allennlp_models) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e-v6Mjk_JvG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840ea469-855d-4788-f2ae-d9bc3b43ec1f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5nOd8wBL-BO",
        "outputId": "304580c2-c8a7-49a6-ca86-1c0883cffc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/NLP Project/generation_utils.py\" /usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\n",
        "!cp \"/content/drive/MyDrive/NLP Project/tokenization_utils_base.py\" /usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\n",
        "!cp \"/content/drive/MyDrive/NLP Project/trainer_seq2seq.py\" /usr/local/lib/python3.8/dist-packages/transformers/trainer_seq2seq.py"
      ],
      "metadata": {
        "id": "k0eiGcjV89P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a summarization task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers.modeling_utils import unwrap_model\n",
        "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, load_metric\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from modeling_bart import BartForConditionalGeneration  # Custom coref bart\n"
      ],
      "metadata": {
        "id": "ZetvZ8yuGtkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreSlFmlIrIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "63a5a682c1ab4196a24eef4013ef7e5b",
            "15ca160474454247baa35cc858a40375",
            "abf314fef2294f55a08c8c743ffe5e14",
            "14860c08cf4b4fbeb471e88616f230a4",
            "87af2e08c3484275a96bf8dc6066c78b",
            "54376cc1520c4861aafb48bc067dbfd5",
            "4e499d1818d74ca880271e234f1b7973",
            "0ea45dcabeff4a0d803ef11517d85063",
            "6edd8a65b76a4e668bc892cfd0907885",
            "7c61756c9e004978b06c86a165d7a26a",
            "33ef762db09d4aaeb62eeaa812bf196c"
          ]
        },
        "outputId": "af28406e-b123-4132-bf8a-95efb06d1c6b"
      },
      "source": [
        "raw_datasets = load_dataset(\"samsum\")\n",
        "raw_datasets = raw_datasets.filter(lambda data:data['dialogue'] != \"\")\n",
        "metric = load_metric(\"rouge\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a5a682c1ab4196a24eef4013ef7e5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-d1fc38f963a75d34.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-0f4686307978eada.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-31d8585b85287fcf.arrow\n",
            "<ipython-input-7-1fa46ba57893>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"rouge\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class CustomTrainer(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "        # Subclass and override for custom behavior.\n",
        "      \n",
        "        if self.label_smoother is not None and \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # Save past state if it exists\n",
        "        # TODO: this needs to be fixed and made cleaner later.\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = outputs[self.args.past_index]\n",
        "\n",
        "        if labels is not None:\n",
        "            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
        "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
        "            else:\n",
        "                loss = self.label_smoother(outputs, labels)\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            cross_loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "            loss += cross_loss\n",
        "        else:\n",
        "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
        "                raise ValueError(\n",
        "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
        "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
        "                )\n",
        "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
        "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "        # Adding CrossEntropyLoss()\n",
        "        # loss_fct = nn.CrossEntropyLoss()\n",
        "        # cross_loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        # loss += cross_loss\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "eIefc8iBq_zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coreference"
      ],
      "metadata": {
        "id": "1YftpcArAVVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing functions"
      ],
      "metadata": {
        "id": "LSGuhaPN4Syw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # code from https://github.com/seq-to-mind/coref_dial_summ modified.\n",
        "# import re\n",
        "# import pickle\n",
        "# from tqdm import tqdm\n",
        "# from allennlp.predictors.predictor import Predictor\n",
        "# import allennlp_models.coref\n",
        "# from allennlp_models import pretrained\n",
        "\n",
        "\n",
        "# class NeuralCoreferenceProcessing:\n",
        "#     def __init__(self, gpu_id=-1):\n",
        "#         \"\"\" download and indicate the path of pre-trained coref-spanbert model \"\"\"\n",
        "#         # self.predictor = Predictor.from_path(\"/content/drive/MyDrive/coref-spanbert-large-2021.03.10.tar.gz.tar\", cuda_device=gpu_id)\n",
        "#         self.predictor = pretrained.load_predictor(\"coref-spanbert\", cuda_device=gpu_id)\n",
        "\n",
        "#     def process(self, input_list, batch_size=4):\n",
        "#         # output_list = {\"dot\": [], \"sharp\": [], \"newline\": [], \"semicolon\": []}\n",
        "#         output_list = []\n",
        "#         dataloader = DataLoader(input_list, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#         # for tmp_content in tqdm(input_list):\n",
        "#         # for tmp_content in input_list:\n",
        "#         for tmp_content in dataloader:\n",
        "#             tmp_content = [dialogue.replace(\"#\", \" \").replace(\"\\r\\n\", \" # \").replace(\"\\n\", \" \").replace(\"ðŸ™‚\", \" \") \n",
        "#                             for dialogue in tmp_content]\n",
        "#             tmp_content = [re.sub(\"\\s+\", \" \", dialogue).strip() for dialogue in tmp_content]\n",
        "\n",
        "#             \"\"\" here we replace the sentence segmenter, to obtain multiple coreference resolution outputs \"\"\"\n",
        "#             tmp_res_with_dot_batch = self.predictor.predict_batch_json([{\"document\": dialogue.replace(\"#\", \".\")} for dialogue in tmp_content])\n",
        "#             tmp_res_with_sharp_batch = self.predictor.predict_batch_json({\"document\": dialogue} for dialogue in tmp_content)\n",
        "#             tmp_res_with_newline_batch = self.predictor.predict_batch_json([{\"document\": dialogue.replace(\"#\", \"\\n\")} for dialogue in tmp_content])\n",
        "#             tmp_res_with_semicolon_batch = self.predictor.predict_batch_json([{\"document\": dialogue.replace(\"#\", \";\")} for dialogue in tmp_content])\n",
        "\n",
        "#             # # \"\"\" check the length of multiple coreference resolution outputs are the same \"\"\"\n",
        "#             # assert len(tmp_res_with_dot['document']) == len(tmp_res_with_sharp['document'])\n",
        "#             # assert len(tmp_res_with_newline['document']) == len(tmp_res_with_sharp['document'])\n",
        "#             # assert len(tmp_res_with_semicolon['document']) == len(tmp_res_with_sharp['document'])\n",
        "\n",
        "#             for (dialogue, tmp_res_with_dot, tmp_res_with_sharp, tmp_res_with_newline, tmp_res_with_semicolon) in \\\n",
        "#                 zip(tmp_content, tmp_res_with_dot_batch, tmp_res_with_sharp_batch, tmp_res_with_newline_batch, tmp_res_with_semicolon_batch):\n",
        "#                 # \"\"\" check the length of multiple coreference resolution outputs are the same \"\"\"\n",
        "#                 assert len(tmp_res_with_dot['document']) == len(tmp_res_with_sharp['document'])\n",
        "#                 assert len(tmp_res_with_newline['document']) == len(tmp_res_with_sharp['document'])\n",
        "#                 assert len(tmp_res_with_semicolon['document']) == len(tmp_res_with_sharp['document'])\n",
        "                \n",
        "#                 tmp_res_with_dot['document'] = tmp_res_with_sharp['document']\n",
        "#                 tmp_res_with_newline['document'] = tmp_res_with_sharp['document']\n",
        "#                 tmp_res_with_semicolon['document'] = tmp_res_with_sharp['document']\n",
        "\n",
        "#                 \"\"\" ensemble multiple coreference resolution outputs \"\"\"\n",
        "#                 output_list.append({\"dot\": (dialogue, tmp_res_with_dot),\n",
        "#                                     \"sharp\": (dialogue, tmp_res_with_sharp),\n",
        "#                                     \"newline\": (dialogue, tmp_res_with_newline),\n",
        "#                                     \"semicolon\": (dialogue, tmp_res_with_semicolon)}, )\n",
        "#             # output_list[\"dot\"].extend(tmp_res_with_dot_batch)\n",
        "#             # output_list[\"sharp\"].extend(tmp_res_with_sharp_batch)\n",
        "#             # output_list[\"newline\"].extend(tmp_res_with_newline_batch)\n",
        "#             # output_list[\"semicolon\"].extend(tmp_res_with_semicolon_batch)\n",
        "\n",
        "#         return output_list"
      ],
      "metadata": {
        "id": "C4zMAwu794rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import copy\n",
        "# from tqdm import tqdm\n",
        "# import re\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# def Prev_Coreference_Matrix(token_length, src_list, tgt_list):\n",
        "#     \"\"\" build the prev-linked coreference matrix \"\"\"\n",
        "#     coref_matrix = np.zeros([token_length, token_length], dtype=float)\n",
        "#     assert len(src_list) == len(tgt_list)\n",
        "#     for i in range(len(src_list)):\n",
        "#         coref_matrix[src_list[i]][tgt_list[i]] = 1\n",
        "#     for i in range(token_length):\n",
        "#         if sum(coref_matrix[i]) == 0:\n",
        "#             coref_matrix[i][i] = 1\n",
        "#     return coref_matrix\n",
        "\n",
        "\n",
        "# class BuildSampleWithCoreferenceInfo:\n",
        "#     def __init__(self, tokenizer):\n",
        "#         \"\"\" Here we use the tokenizer from BART \"\"\"\n",
        "#         self.global_tokenizer = tokenizer\n",
        "\n",
        "#     def build_sample_with_coref_to_file(self, input_list, aux_condition_name_file=None, conditional_file_path=None, debug=False):\n",
        "#         \"\"\"\n",
        "#         :param aux_condition_name_file: each row will contain the speaker roles / personal named entities\n",
        "#         :param input_list: the list of conversations\n",
        "#         :param conditional_file_path: the list of conversations with conditional planning\n",
        "#         :param debug: for debug print\n",
        "#         :return: directly write a file.\n",
        "#         \"\"\"\n",
        "\n",
        "#         if conditional_file_path is not None:\n",
        "#             conditional_line_list = open(conditional_file_path, encoding=\"utf-8\").readlines()\n",
        "#             assert len(conditional_line_list) == len(input_list)\n",
        "\n",
        "#         output_list = {'input_ids': [], 'coref_information': []}\n",
        "\n",
        "#         if aux_condition_name_file is not None:\n",
        "#             aux_name_list = open(aux_condition_name_file, encoding=\"utf-8\").readlines()\n",
        "#             assert len(aux_name_list) == len(input_list)\n",
        "#         else:\n",
        "#             aux_name_list = None\n",
        "\n",
        "#         tmp_line_idx = 0\n",
        "#         # for tmp_k, tmp_dict_node in tqdm(enumerate(input_list)):\n",
        "#         for tmp_k, tmp_dict_node in enumerate(input_list):\n",
        "#             \"\"\" we use the multiple coreference resolution outputs \"\"\"\n",
        "#             for coref_idx, coref_type in enumerate(['newline', 'dot', 'sharp', 'semicolon']):\n",
        "#                 tmp_i = tmp_dict_node[coref_type]\n",
        "#                 tmp_token_list = tmp_i[1][\"document\"]\n",
        "#                 tmp_clusters = tmp_i[1][\"clusters\"]\n",
        "\n",
        "#                 # print(tmp_i[0])\n",
        "#                 raw_coref_cluster_info = []\n",
        "#                 for k, i in enumerate(tmp_clusters):\n",
        "#                     one_list = [\" \".join(tmp_token_list[j[0]:j[1] + 1]) for j in i]\n",
        "#                     raw_coref_cluster_info.append((k, one_list))\n",
        "\n",
        "#                 \"\"\" Tackle the issue that some speaker names are not included in coreference chains \"\"\"\n",
        "#                 tmp_new_coref_cluster_info = copy.deepcopy(raw_coref_cluster_info)\n",
        "\n",
        "#                 tmp_titled_speakers = set([i[:-1] for i in tmp_i[0].split() if i[-1] == \":\" and i.istitle()])\n",
        "#                 tmp_speaker_label_dict = {}\n",
        "#                 for k, v in enumerate(tmp_titled_speakers):\n",
        "#                     tmp_cluster_res = [j[0] for j in tmp_new_coref_cluster_info if v in j[1]]\n",
        "#                     if len(tmp_cluster_res) < 1 and v not in tmp_speaker_label_dict.keys():\n",
        "#                         tmp_speaker_label_dict[v] = len(tmp_new_coref_cluster_info) + 30\n",
        "#                         tmp_new_coref_cluster_info.append((len(tmp_new_coref_cluster_info) + 30, [v]))\n",
        "#                     else:\n",
        "#                         if len(tmp_cluster_res) == 1:\n",
        "#                             tmp_speaker_label_dict[v] = tmp_cluster_res[0]\n",
        "#                         if len(tmp_cluster_res) > 1:\n",
        "#                             \"\"\" Here we select the first found token as the cluster label \"\"\"\n",
        "#                             q_list = [(q, tmp_clusters[q][tmp_new_coref_cluster_info[q][1].index(v)][0]) for q in tmp_cluster_res]\n",
        "#                             q_list = sorted(q_list, key=lambda x: x[1])\n",
        "#                             tmp_speaker_label_dict[v] = q_list[0][0]\n",
        "\n",
        "#                 if aux_name_list is not None:\n",
        "#                     assert len(re.findall(\"\\}\\s+\\#\", aux_name_list[tmp_k])) == 1\n",
        "#                     aux_one_cond_name_set = set(re.sub(\"[\\#\\.\\|\\{\\}]]\", \" \", aux_name_list[tmp_k].split(\"}\")[0]).split())\n",
        "#                 else:\n",
        "#                     aux_one_cond_name_set = set()\n",
        "\n",
        "#                 continue_flag = False\n",
        "#                 for tmp_item in tmp_new_coref_cluster_info:\n",
        "#                     tmp_small_set = set([i.strip().split()[0] for i in tmp_item[1]])\n",
        "#                     intersection = tmp_small_set & (set(tmp_titled_speakers) | set(aux_one_cond_name_set))\n",
        "#                     if len(intersection) > 1:\n",
        "#                         if len(set([i[:2] for i in intersection])) > 1:\n",
        "#                             if coref_idx == 3:\n",
        "#                                 print(\"one plausible coreference chain.\")\n",
        "#                             continue_flag = True\n",
        "\n",
        "#                 if continue_flag is False:\n",
        "#                     break\n",
        "\n",
        "#             \"\"\" Further add titled words to increase coverage \"\"\"\n",
        "#             tmp_titled_other_tokens = set([i for i in tmp_i[1][\"document\"] if len(i) > 2 and i.istitle()])\n",
        "#             for k, v in enumerate(tmp_titled_other_tokens):\n",
        "#                 tmp_cluster_res = [j[0] for j in tmp_new_coref_cluster_info if v in j[1]]\n",
        "#                 if len(tmp_cluster_res) < 1 and v not in tmp_speaker_label_dict.keys():\n",
        "#                     tmp_cluster_res = [(j[0], j[1].count(v)) for j in tmp_new_coref_cluster_info if v in \" \".join(j[1]).split()]\n",
        "#                     tmp_cluster_res = sorted(tmp_cluster_res, key=lambda x: x[1], reverse=True)\n",
        "#                     if len(tmp_cluster_res) > 0:\n",
        "#                         tmp_speaker_label_dict[v] = tmp_cluster_res[0][0]\n",
        "#                     else:\n",
        "#                         tmp_speaker_label_dict[v] = len(tmp_new_coref_cluster_info) + 100\n",
        "#                         tmp_new_coref_cluster_info.append((len(tmp_new_coref_cluster_info) + 100, [v]))\n",
        "\n",
        "#             \"\"\" Adding spaces in tokenized list, to recover the same tokenization via BART \"\"\"\n",
        "#             tmp_doc = copy.deepcopy(\" \" + tmp_i[0])\n",
        "\n",
        "#             tmp_token_list_with_space = []\n",
        "#             for k, v in enumerate(tmp_token_list):\n",
        "#                 find_idx = str(tmp_doc).index(v)\n",
        "#                 if find_idx > 0 and tmp_doc[find_idx - 1] == \" \":\n",
        "#                     tmp_token_list_with_space.append([\" \" + v, -1])\n",
        "#                 else:\n",
        "#                     tmp_token_list_with_space.append([v, -1])\n",
        "#                 tmp_doc = tmp_doc[find_idx + len(v):]\n",
        "\n",
        "#             \"\"\" Labeling the token list with the coreference cluster labels \"\"\"\n",
        "#             \"\"\" From the longer spans to shorter spans, to avoiding labels to be re-changed \"\"\"\n",
        "#             tmp_span_len_list = []\n",
        "#             for i in tmp_clusters:\n",
        "#                 tmp_span_len_list.extend([j[-1] + 1 - j[0] for j in i])\n",
        "\n",
        "#             tmp_span_len_list = list(set(tmp_span_len_list))\n",
        "#             tmp_span_len_list = sorted(tmp_span_len_list, reverse=True)\n",
        "\n",
        "#             for one_len in tmp_span_len_list:\n",
        "#                 for i in range(len(tmp_clusters)):\n",
        "#                     for j in tmp_clusters[i]:\n",
        "#                         if (j[1] + 1 - j[0]) == one_len:\n",
        "#                             for e in j:\n",
        "#                                 tmp_token_list_with_space[e][1] = i\n",
        "\n",
        "#             \"\"\" Tackle the issue that speaker names do not have coreference \"\"\"\n",
        "#             assert len(tmp_token_list_with_space) == len(tmp_token_list)\n",
        "#             for k in range(len(tmp_token_list_with_space)):\n",
        "#                 if (k == len(tmp_token_list_with_space) - 1 or tmp_token_list_with_space[k + 1][0].strip() == \":\") \\\n",
        "#                         and tmp_token_list_with_space[k][0].strip() in tmp_speaker_label_dict.keys() \\\n",
        "#                         and tmp_token_list_with_space[k][1] == -1:\n",
        "#                     tmp_token_list_with_space[k][1] = tmp_speaker_label_dict[tmp_token_list_with_space[k][0].strip()]\n",
        "#                     # print(tmp_token_list_with_space)\n",
        "\n",
        "#             \"\"\" Merge the token list with the same coreference cluster \"\"\"\n",
        "#             merged_tmp_token_list_with_space = []\n",
        "#             current_merge_set = []\n",
        "#             current_cluster_id_to_merge = -999\n",
        "#             for i in range(len(tmp_token_list_with_space)):\n",
        "#                 if tmp_token_list_with_space[i][1] == current_cluster_id_to_merge:\n",
        "#                     current_merge_set.append(tmp_token_list_with_space[i])\n",
        "#                     current_cluster_id_to_merge = tmp_token_list_with_space[i][1]\n",
        "#                 else:\n",
        "#                     if len(current_merge_set) > 0:\n",
        "#                         merged_tmp_token_list_with_space.append([[j[0] for j in current_merge_set], current_merge_set[0][1]])\n",
        "#                         current_merge_set = []\n",
        "#                     current_merge_set.append(tmp_token_list_with_space[i])\n",
        "#                     current_cluster_id_to_merge = tmp_token_list_with_space[i][1]\n",
        "#                 if i == len(tmp_token_list_with_space) - 1 and len(current_merge_set) > 0:\n",
        "#                     merged_tmp_token_list_with_space.append([[j[0] for j in current_merge_set], current_merge_set[0][1]])\n",
        "#                     current_merge_set = []\n",
        "\n",
        "#             # print(merged_tmp_token_list_with_space)\n",
        "\n",
        "#             \"\"\" Using the BART tokenizer to process the new token list \"\"\"\n",
        "#             for i in range(len(merged_tmp_token_list_with_space)):\n",
        "#                 merged_tmp_token_list_with_space[i][0] = self.global_tokenizer.tokenize(\"\".join(merged_tmp_token_list_with_space[i][0]))\n",
        "\n",
        "#             \"\"\" V2 only point to the first token of spans \"\"\"\n",
        "#             tmp_token_list_with_cluster_ids = []\n",
        "#             for i in merged_tmp_token_list_with_space:\n",
        "#                 for j in range(len(i[0])):\n",
        "#                     if j == 0:\n",
        "#                         tmp_token_list_with_cluster_ids.append([i[0][j], i[1]])\n",
        "#                     else:\n",
        "#                         tmp_token_list_with_cluster_ids.append([i[0][j], -1])\n",
        "\n",
        "#             if debug:\n",
        "#                 tmp_t = \" \".join(self.global_tokenizer.tokenize(tmp_i[0])).strip()\n",
        "#                 tmp_c = \" \".join([j[0] for j in tmp_token_list_with_cluster_ids]).strip()\n",
        "#                 print(\"\\n\", tmp_t, \"\\n\", tmp_c)\n",
        "\n",
        "#             \"\"\" Adding coreference of the conditional personal names \"\"\"\n",
        "#             if conditional_file_path is not None:\n",
        "#                 assert len(re.findall(\"\\}\\s+\\#\", conditional_line_list[tmp_k])) == 1\n",
        "#                 conditional_names = (conditional_line_list[tmp_k].split(\"}\")[0] + \"} #\").split()\n",
        "#                 conditional_names = [[i.strip(), -1] for i in conditional_names]\n",
        "\n",
        "#                 for k, v in enumerate(conditional_names):\n",
        "#                     if v[0] not in [\"{\", \"}\", \"#\", \"|\"]:\n",
        "#                         tmp_cluster_res = [(j[0], j[1].count(v[0])) for j in tmp_new_coref_cluster_info if v[0] in j[1]]\n",
        "#                         if len(tmp_cluster_res) > 0:\n",
        "#                             conditional_names[k][1] = tmp_cluster_res[0][0]\n",
        "#                             # if len(tmp_cluster_res) > 1:\n",
        "#                             #     print(tmp_cluster_res)\n",
        "#                         else:\n",
        "#                             \"\"\" splitting every name in the cluster keys, then find more names \"\"\"\n",
        "#                             tmp_cluster_res = [(j[0], j[1].count(v[0])) for j in tmp_new_coref_cluster_info if v[0] in \" \".join(j[1]).split()]\n",
        "\n",
        "#                             if len(tmp_cluster_res) > 0:\n",
        "#                                 conditional_names[k][1] = tmp_cluster_res[0][0]\n",
        "#                             else:\n",
        "#                                 \"\"\" To tackle the exception of names are not included \"\"\"\n",
        "#                                 tmp_new_coref_cluster_info.append((len(tmp_new_coref_cluster_info) + 50, [v[0]]))\n",
        "#                                 for n, i in enumerate(tmp_token_list_with_cluster_ids):\n",
        "#                                     if i[0][1:] == v[0] and i[1] == -1:\n",
        "#                                         tmp_token_list_with_cluster_ids[n][1] = tmp_new_coref_cluster_info[-1][0]\n",
        "#                                         conditional_names[k][1] = tmp_new_coref_cluster_info[-1][0]\n",
        "#                                 print(\"\\n\\n\\n\")\n",
        "#                                 print(tmp_i[0])\n",
        "#                                 print(v[0])\n",
        "#                                 print(tmp_token_list_with_cluster_ids)\n",
        "#                                 pass\n",
        "\n",
        "#                 print(conditional_names)\n",
        "#                 condition_prefix = conditional_names\n",
        "#                 tmp_prefix = []\n",
        "#                 for i in condition_prefix:\n",
        "#                     tmp_t = self.global_tokenizer.tokenize(\" \" + i[0])\n",
        "#                     for j in range(len(tmp_t)):\n",
        "#                         if j == 0:\n",
        "#                             tmp_prefix.append((tmp_t[j], i[1]))\n",
        "#                         else:\n",
        "#                             tmp_prefix.append((tmp_t[j], -1))\n",
        "\n",
        "#                 tmp_token_list_with_cluster_ids = tmp_prefix + tmp_token_list_with_cluster_ids\n",
        "\n",
        "#             else:\n",
        "#                 tmp_token_list_with_cluster_ids = [['#', -1]] + tmp_token_list_with_cluster_ids\n",
        "\n",
        "#             \"\"\" Build the src list ang tgt list for DGL GNN implementation \"\"\"\n",
        "#             src_list = []\n",
        "#             tgt_list = []\n",
        "#             text_input_list = []\n",
        "#             for k, v in enumerate(tmp_token_list_with_cluster_ids):\n",
        "#                 text_input_list.append(v[0])\n",
        "#                 if v[1] != -1:\n",
        "#                     find_precedent = [j for j in range(k) if tmp_token_list_with_cluster_ids[j][1] == v[1]]\n",
        "#                     if len(find_precedent) > 0:\n",
        "#                         src_list.append(k)\n",
        "#                         tgt_list.append(max(find_precedent))\n",
        "\n",
        "#             assert len(src_list) == len(tgt_list)\n",
        "\n",
        "#             tmp_line_idx += 1\n",
        "\n",
        "#             \"\"\" Truncating the lengthy samples \"\"\"\n",
        "#             if len(text_input_list) > 1023:\n",
        "#                 print(\"Truncate the lengthy sample:\", len(text_input_list))\n",
        "#                 cut_num = len([i for i in src_list if i > 1022])\n",
        "#                 print(cut_num)\n",
        "#                 src_list = src_list[:-cut_num]\n",
        "#                 tgt_list = tgt_list[:-cut_num]\n",
        "#                 print(src_list)\n",
        "#                 print(tgt_list)\n",
        "#                 assert len(src_list) == len(tgt_list)\n",
        "\n",
        "#             \"\"\" We write all information as a text file \"\"\"\n",
        "#             text_input_list = text_input_list[:1023]\n",
        "#             # output_fp.write(\" \".join(text_input_list) + \" ##### \" + str(self.global_tokenizer.convert_tokens_to_ids(text_input_list)) + \\\n",
        "#             #                 \" ##### \" + str(src_list) + \" ##### \" + str(tgt_list) + \" ##### \" + str(len(text_input_list)) + \"\\n\")\n",
        "#             output_list[\"input_ids\"].append(self.global_tokenizer.convert_tokens_to_ids(text_input_list))\n",
        "#             output_list[\"coref_information\"].append((src_list, tgt_list))\n",
        "\n",
        "#             assert len(text_input_list) == len(self.global_tokenizer.convert_tokens_to_ids(text_input_list))\n",
        "#             assert len(self.global_tokenizer.convert_tokens_to_ids(text_input_list)) < 1024\n",
        "\n",
        "#             if debug:\n",
        "#                 for k, v in enumerate(text_input_list):\n",
        "#                     if k in src_list:\n",
        "#                         print(\">>>>>>>> \", v.replace(\"Ä \", \"\"), k, tgt_list[src_list.index(k)])\n",
        "#                     else:\n",
        "#                         print(v.replace(\"Ä \", \"\"), k, \"X\")\n",
        "#                 tmp_matrix = Prev_Coreference_Matrix(len(text_input_list), src_list, tgt_list)\n",
        "#                 print(tmp_matrix)\n",
        "\n",
        "#         return output_list"
      ],
      "metadata": {
        "id": "Ef1gw6jMIpVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_target_length = 128\n",
        "# def preprocess_function(examples):\n",
        "#     # model_inputs = tokenizer(examples[\"dialogue\"], max_length=max_input_length, truncation=True)\n",
        "#     result = coref_model.process(examples[\"dialogue\"], batch_size=4)\n",
        "#     model_inputs = coref_build.build_sample_with_coref_to_file(result)\n",
        "\n",
        "#     # Setup the tokenizer for targets\n",
        "#     labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs"
      ],
      "metadata": {
        "id": "LRjZuNhcEkKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processed_dataset = raw_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "CQxnCxAEEem6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess dataset"
      ],
      "metadata": {
        "id": "JNaJb-yv4ZZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_processed_coref_dataset(prefix, datasets):\n",
        "    \"\"\"\n",
        "    :param prefix: prefix arbitrary name\n",
        "    :param datasets: the datasets need to save.\n",
        "    :return: directly write a file.\n",
        "    \"\"\"\n",
        "    task_names = ['train', 'test', 'validation']\n",
        "    for name in task_names:\n",
        "        output_fp = open(\"data/\" + prefix + '-' + name + '.source', 'w', encoding='utf-8')\n",
        "        current_dataset = datasets[name]\n",
        "        for data in current_dataset:\n",
        "            output_fp.write(data['id'] + '#' + str(data['input_ids']) + '#' \n",
        "                + str(data['coref_information'][0]) + '#' + str(data['coref_information'][1]) + '\\n')\n",
        "        output_fp.close()\n",
        "\n",
        "\n",
        "def convert_str_list_to_list(str_list):\n",
        "    tmp = str_list.strip().replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
        "    tmp = [int(i.strip()) for i in tmp if len(i.strip()) > 0]\n",
        "    return tmp\n",
        "\n",
        "\n",
        "def load_processed_coref_dataset(prefix):\n",
        "    \"\"\"\n",
        "    :param prefix: prefix arbitrary name\n",
        "    :return: input_ids and coref_information\n",
        "    \"\"\"\n",
        "    task_names = ['train', 'test', 'validation']\n",
        "    datasets = {}\n",
        "    for name in task_names:\n",
        "        input_ids_list = []\n",
        "        coref_information_list = []\n",
        "        result_dict = {}\n",
        "        input_fp = open(\"data/\" + prefix + '-' + name + '.source', 'r', encoding='utf-8')\n",
        "        for line in input_fp:\n",
        "            tmp = line.split('#')\n",
        "            input_ids_list.append(convert_str_list_to_list(tmp[1]))\n",
        "            coref_information_list.append((convert_str_list_to_list(tmp[2]), convert_str_list_to_list(tmp[3])))\n",
        "        result_dict['input_ids'] = input_ids_list\n",
        "        result_dict['coref_information'] = coref_information_list\n",
        "        datasets[name] = result_dict\n",
        "    \n",
        "    return datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "cfusKMjaJWis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load coreference dataset, prefix: ['bart', 't5-base']\n",
        "prefix = 'bart'\n",
        "coref_datasets = load_processed_coref_dataset(prefix)"
      ],
      "metadata": {
        "id": "DQH1BDp1OFGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_preprocessed_data_train(x, indice):\n",
        "    x['input_ids'] = coref_datasets['train']['input_ids'][indice]\n",
        "    x['attention_mask'] = [1] * len(coref_datasets['train']['input_ids'][indice])\n",
        "    x['coref_information'] = coref_datasets['train']['coref_information'][indice]\n",
        "    return x\n",
        "\n",
        "\n",
        "def add_preprocessed_data_test(x, indice):\n",
        "    x['input_ids'] = coref_datasets['test']['input_ids'][indice]\n",
        "    x['attention_mask'] = [1] * len(coref_datasets['test']['input_ids'][indice])\n",
        "    x['coref_information'] = coref_datasets['test']['coref_information'][indice]\n",
        "    return x\n",
        "\n",
        "\n",
        "def add_preprocessed_data_validation(x, indice):\n",
        "    x['input_ids'] = coref_datasets['validation']['input_ids'][indice]\n",
        "    x['attention_mask'] = [1] * len(coref_datasets['validation']['input_ids'][indice])\n",
        "    x['coref_information'] = coref_datasets['validation']['coref_information'][indice]\n",
        "    return x\n",
        "\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = examples\n",
        "    \n",
        "    # Setup the tokenizer for targets\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def save_result(result, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as fp:\n",
        "        for i in result:\n",
        "            fp.write(i.strip() + \"\\n\")"
      ],
      "metadata": {
        "id": "hRdPBcoUJRyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_dataset = raw_datasets['train'].map(add_preprocessed_data_train, with_indices=True)\n",
        "processed_test_dataset = raw_datasets['test'].map(add_preprocessed_data_test, with_indices=True)\n",
        "processed_validation_dataset = raw_datasets['validation'].map(add_preprocessed_data_validation, with_indices=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSd-AtnzJ9OF",
        "outputId": "8341eafd-e01f-4a87-9e9c-f004274d0439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-a594bfdffdeecb06.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-f1edb3e986503906.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-a159f222fe260315.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "processed_datasets = copy.deepcopy(raw_datasets)"
      ],
      "metadata": {
        "id": "FNFQlrH1QQCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_datasets['train'] = processed_train_dataset\n",
        "processed_datasets['test'] = processed_test_dataset\n",
        "processed_datasets['validation'] = processed_validation_dataset"
      ],
      "metadata": {
        "id": "M0TuYBrNK9zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"facebook/bart-base\""
      ],
      "metadata": {
        "id": "WHvNDAhHZUEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "v5KShfXKaSUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_coref_datasets = processed_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOn8cBijbUTR",
        "outputId": "04c66d5b-36a1-4962-8480-fe324623af07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-36f0826aea95646e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-d2bceb9cef7881eb.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3d2540ed6b40ea05.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "4JdkaXHYaPGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(model_checkpoint, output_hidden_states=False)"
      ],
      "metadata": {
        "id": "onJ19HiKZPEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb2b231-87ca-4711-cee4-e8d763250e2b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7olpPP6acJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ac09da-e8a1-4120-cd73-a468a97586dc"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    \"16-coref-bart-dialogue-summarization\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    # gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    # save_total_limit=2,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps = 10, ## added\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtDg-_hqacJ9"
      },
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_coref_datasets_train = tokenized_coref_datasets['train']\n",
        "tokenized_coref_dataset_val = tokenized_coref_datasets['validation']"
      ],
      "metadata": {
        "id": "pTikd6_UdPfW"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_coref_datasets_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lG9mwFzgk_4",
        "outputId": "7de23f7c-e4fe-4cd2-d18c-ceb932b9f8f5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'coref_information', 'labels'],\n",
              "    num_rows: 14731\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109820ca-306b-455d-a85b-5972b845b9c4",
        "id": "ieroGHrLacJ9"
      },
      "source": [
        "# trainer = CustomTrainer(\n",
        "#     model,\n",
        "#     args,\n",
        "#     train_dataset=tokenized_coref_datasets_train,\n",
        "#     eval_dataset=tokenized_coref_dataset_val,\n",
        "#     data_collator=data_collator,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_coref_datasets_train,\n",
        "    eval_dataset=tokenized_coref_dataset_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82df8b00-4d60-4321-b32a-67aa3fcce3f8",
        "id": "cmF4susRacJ9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "9f2527d6-c57c-4650-9010-08929cd41c1f",
        "id": "R4RKBztwacJ9"
      },
      "source": [
        "trainer.evaluate() #before training"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [52/52 00:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.5141791105270386,\n",
              " 'eval_rouge1': 47.7643,\n",
              " 'eval_rouge2': 24.7228,\n",
              " 'eval_rougeL': 40.606,\n",
              " 'eval_rougeLsum': 44.4947,\n",
              " 'eval_gen_len': 18.2592,\n",
              " 'eval_runtime': 30.4626,\n",
              " 'eval_samples_per_second': 26.853,\n",
              " 'eval_steps_per_second': 1.707}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MGB4InuUOlmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bac82eee-6fd9-4595-ead9-5d8b29611fdc"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 14731\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 4605\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4605' max='4605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4605/4605 16:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.487200</td>\n",
              "      <td>1.522166</td>\n",
              "      <td>48.490700</td>\n",
              "      <td>25.503200</td>\n",
              "      <td>41.458900</td>\n",
              "      <td>45.131200</td>\n",
              "      <td>18.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.438400</td>\n",
              "      <td>1.509396</td>\n",
              "      <td>48.077600</td>\n",
              "      <td>25.406300</td>\n",
              "      <td>40.953600</td>\n",
              "      <td>44.903200</td>\n",
              "      <td>18.497600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.359400</td>\n",
              "      <td>1.501562</td>\n",
              "      <td>48.083000</td>\n",
              "      <td>25.053100</td>\n",
              "      <td>40.891600</td>\n",
              "      <td>44.566800</td>\n",
              "      <td>18.257900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.246200</td>\n",
              "      <td>1.501088</td>\n",
              "      <td>48.404700</td>\n",
              "      <td>25.316700</td>\n",
              "      <td>40.971500</td>\n",
              "      <td>44.840400</td>\n",
              "      <td>18.503700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.217500</td>\n",
              "      <td>1.507408</td>\n",
              "      <td>48.471100</td>\n",
              "      <td>25.671300</td>\n",
              "      <td>41.303500</td>\n",
              "      <td>45.113400</td>\n",
              "      <td>18.409500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-500\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-500/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1844: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [52/52 03:38]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-1000\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-1000/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-1000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-1500\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-1500/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-1500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1844: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n",
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-2000\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-2000/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-2000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-2500\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-2500/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-2500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1844: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n",
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-3000\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-3000/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-3000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-3500\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-3500/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-3500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1844: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n",
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-4000\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-4000/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-4000/special_tokens_map.json\n",
            "Saving model checkpoint to 16-coref-bart-dialogue-summarization/checkpoint-4500\n",
            "Configuration saved in 16-coref-bart-dialogue-summarization/checkpoint-4500/config.json\n",
            "Model weights saved in 16-coref-bart-dialogue-summarization/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in 16-coref-bart-dialogue-summarization/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in 16-coref-bart-dialogue-summarization/checkpoint-4500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1844: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4605, training_loss=1.3700128267435245, metrics={'train_runtime': 975.1186, 'train_samples_per_second': 75.534, 'train_steps_per_second': 4.723, 'total_flos': 1.692012598069248e+16, 'train_loss': 1.3700128267435245, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "75kctJToPZmE",
        "outputId": "236c011e-0b66-4fa2-ffbe-a67d05df4e57"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [52/52 00:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.507407546043396,\n",
              " 'eval_rouge1': 48.4711,\n",
              " 'eval_rouge2': 25.6713,\n",
              " 'eval_rougeL': 41.3035,\n",
              " 'eval_rougeLsum': 45.1134,\n",
              " 'eval_gen_len': 18.4095,\n",
              " 'eval_runtime': 30.1974,\n",
              " 'eval_samples_per_second': 27.088,\n",
              " 'eval_steps_per_second': 1.722,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save result"
      ],
      "metadata": {
        "id": "MkX0amRL4kt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = trainer.predict(tokenized_coref_dataset_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "V9C7HJhMp3Ts",
        "outputId": "e2ce2c3d-0fcb-4403-b80c-35b14e34f2b0"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 818\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='135' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [52/52 13:58]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"generation.txt\"\n",
        "batch_result = tokenizer.batch_decode(prediction.predictions, skip_special_tokens=True)\n",
        "save_result(batch_result, path)"
      ],
      "metadata": {
        "id": "XhTUA4oeo0Y4"
      },
      "execution_count": 104,
      "outputs": []
    }
  ]
}