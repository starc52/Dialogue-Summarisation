{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1244,"status":"ok","timestamp":1670445915459,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"},"user_tz":300},"id":"gMzbM4naD2g9"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1670445915914,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"},"user_tz":300},"id":"eHu9oO2Vqiud"},"outputs":[],"source":["# !pip install wandb\n","# !wandb login"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670445915915,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"},"user_tz":300},"id":"xwFTxxJiqwvI"},"outputs":[],"source":["# import wandb\n","\n","# wandb.init(project=\"t5-token\", entity=\"akatsuki_leaf\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2074,"status":"ok","timestamp":1670445917985,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"},"user_tz":300},"id":"On8STYHmSKtA","outputId":"211f58d0-b9a6-41c0-cc4a-c46b01d02327"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","A100-SXM4-40GB\n","Memory Usage: 39.6 GB\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  FutureWarning)\n"]}],"source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    \n","    print('Memory Usage:',round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670445917987,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"},"user_tz":300},"id":"faaqh5xzw8pI"},"outputs":[],"source":["import torch\n","import numpy as np\n","import random\n","\n","def set_random_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","set_random_seed(0)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOsHUjgdIrIW","executionInfo":{"status":"ok","timestamp":1670445921319,"user_tz":300,"elapsed":3339,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"60454f2f-0869-49ea-dd8f-bbfc791dcf47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.7.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.25.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: py7zr in /usr/local/lib/python3.7/dist-packages (0.20.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.10.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.2.3)\n","Requirement already satisfied: texttable in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.6.7)\n","Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.1)\n","Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.15.3)\n","Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.3.1)\n","Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from py7zr) (3.16.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from py7zr) (5.4.8)\n","Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.9)\n","Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n"]}],"source":["! pip install datasets transformers rouge-score nltk py7zr"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"e-v6Mjk_JvG3","executionInfo":{"status":"ok","timestamp":1670445921322,"user_tz":300,"elapsed":19,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"z5nOd8wBL-BO","executionInfo":{"status":"ok","timestamp":1670445921323,"user_tz":300,"elapsed":17,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["# %cd /content/drive/MyDrive/NLP Project with SCL"]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["766ced37004348c9bd5e33f04b376c66","bd397ec05fd241c7b059431df8d220d7","61eb414ece694830acf1bf7cb8f6d09e","3ce9e61851c04fd49834f784c1d6e827","6b46d6c0b86b48ef9a19259de6a9878e","f69a732f4cc54f51a20b147be6063d0e","5c76f244ef9a4cc7990c64171f6f1a49","d2684a802a3c4816a695b9c76e9222cd","c8b7643eafc04f34abf474e73875825b","9267e38cba734cd8bf585ee723863ee5","9793d87922e846aaaa977cd89e6475a1"]},"id":"IreSlFmlIrIm","executionInfo":{"status":"ok","timestamp":1670445927435,"user_tz":300,"elapsed":6125,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"7c4fdbc7-4458-4139-c89f-0b87c2e160cd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"766ced37004348c9bd5e33f04b376c66","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  \"\"\"\n"]}],"source":["from datasets import load_dataset, load_metric\n","\n","raw_datasets = load_dataset(\"samsum\")\n","\n","metric = load_metric(\"rouge\")"]},{"cell_type":"markdown","metadata":{"id":"X23VZ_cqSQJ5"},"source":["## T5"]},{"cell_type":"markdown","metadata":{"id":"EGgU3K1eSZYo"},"source":["### Preprocessing the data"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"y1It_TQ_SZYo","executionInfo":{"status":"ok","timestamp":1670445927440,"user_tz":300,"elapsed":31,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["model_checkpoint = \"t5-base\""]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMAHEIXVSZYo","executionInfo":{"status":"ok","timestamp":1670445933338,"user_tz":300,"elapsed":5925,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"58f29787-d9b6-4404-fed3-66386694bdcc"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:165: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  FutureWarning,\n"]}],"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"EXXAc4FOAw5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670445933339,"user_tz":300,"elapsed":16,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"e810463d-dc4d-4d1e-e81b-e178f4afbb68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[21542, 10, 3, 4605, 3, 4605, 12630, 10, 12, 63, 32, 17, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":12}],"source":["tokenizer([\"Amanda: bla bla\\r\\nGrey: toyot\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"b9hiJeaPFqEK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670446196386,"user_tz":300,"elapsed":263056,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"beb6c0b8-2d93-4712-fc6f-0a556a6ef83e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["[4269, 9491]\n","[]\n"]},{"output_type":"stream","name":"stderr","text":["Parameter 'indices'=<generator object remove_idx.<locals>.<genexpr> at 0x7f5d5abc2d50> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"stream","name":"stdout","text":["[]\n"]}],"source":["def check_token_length(dataset):\n","    ids=[]\n","    for i in range(len(dataset['dialogue'])):\n","        if len(tokenizer(dataset['dialogue'][i])['input_ids'])>1000:\n","            ids.append(i)\n","    print(ids)\n","    return ids\n","def remove_idx(list_idx, dataset):\n","    return dataset.select((\n","          i for i in range(len(dataset)) \n","          if i not in set(list_idx)))\n","    \n","train_ids=check_token_length(raw_datasets['train'])\n","validation_ids=check_token_length(raw_datasets['validation'])\n","test_ids = check_token_length(raw_datasets['test'])\n","changed_datasets_train=remove_idx(train_ids, raw_datasets['train'])\n","changed_datasets_val = remove_idx(validation_ids, raw_datasets['validation'])\n","changed_datasets_test = remove_idx(test_ids, raw_datasets['test'])"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"vc0BSBLIIrJQ","executionInfo":{"status":"ok","timestamp":1670446196388,"user_tz":300,"elapsed":22,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["max_input_length = 1024\n","max_target_length = 128\n","\n","def make_one_hot_sequence(input_ids, sequence_ids):\n","    changed_sequence_id=[]\n","    token_to_speaker_id={}\n","    uniq_id = 1\n","    for dic in sequence_ids:\n","        if str(input_ids[dic['spk'][0]:dic['spk'][1]]) in token_to_speaker_id:\n","            speaker_id = token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])]\n","        else:\n","            token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])] = uniq_id\n","            speaker_id = uniq_id\n","            uniq_id+=1\n","        for _ in range(dic['spk'][0], dic['spk'][1]):\n","            changed_sequence_id.append(speaker_id)\n","        for _ in range(dic['utt'][0], dic['utt'][1]):\n","            changed_sequence_id.append(-1)\n","    changed_sequence_id.append(0)\n","    return changed_sequence_id \n","\n","\n","def preprocess_function(examples): ## hit gold here. change this preprocess function to include speaker and turn information. \n","    # slash_n = tokenizer([\"\\r\\n\"])['input_ids'][0][1:-1]\n","    # slash_n_mask = tokenizer([\"\\r\\n\"])['attention_mask'][0][1:-1]\n","    inputs_list=[]\n","    masks_list=[]\n","    pos_list=[]\n","    for index in range(len(examples['dialogue'])):\n","        # breaking the dialogue for spk:utt info\n","        \n","        \n","        original = tokenizer(examples['dialogue'][index])['input_ids']\n","        \n","        \n","        broken=[]\n","        for utt in examples['dialogue'][index].split(\"\\r\\n\"):\n","            first_ind = utt.find(':')\n","            broken.append(utt[:first_ind+1])\n","            broken.append(utt[first_ind+1:])\n","        \n","        tokenized_broken = tokenizer(broken)['input_ids']\n","        attention_broken = tokenizer(broken)['attention_mask']\n","        # # adding \\r\\n tokens\n","        # for i in range(1, len(tokenized_broken)-1, 2):\n","        #     print(slash_n[0])\n","\n","        #     tokenized_broken[i].insert(-1, slash_n[0])\n","        #     tokenized_broken[i].insert(-1, slash_n[1])\n","        #     attention_broken[i].insert(-1, slash_n_mask[0])\n","        #     attention_broken[i].insert(-1, slash_n_mask[1])\n","        #     print(\"second\",tokenized_broken[i])\n","\n","        joined = tokenized_broken[0]\n","\n","        # annotating for spk_utt_pos\n","        assoc_dict={}\n","        assoc_dict['spk'] = [0, len(tokenized_broken[0])-1] # the range is actually exclusive of the last index. \n","        odd_bool = True\n","        running_length = len(tokenized_broken[0])\n","        sequence_ids=[]\n","        for inner in tokenized_broken[1:]:\n","            if odd_bool==True:\n","                assoc_dict['utt']=[running_length-1, running_length+len(inner)-2]\n","                odd_bool=False\n","                sequence_ids.append(assoc_dict)\n","                assoc_dict={}\n","            else:\n","                assoc_dict['spk']=[running_length-1, running_length+len(inner)-2]\n","                odd_bool=True\n","            joined = joined[:-1]+inner\n","            running_length += (len(inner)-1)\n","        \n","        # test for CUDA assert error\n","        if(len(joined)>1024):\n","            print(\"input tokens list length greater than 1024, skipping example\", end=' ')\n","            print(\"equal to\", len(joined))\n","            print(tokenizer.decode(joined))\n","        \n","        # creating inputs list\n","        inputs_list.append(joined)\n","        one_hot_spk_pos = make_one_hot_sequence(joined, sequence_ids)\n","        pos_list.append(one_hot_spk_pos)\n","        \n","        # creating new mask\n","        joined_mask = attention_broken[0]\n","        for inner_attention in attention_broken[1:]:\n","            joined_mask = joined_mask[:-1]+inner_attention\n","        masks_list.append(joined_mask)\n","    \n","    # overriding normal model_inputs\n","    inputs = [doc for doc in examples[\"dialogue\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","    model_inputs['input_ids'] = inputs_list\n","    model_inputs['attention_mask'] = masks_list\n","    model_inputs['spk_utt_pos'] = pos_list\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"DDtsaJeVIrJT","colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["47ea125de39b4700a7afb83b3e0e8620","874d567d045b46338054f5569f0b5c69","506aa8621f6a4cd6bc6986acb3d3f25f","4039baf7b2814411910fd854b0993ad4","1542d1e0e8614d5f938b0f0b1d04c56b","4d79b87a02b9441cbb0549cca72dbb2d","ced2ca22d3c34828b782fa01c22e99eb","100e79bc9ea54984ae63c80fe06b6d78","f758a50ce677461a81e8dc19421fda20","efbd7ed94bf04f3f8cb52ca9f402ea11","2924229d35f74bfca1d33b4ca23ce21c"]},"executionInfo":{"status":"ok","timestamp":1670446198354,"user_tz":300,"elapsed":1986,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"93c42f76-c03d-4802-ba47-7d0128c9f792"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-199093c5bc04cca3.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-6b9cd97b7cbc6058.arrow\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47ea125de39b4700a7afb83b3e0e8620","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3579: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n"]}],"source":["tokenized_datasets_train_o = changed_datasets_train.map(preprocess_function, batched=True)\n","tokenized_datasets_val_o = changed_datasets_val.map(preprocess_function, batched=True)\n","tokenized_datasets_test_o = changed_datasets_test.map(preprocess_function, batched=True)\n","\n","# tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets_train = tokenized_datasets_train_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_val = tokenized_datasets_val_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_test = tokenized_datasets_test_o.remove_columns(['id', 'dialogue', 'summary'])"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"eXTmGGJbtFhr","executionInfo":{"status":"ok","timestamp":1670446198356,"user_tz":300,"elapsed":15,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","\n","\n","class CustomTrainer(Seq2SeqTrainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # How the loss is computed by Trainer. By default, all models return the loss in the first element.\n","        # Subclass and override for custom behavior.\n","        # print(inputs)\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","\n","        # Save past state if it exists\n","        # TODO: this needs to be fixed and mselfade cleaner later.\n","\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n","                loss = self.label_smoother(outputs, labels, shift_labels=True)\n","            else:\n","                loss = self.label_smoother(outputs, labels)\n","        else:\n","            if isinstance(outputs, dict) and \"loss\" not in outputs:\n","                raise ValueError(\n","                    \"The model did not return a loss from the inputs, only the following keys: \"\n","                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n","                )\n","            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","        return (loss, outputs) if return_outputs else loss\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"HEZHf93mtTx2","executionInfo":{"status":"ok","timestamp":1670446198357,"user_tz":300,"elapsed":13,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["from transformers import PreTrainedTokenizerBase\n","from transformers.utils import PaddingStrategy\n","from transformers import DataCollatorForSeq2Seq\n","from typing import Optional, Any, Union\n","import numpy as np\n","\n","\n","class CustomCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n","    r\"\"\"\n","    Data collator that will dynamically pad the inputs received, as well as the labels.\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        model ([`PreTrainedModel`]):\n","            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n","            prepare the *decoder_input_ids*\n","            This is useful when using *label_smoothing* to avoid calculating loss twice.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n","              is provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n","              lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","        label_pad_token_id (`int`, *optional*, defaults to -100):\n","            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    model: Optional[Any] = None\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    label_pad_token_id: int = -100\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features, return_tensors=None):\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n","        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n","        # same length to return tensors.\n","        if labels is not None:\n","            max_label_length = max(len(l) for l in labels)\n","            if self.pad_to_multiple_of is not None:\n","                max_label_length = (\n","                        (max_label_length + self.pad_to_multiple_of - 1)\n","                        // self.pad_to_multiple_of\n","                        * self.pad_to_multiple_of\n","                )\n","\n","            padding_side = self.tokenizer.padding_side\n","            for feature in features:\n","                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n","                if isinstance(feature[\"labels\"], list):\n","                    feature[\"labels\"] = (\n","                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n","                    )\n","                elif padding_side == \"right\":\n","                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n","                else:\n","                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n","        # added here\n","        spk_utt_pos = [feature[\"spk_utt_pos\"] for feature in features]\n","        max_spk_utt_pos_length = max(len(l) for l in spk_utt_pos)\n","\n","        if self.pad_to_multiple_of is not None:\n","            max_spk_utt_pos_length = (\n","                    (max_spk_utt_pos_length + self.pad_to_multiple_of - 1)\n","                    // self.pad_to_multiple_of\n","                    * self.pad_to_multiple_of\n","            )\n","\n","        padding_side = self.tokenizer.padding_side\n","        for feature in features:\n","            remainder = [0] * (max_spk_utt_pos_length - len(feature[\"spk_utt_pos\"]))\n","            if isinstance(feature[\"spk_utt_pos\"], list):\n","                feature[\"spk_utt_pos\"] = (\n","                    feature[\"spk_utt_pos\"] + remainder if padding_side == \"right\" else remainder + feature[\n","                        \"spk_utt_pos\"]\n","                )\n","            elif padding_side == \"right\":\n","                feature[\"spk_utt_pos\"] = np.concatenate([feature[\"spk_utt_pos\"], remainder]).astype(np.int64)\n","            else:\n","                feature[\"spk_utt_pos\"] = np.concatenate([remainder, feature[\"spk_utt_pos\"]]).astype(np.int64)\n","\n","        features = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=return_tensors,\n","        )\n","\n","        # prepare decoder_input_ids\n","        if (\n","                labels is not None\n","                and self.model is not None\n","                and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n","        ):\n","            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n","            features[\"decoder_input_ids\"] = decoder_input_ids\n","\n","        return features\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"nEvSIc03tW7d","executionInfo":{"status":"ok","timestamp":1670446200400,"user_tz":300,"elapsed":2055,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["from transformers.models.t5.modeling_t5 import T5ForConditionalGeneration\n","from torch import nn\n","from transformers import BartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers.modeling_outputs import BaseModelOutput\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","from torch.nn import CrossEntropyLoss\n","from transformers.models.bart.modeling_bart import BartConfig\n","import torch\n","from typing import *\n","from transformers.modeling_outputs import Seq2SeqLMOutput\n","from transformers.models.bart.modeling_bart import shift_tokens_right\n","import random\n","from tqdm import tqdm\n","import gc\n","import itertools\n","__HEAD_MASK_WARNING_MSG = \"\"\"\n","The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n","`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\n","If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\n","num_heads)`.\n","\"\"\"\n","class T5WithSCL(T5ForConditionalGeneration):\n","    def __init__(self, config: BartConfig):\n","        super().__init__(config)\n","\n","    def set_losses_list(self, SCLossesList=['token']):\n","        self.SCLossesList = SCLossesList\n","    \n","    def set_scl_coeff(self, scl_coeff=1e-1):\n","        self.scl_coeff=scl_coeff\n","    \n","    def token_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Token Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        spk_utt_states[spk].append(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]])\n","            \n","            \n","            #---------- hitesh------------------------------\n","            # positive samples\n","            # L_pos = 0\n","            # L_neg = 0 \n","\n","            # sampled_spk_utt_states = []           \n","\n","            # for spk in uniq_spks:\n","            #     utts = len(spk_utt_states[spk])\n","            #     spk_utt = []\n","            #     if utts > 1:\n","            #         # ids = random.sample(list(range(len(spk_utt_states[spk]))), random.randint(1, utts))\n","            #         ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","            #         for i in ids:\n","            #           spk_utt.append(spk_utt_states[spk][i])\n","            #     sampled_spk_utt_states.append(spk_utt)\n","\n","            # for instance in sampled_spk_utt_states:\n","            #   for i in range(len(instance)):\n","            #     for j in range(len(instance)):\n","            #       mat_mul = torch.einsum('ij, kj->ik', instance[i], instance[j])\n","            #       sigm = torch.sigmoid(mat_mul)\n","            #       log = torch.log(sigm)\n","            #       L_pos += torch.sum(-1 * log)\n","            # # print(\"L_pos\", L_pos)\n","\n","            # #negative loss\n","            # for i in range(0,len(sampled_spk_utt_states)):\n","            #   instance = sampled_spk_utt_states[i]\n","\n","            #   neg_instances = sampled_spk_utt_states[:i]+sampled_spk_utt_states[i+1:]\n","            #   neg_instances = list(itertools.chain(*neg_instances))\n","            #   # neg_instances = random.choices(neg_instances,k = random.randint(1, len(neg_instances)))\n","            #   if len(neg_instances)>0:\n","            #     # print(len(neg_instances))\n","            #     # print(\"-------------------------\")\n","            #     # print(sampled_spk_utt_states)\n","            #     neg_instances = random.choices(neg_instances,k = 2)\n","            #     for i in range(len(instance)):\n","            #       for j in range(len(neg_instances)):\n","            #         mat_mul = torch.einsum('ij, kj->ik', instance[i], neg_instances[j])\n","            #         sigm = torch.sigmoid(mat_mul)\n","            #         log = torch.log(1 - sigm+1e-5)\n","            #         L_neg += torch.sum(-1 * log)\n","            #---------- hitesh------------------------------\n","            \n","            \n","            # positive samples\n","            L_pos = 0\n","            \n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk][id1])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            \n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            # wandb.log({\"batch_scl-token\": batch_scl})\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def turn_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk][id1])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    # L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                # L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            # wandb.log({\"batch_scl-turn\": batch_scl})\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def global_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.choice(list(range(len(spk_utt_states[spk]))))\n","                    \n","                    spk_mean_exc = torch.mean(torch.vstack([spk_utt_states[spk][temp] for temp in range(len(spk_utt_states[spk])) if temp != ids]), 0)\n","                    \n","                    pos_mat_mul = torch.einsum('i, j->', spk_utt_states[spk][ids], spk_mean_exc)\n","                    pos_sigm = torch.sigmoid(pos_mat_mul)\n","                    pos_log = torch.log(pos_sigm)\n","                    L_pos += torch.sum(-1 * pos_log)\n","\n","                    # negative sample\n","\n","                    new_uniq_spks = uniq_spks.copy()\n","                    new_uniq_spks.remove(spk)\n","                    \n","                    spk2 = random.choice(new_uniq_spks)\n","                    id_neg = random.choice(list(range(len(spk_utt_states[spk2]))))\n","                    neg_mat_mul = torch.einsum('i, j->', spk_utt_states[spk2][id_neg], spk_mean_exc)\n","                    neg_sigm = torch.sigmoid(neg_mat_mul)\n","                    neg_log = torch.log(1 - neg_sigm+1e-5)\n","                    L_neg += torch.sum(-1 * neg_log)\n","                \n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            # wandb.log({\"batch_scl-global\": batch_scl})\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        spk_utt_pos: Optional[torch.Tensor] = None, ##changed here\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        decoder_head_mask: Optional[torch.FloatTensor] = None,\n","        cross_attn_head_mask: Optional[torch.Tensor] = None,\n","        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n","            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n","            labels in `[0, ..., config.vocab_size]`\n","        Returns:\n","        \"\"\"\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n","        if head_mask is not None and decoder_head_mask is None:\n","            if self.config.num_layers == self.config.num_decoder_layers:\n","                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n","                decoder_head_mask = head_mask\n","\n","        # Encode if needed (training, first prediction pass)\n","        if encoder_outputs is None:\n","            # Convert encoder inputs in embeddings if needed\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","            turn_attention_mask=None\n","            token_encoder_outputs=None\n","            tog_encoder_outputs=None\n","            \n","            if 'token' in self.SCLossesList:\n","                token_encoder_outputs = self.encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","\n","            if 'turn' in self.SCLossesList or 'global' in self.SCLossesList:\n","                tog_attention_mask = torch.where(spk_utt_pos>0, 0, attention_mask)\n","                tog_encoder_outputs = self.encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=tog_attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","            \n","        hidden_states = encoder_outputs[0]\n","\n","        if self.model_parallel:\n","            torch.cuda.set_device(self.decoder.first_device)\n","\n","        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n","            # get decoder inputs from shifting lm labels to the right\n","            decoder_input_ids = self._shift_right(labels)\n","\n","        # Set device for model parallelism\n","        if self.model_parallel:\n","            torch.cuda.set_device(self.decoder.first_device)\n","            hidden_states = hidden_states.to(self.decoder.first_device)\n","            if decoder_input_ids is not None:\n","                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n","            if attention_mask is not None:\n","                attention_mask = attention_mask.to(self.decoder.first_device)\n","            if decoder_attention_mask is not None:\n","                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n","\n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=attention_mask,\n","            head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = decoder_outputs[0]\n","\n","        # Set device for model parallelism\n","        if self.model_parallel:\n","            torch.cuda.set_device(self.encoder.first_device)\n","            self.lm_head = self.lm_head.to(self.encoder.first_device)\n","            sequence_output = sequence_output.to(self.lm_head.weight.device)\n","\n","        if self.config.tie_word_embeddings:\n","            # Rescale output before projecting on vocab\n","            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n","            sequence_output = sequence_output * (self.model_dim**-0.5)\n","\n","        lm_logits = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            masked_lm_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n","            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n","        \n","        \n","        # added here\n","        sc_loss = 0\n","        if 'token' in self.SCLossesList and labels is not None:\n","            sc_loss += self.token_scl(last_hidden_state=token_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","            # print(sc_loss)\n","        if 'turn' in self.SCLossesList and labels is not None:\n","            sc_loss += self.turn_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if 'global' in self.SCLossesList and labels is not None:\n","            sc_loss += self.global_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if not return_dict:\n","            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n","            return ((masked_lm_loss+(self.scl_coeff*sc_loss),) + output) if loss is not None else output\n","\n","        loss = None\n","        if masked_lm_loss is None:\n","            loss = None\n","        else:\n","            loss = masked_lm_loss+(self.scl_coeff*sc_loss)        \n","\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            cross_attentions=decoder_outputs.cross_attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )\n","\n","    "]},{"cell_type":"code","execution_count":19,"metadata":{"id":"0K6zHZKL7Tle","executionInfo":{"status":"ok","timestamp":1670446202897,"user_tz":300,"elapsed":2504,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["model_checkpoint = '/content/t5-token-b4c0.1/checkpoint-18000'\n","\n","model = T5WithSCL.from_pretrained(model_checkpoint)\n","model.set_losses_list(['global'])\n","model.set_scl_coeff(1)\n"]},{"cell_type":"markdown","metadata":{"id":"fZNKwHmTpjxX"},"source":["### Fine-tuning the model"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"37a4Gih6lB8t","executionInfo":{"status":"ok","timestamp":1670446202899,"user_tz":300,"elapsed":13,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["# Parameters\\\n","batch_size=4\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"t5-token-b4c0.1\",\n","    num_train_epochs=5,\n","    do_train=True,\n","    do_eval=True,\n","    evaluation_strategy = \"epoch\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    learning_rate=1e-4,\n","    warmup_steps=500,\n","    weight_decay=0.1,\n","    # label_smoothing_factor=0.1, ## causes to throw an error\n","    predict_with_generate=True,\n","    # logging_dir=\"logs\",\n","    logging_steps=10,\n","    # save_total_limit=3,\n",")\n","\n","\n","data_collator = CustomCollatorForSeq2Seq(tokenizer, model=model)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"UmvbnJ9JIrJd","executionInfo":{"status":"ok","timestamp":1670446202901,"user_tz":300,"elapsed":13,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["import nltk\n","import numpy as np\n","import torch\n","torch.cuda.empty_cache()\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    for i in range(0,50):\n","      # print(tokenized_datasets_val[\"dialogue\"][i])\n","      print(\"-----------\",i,\"--------------\")\n","      print(\"------>Predictions by Model\")\n","      print(decoded_preds[i])\n","      print(\"----->Predictions Original\")\n","      print(decoded_labels[i])\n","      print(\"**************************\")\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"fUlQNc7i7ziP","executionInfo":{"status":"ok","timestamp":1670446210097,"user_tz":300,"elapsed":7207,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets_train,\n","    eval_dataset=tokenized_datasets_val,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"2xixI4gdbuoe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670446210098,"user_tz":300,"elapsed":21,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"4f86f0bf-d417-47d6-a501-4dc5d4c4d40e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"k15h5mOYVSoC","executionInfo":{"status":"error","timestamp":1670456376626,"user_tz":300,"elapsed":9951279,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"77cbfd2b-6832-4d37-cf92-26464cecbd55"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 14730\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18415\n","  Number of trainable parameters = 222903552\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15501' max='18415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15501/18415 2:45:42 < 31:09, 1.56 it/s, Epoch 4.21/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.830100</td>\n","      <td>3.628006</td>\n","      <td>47.835000</td>\n","      <td>24.155000</td>\n","      <td>40.357600</td>\n","      <td>44.371800</td>\n","      <td>17.042800</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.221700</td>\n","      <td>3.736091</td>\n","      <td>48.025500</td>\n","      <td>24.613100</td>\n","      <td>40.355500</td>\n","      <td>44.324700</td>\n","      <td>17.116100</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.039400</td>\n","      <td>3.792296</td>\n","      <td>48.365200</td>\n","      <td>25.624600</td>\n","      <td>41.057300</td>\n","      <td>44.780000</td>\n","      <td>17.003700</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>3.587500</td>\n","      <td>3.915245</td>\n","      <td>47.960300</td>\n","      <td>24.671800</td>\n","      <td>40.217400</td>\n","      <td>44.179300</td>\n","      <td>17.176000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-500\n","Configuration saved in t5-token-b4c0.1/checkpoint-500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-1000\n","Configuration saved in t5-token-b4c0.1/checkpoint-1000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-1500\n","Configuration saved in t5-token-b4c0.1/checkpoint-1500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-1500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-2000\n","Configuration saved in t5-token-b4c0.1/checkpoint-2000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-2000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-2500\n","Configuration saved in t5-token-b4c0.1/checkpoint-2500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-2500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-3000\n","Configuration saved in t5-token-b4c0.1/checkpoint-3000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-3000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-3500\n","Configuration saved in t5-token-b4c0.1/checkpoint-3500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-3500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='410' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [205/205 43:15]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A wants to get a puppy for his son.\n","He will take him to the animal shelter\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to buy an advent calendar for her children.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant.\n","She doesn't want to talk about it.\n","Iggy's friend\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found male underwear under her bed.\n","Kiki and Marla are investigating a\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert is looking for a music shop.\n","Fred sends him the address.\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Keith and Megan are running out of milk and cereals.\n","Megan will buy some.\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised that she is making a noise.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited Luis and Marion for a dinner at Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Jane wants to book a table for 6 people tonight at 21:00.\n","Vegano Resto\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy is coming home in 6 weeks.\n","She's travelling with 3 other Brits working here\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura needs a new printer.\n","Jamie suggests buying a second hand one.\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything.\n","Haylee is in dairy section.\n","Haylee is looking for\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Norbert is waiting for the tour.\n","Wendy is buying something.\n","Norbert is waiting for him\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Cecil and Peter went to the Jandia Peninsula.\n","It was windy.\n","They\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie hasn't found it yet.\n","Nickola advises checking pockets and handbag\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Rosie is writing an essay on bad movies.\n","Elle recommends \"Plan 9 from outer space\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","Julia wants to listen to James as a radio speaker.\n","James has a microphone at home\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Poppy and Alice are going for drinks after work.\n","Poppy is going to invite Fred and\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Sash will open the door before Caron arrives.\n","Caron is out from 12 and will\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed by Gosia, because she criticizes him for playing\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Jannette will join Ramzi for supper.\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer is preparing ravioli.\n","He got the recipe from his grandmother Hildegard\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence is writing the article.\n","He will get back to Madison when he is through with it\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent a photo of himself to Brennen.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie will borrow Chloe's bike on Thursday.\n","She will pick it up\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Olivia is working on a new restaurant in the city centre.\n","She would like to include it\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kristine is at St Marks Hospital near Asda.\n","Kenny is recovering from\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up after years of dating.\n","Skyler met Olivia 2 days ago.\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda is pregnant.\n","Amanda went to dance classes with Michael yesterday.\n","The instructor needed a partner\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Taylor wants Isabel to introduce her to her boyfriend.\n","Isabel hasn't introduced her boyfriend.\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is leaving on Friday.\n","He will go to the Italian Alpes.\n","Toby will\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Clara is not sure if Brandon is in yet.\n","Phil wants Brandon to come to her\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie cancelled her last time because she was sick.\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Diane is afraid of her children's Lorelai.\n","Kate is going to be Lor\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold and he has to call in.\n","Daniel will pick up some stuff\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","John, Alex and Sam are watching 'Millionaires' on tv.\n","Sam\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Sophie is waiting for a client who is 40 minutes late.\n","She will meet with Gw\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way to meet Sue downstairs.\n","She is with the Volvo.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty is going to din din at Asian restaurant.\n","George will pan fry the salmon and\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken installed Endomondo daily and he started running.\n","He checked his phone for his results\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ivan has bought Ann something for her birthday.\n","They are meeting next week.\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Delilah and Rowan are shocked by Ashley's facebook page.\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj's wife's papers with the working permission were sent to him by\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Maria and Thomas will have lunch together.\n","Ann is at the red table in the 3rd\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim right now.\n","She will meet with Terry and Kai at\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","They are waiting for her to pick\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla received 250.\n","She will let Adrian know when she has checked.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-4000\n","Configuration saved in t5-token-b4c0.1/checkpoint-4000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-4000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-4500\n","Configuration saved in t5-token-b4c0.1/checkpoint-4500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-4500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-5000\n","Configuration saved in t5-token-b4c0.1/checkpoint-5000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-5000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-5500\n","Configuration saved in t5-token-b4c0.1/checkpoint-5500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-5500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-6000\n","Configuration saved in t5-token-b4c0.1/checkpoint-6000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-6000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-6500\n","Configuration saved in t5-token-b4c0.1/checkpoint-6500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-6500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-7000\n","Configuration saved in t5-token-b4c0.1/checkpoint-7000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-7000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A wants to get a puppy for her son.\n","B will take him to the animal shelter\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to buy an advent calendar for her kids.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant.\n","She doesn't want to talk about it.\n","Iggy's friend\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found under her bed some male underwear.\n","Kiki thinks it was Marla\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert wants to buy a guitar cable from Fred.\n","Fred sends him the address of the\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Keith and Megan have run out of milk and cereals.\n","Megan will buy them.\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised that she is making a noise.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited Luis and Marion for dinner in Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Jane wants to book a table for six people under her name tonight at 21:00.\n","Vegan\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy is happy at her new job.\n","Tina misses her.\n","Nancy is coming home\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura needs a new printer.\n","Jamie suggests she can buy a second hand one.\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything she wanted from the dairy section.\n","Haylee is in the dairy section.\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Norbert and Wendy are going to catch the tour.\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Cheryl went to the Jandia Peninsula.\n","Peter took nice pictures.\n","He will let\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie hasn't found it.\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Rosie needs to write an essay.\n","Elle recommends \"Plan 9 from outer space\" and\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","James has a dream to become a voice actor.\n","He's worked in radio,\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Poppy and Alice had lunch together today.\n","Poppy is going for drinks after work.\n","Alice\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Caron is out from 12 and Sash will be before him.\n","Caron needs to go\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed by Gosia's criticism.\n","Giuseppe doesn'\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Ramzi's mom is calling for supper.\n","Jannette will join Ramzi\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer and Hildegard are making ravioli.\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence will be in a few minutes to finish the article that Madison needs.\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent Brennen a photo.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie will borrow Chloe's bike for Thursday.\n","She will pick it up\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Olivia is involved with a new Spicy and chilled restaurant in the city centre.\n","Carter is\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kenny has a surgery scheduled tomorrow.\n","He might have to reschedul\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up.\n","Skyler met her 2 days ago.\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda is pregnant.\n","She went to dance classes with Michael yesterday.\n","The instructor needed a partner\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Taylor wants Isabel to introduce her to her boyfriend.\n","Isabel hasn't had any boyfriends\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is leaving on Friday.\n","Toby will join him for the weekend.\n","They will go\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Brandon hasn't called to say he would be late.\n","Clara will prepare\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie is sick again and she's caughing like crazy.\n","Olga will\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Diane is afraid that Lorelai will be her mother.\n","Kate will be there for her\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold.\n","Daniel will buy some stuff from the pharmacy on his way back.\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","John, Alex and Sam are watching 'Millionaires' on TV.\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Sophie is waiting for a client who is 40 minutes late.\n","Gwen will let Sophie\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way.\n","Sue is going downstairs now.\n","Daniel is with the Volvo.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty will buy shaving cream for George at CVS.\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken installed Endomondo and checked it for his results and it was off.\n","Ken had it\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ann couldn't come to Ann's birthday party.\n","Ivan bought Ann a birthday\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Ashley posted a picture of herself on Facebook.\n","Rowan has more important work to do.\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj's wife has been denied the papers with the working permission.\n","The government\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Maria and Thomas will have lunch together.\n","Ann is at the red table in the hotel lobby.\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy because they don't want to work.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim right now.\n","She wants to go to the Museum of\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","She will come to Broke at\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla needs to check to make sure she received the money.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-7500\n","Configuration saved in t5-token-b4c0.1/checkpoint-7500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-7500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-8000\n","Configuration saved in t5-token-b4c0.1/checkpoint-8000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-8000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-8500\n","Configuration saved in t5-token-b4c0.1/checkpoint-8500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-8500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-9000\n","Configuration saved in t5-token-b4c0.1/checkpoint-9000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-9000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-9500\n","Configuration saved in t5-token-b4c0.1/checkpoint-9500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-9500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-10000\n","Configuration saved in t5-token-b4c0.1/checkpoint-10000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-10000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-10500\n","Configuration saved in t5-token-b4c0.1/checkpoint-10500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-10500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-11000\n","Configuration saved in t5-token-b4c0.1/checkpoint-11000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-11000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A wants to get a puppy for his son.\n","B will take him to the animal shelter\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to buy an advent calendar for her kids.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant.\n","Iggy and Jackie are talking about the same person.\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found under her bed.\n","Kiki thinks it's her sister's idea\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert is going to buy guitar cable at the music shop mentioned before.\n","Fred sends him the\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Keith and Megan have run out of milk and cereals.\n","Megan will buy some more\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised by the noise.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited his new place for dinner in Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Vegano Resto will book a table for six people under Jane's name tonight at\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy is a Texasan.\n","She's coming home in 6 weeks.\n","She's\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura needs a new printer.\n","Jamie suggests buying a second hand one.\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything she needed.\n","Haylee is in dairy section but she can't find\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Wendy is buying something to be right out of the tour.\n","Norbert missed the last tour because\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Cheryl went to the Jandia Peninsula.\n","Peter took some nice pictures.\n","He will\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie hasn't found it yet.\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Rosie needs to write an essay.\n","She chose bad movies as her topic.\n","She will cover\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","James has a dream to become a voice actor.\n","He's worked in radio during\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Poppy is going for drinks after work.\n","Alice will invite Fred and a bunch of others\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Caron is out from 12 and will be before Sash.\n","Caron needs to go out\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed by Gosia because she likes football and video games.\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Ramzi's mom is calling for supper.\n","Jannette will join him.\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer is preparing ravioli.\n","She got the recipe from her grandmother Hildegard\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence will get back to Madison when he's done with the article.\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent a photo to Brennen.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie will borrow Chloe's bike on Thursday evening.\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Olivia is working on a new restaurant in the city centre.\n","Carter is a bit nervous\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kenny is recovering from surgery.\n","He has another surgery scheduled tomorrow.\n","Guy will take care\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up.\n","He cheated on her with numerous girls and got pregnant\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda is pregnant.\n","She went to dancing classes with Michael yesterday.\n","The instructor needed a partner\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Taylor wants Isabel to introduce her boyfriend to her.\n","Isabel hasn't had any boyfriends\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is leaving on Friday.\n","He will go to the Italian Alps.\n","Toby\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Brandon hasn't called to say he would be late.\n","Clara will prepare\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie is sick again and she's cancelling her scheduled appointment with Olga.\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Diane is afraid that Lorelai will be her mother.\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold and needs to call in.\n","Daniel will help him on his way back\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","John, Alex and Sam are watching 'Millionaires' on TV.\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Sophie is waiting for a client who is 40 minutes late.\n","She will meet with Dee\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way.\n","Sue is going downstairs now.\n","Daniel is with the Volvo.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty will pick up shaving cream for George at CVS.\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken installed Endomondo and it didn't work.\n","He had it with him while\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ivan hasn't been to Ann's birthday party yet.\n","He bought Ann something\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Ashley posted pictures of her facebook page.\n","Rowan doesn't like the pictures.\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj's wife's papers with the working permission have been turned down by\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Maria and Thomas will have lunch together.\n","Ann is at the red table on the 3rd\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim right now.\n","She will be there around 2-2.\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","They are waiting for her to pick\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla received 250.\n","She will check if she has received it.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-11500\n","Configuration saved in t5-token-b4c0.1/checkpoint-11500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-11500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-12000\n","Configuration saved in t5-token-b4c0.1/checkpoint-12000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-12000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-12500\n","Configuration saved in t5-token-b4c0.1/checkpoint-12500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-12500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-12500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-12500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-13000\n","Configuration saved in t5-token-b4c0.1/checkpoint-13000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-13000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-13000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-13000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-13500\n","Configuration saved in t5-token-b4c0.1/checkpoint-13500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-13500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-13500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-13500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-14000\n","Configuration saved in t5-token-b4c0.1/checkpoint-14000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-14000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-14000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-14000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-14500\n","Configuration saved in t5-token-b4c0.1/checkpoint-14500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-14500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-14500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-14500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A wants to get a puppy for his son.\n","He's ready now.\n","A took\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to get an advent calendar for her children.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant.\n","Iggy and Jackie are talking about the same person.\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found under her bed some male underwear.\n","Kiki thinks it was her sister\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert is looking for a music shop.\n","Fred sends him the address.\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Keith and Megan have run out of milk and cereals.\n","Megan will buy some more\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised by the noise made by her.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited his friends for dinner to Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Vegano Resto will book a table for six people under Jane's name tonight at\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy's kids are behaving themselves.\n","Nancy's accent is not very good.\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura needs a new printer.\n","Jamie suggests she can buy a second hand one.\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything she needed except for the coconut milk yoghurt.\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Wendy is buying something to be right out of the tour.\n","Norbert missed the last tour because\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Peter took some pictures of the Jandia Peninsula.\n","He took a photo of it\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie hasn't found it yet.\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Rosie needs to write an essay.\n","Elle recommends \"Plan 9 from outer space\" and\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","James has a dream to become a voice actor.\n","He has worked in radio but\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Poppy and Alice had a long day.\n","Poppy is going for drinks after work.\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Caron is out from 12 and Sash will be before him.\n","Caron needs to go\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed by Gosia, because she likes football and video games\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Ramzi's mom is calling for supper.\n","Jannette will join him.\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer is preparing ravioli.\n","She got the recipe from her grandmother Hildegard\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence will get back to Madison after completing the article.\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent a photo to Brennen.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie needs to go to the dentist on Thursday.\n","Chloe will lend her her\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Olivia is working on a new restaurant in the city centre.\n","Carter is a bit nervous\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kenny is back from a surgery.\n","He might have to reschedul\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up.\n","He cheated on her with numerous girls and got pregnant\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda is pregnant.\n","She went to dancing classes with Michael yesterday.\n","The instructor needed a partner\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Taylor wants Isabel to introduce her boyfriend to her.\n","Isabel hasn't had any boyfriends\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is leaving on Friday.\n","Toby will join him and Theo for the Italian\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Brandon hasn't called to say he would be late.\n","Clara will prepare\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie is sick again and she's cancelling her scheduled appointment with Olga.\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Diane is afraid that Lorelai will be hers.\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold.\n","Daniel will pick him up from the pharmacy on his way back.\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","John, Alex and Sam are watching 'Millionaires' on TV.\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Sophie is waiting for a client.\n","He's already 40 minutes late.\n","Gwen\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way.\n","Sue is going downstairs now.\n","Daniel is with the Volvo.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty will pick up shaving cream for George at CVS.\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken's phone battery was almost depleted when he was running.\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ivan hasn't been to Ann's birthday party yet.\n","He bought Ann something\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Ashley posted a picture of herself on Facebook.\n","Rowan is busy with other things.\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj's wife has been issued with the working permission.\n","The government had\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Maria, Thomas and Ann will have lunch together.\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy because they don't want to work.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim right now.\n","She will be there around 2-2.\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","They are waiting for her to pick\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla received 250.\n","She will check if she has received it.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-15000\n","Configuration saved in t5-token-b4c0.1/checkpoint-15000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-15000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-15000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-15000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-15500\n","Configuration saved in t5-token-b4c0.1/checkpoint-15500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-15500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-15500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-15500/special_tokens_map.json\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m         )\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1850\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 829759104 vs 829758992"]}]},{"cell_type":"code","execution_count":25,"metadata":{"id":"l24RBo5J75kp","executionInfo":{"status":"ok","timestamp":1670446389534,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["# trainer.train()"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"5RY4ZQIDrmWr","executionInfo":{"status":"ok","timestamp":1670446389535,"user_tz":300,"elapsed":27,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"outputs":[],"source":["# evaluate before training for comparison\n","# trainer.evaluate()"]},{"cell_type":"code","source":["# torch.save(model.state_dict(), \"./best_model.bin\")"],"metadata":{"id":"GvJXL5br0VUZ","executionInfo":{"status":"ok","timestamp":1670446389536,"user_tz":300,"elapsed":25,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":27,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"766ced37004348c9bd5e33f04b376c66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd397ec05fd241c7b059431df8d220d7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_61eb414ece694830acf1bf7cb8f6d09e","IPY_MODEL_3ce9e61851c04fd49834f784c1d6e827","IPY_MODEL_6b46d6c0b86b48ef9a19259de6a9878e"]}},"bd397ec05fd241c7b059431df8d220d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61eb414ece694830acf1bf7cb8f6d09e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f69a732f4cc54f51a20b147be6063d0e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5c76f244ef9a4cc7990c64171f6f1a49"}},"3ce9e61851c04fd49834f784c1d6e827":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d2684a802a3c4816a695b9c76e9222cd","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8b7643eafc04f34abf474e73875825b"}},"6b46d6c0b86b48ef9a19259de6a9878e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9267e38cba734cd8bf585ee723863ee5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/3 [00:00&lt;00:00,  9.82it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9793d87922e846aaaa977cd89e6475a1"}},"f69a732f4cc54f51a20b147be6063d0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5c76f244ef9a4cc7990c64171f6f1a49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2684a802a3c4816a695b9c76e9222cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c8b7643eafc04f34abf474e73875825b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9267e38cba734cd8bf585ee723863ee5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9793d87922e846aaaa977cd89e6475a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"47ea125de39b4700a7afb83b3e0e8620":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_874d567d045b46338054f5569f0b5c69","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_506aa8621f6a4cd6bc6986acb3d3f25f","IPY_MODEL_4039baf7b2814411910fd854b0993ad4","IPY_MODEL_1542d1e0e8614d5f938b0f0b1d04c56b"]}},"874d567d045b46338054f5569f0b5c69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"506aa8621f6a4cd6bc6986acb3d3f25f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4d79b87a02b9441cbb0549cca72dbb2d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ced2ca22d3c34828b782fa01c22e99eb"}},"4039baf7b2814411910fd854b0993ad4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_100e79bc9ea54984ae63c80fe06b6d78","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f758a50ce677461a81e8dc19421fda20"}},"1542d1e0e8614d5f938b0f0b1d04c56b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_efbd7ed94bf04f3f8cb52ca9f402ea11","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.43s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2924229d35f74bfca1d33b4ca23ce21c"}},"4d79b87a02b9441cbb0549cca72dbb2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ced2ca22d3c34828b782fa01c22e99eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"100e79bc9ea54984ae63c80fe06b6d78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f758a50ce677461a81e8dc19421fda20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"efbd7ed94bf04f3f8cb52ca9f402ea11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2924229d35f74bfca1d33b4ca23ce21c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}