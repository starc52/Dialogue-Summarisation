{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670445842323,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"},"user_tz":300},"id":"gMzbM4naD2g9"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670445843139,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"},"user_tz":300},"id":"eHu9oO2Vqiud"},"outputs":[],"source":["# !pip install wandb\n","# !wandb login"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670445843141,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"},"user_tz":300},"id":"xwFTxxJiqwvI"},"outputs":[],"source":["# import wandb\n","\n","# wandb.init(project=\"t5-token\", entity=\"akatsuki_leaf\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2568,"status":"ok","timestamp":1670445845700,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"},"user_tz":300},"id":"On8STYHmSKtA","outputId":"c79e9b79-593e-48a6-ede4-34110ae430c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","A100-SXM4-40GB\n","Memory Usage: 39.6 GB\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  FutureWarning)\n"]}],"source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    \n","    print('Memory Usage:',round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1670445845700,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"},"user_tz":300},"id":"faaqh5xzw8pI"},"outputs":[],"source":["import torch\n","import numpy as np\n","import random\n","\n","def set_random_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","set_random_seed(0)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOsHUjgdIrIW","executionInfo":{"status":"ok","timestamp":1670445849612,"user_tz":300,"elapsed":3924,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"a58ab5c7-661f-4f80-d753-b9038229e213"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.7.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.25.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: py7zr in /usr/local/lib/python3.7/dist-packages (0.20.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.10.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n","Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.15.3)\n","Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.2.3)\n","Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.0)\n","Requirement already satisfied: texttable in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.6.7)\n","Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.3.1)\n","Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.9)\n","Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from py7zr) (3.16.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from py7zr) (5.4.8)\n","Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n"]}],"source":["! pip install datasets transformers rouge-score nltk py7zr"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"e-v6Mjk_JvG3","executionInfo":{"status":"ok","timestamp":1670445849614,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"z5nOd8wBL-BO","executionInfo":{"status":"ok","timestamp":1670445849616,"user_tz":300,"elapsed":22,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["# %cd /content/drive/MyDrive/NLP Project with SCL"]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["dd2256311355469d9a2f87d8fc59787b","a12d7867bf27487d939cde6f7a3d357e","f3360edeb6344aa98206772291806d7d","f0a749af013047eca5d70b4836ba44b5","b3bf9230d95d4d309facc82cf6feac35","f2c2166743664ccf91a43e4d8c258e2c","e1d5049a63f94b7c9eda9afa9688b151","33ce63d9a9ea4d90a6853355de364d97","0abacb02160a46a78d077185e9473786","aa7f73136c20497e9ff26499244cafc6","351b2dd236f74715a447ebb8ea52e3be"]},"id":"IreSlFmlIrIm","executionInfo":{"status":"ok","timestamp":1670445854659,"user_tz":300,"elapsed":5060,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"4cc2a8fd-f897-47ad-cbdb-174cf12030f0"},"outputs":[{"output_type":"stream","name":"stderr","text":["Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd2256311355469d9a2f87d8fc59787b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  \"\"\"\n"]}],"source":["from datasets import load_dataset, load_metric\n","\n","raw_datasets = load_dataset(\"samsum\")\n","\n","metric = load_metric(\"rouge\")"]},{"cell_type":"markdown","metadata":{"id":"X23VZ_cqSQJ5"},"source":["## T5"]},{"cell_type":"markdown","metadata":{"id":"EGgU3K1eSZYo"},"source":["### Preprocessing the data"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"y1It_TQ_SZYo","executionInfo":{"status":"ok","timestamp":1670445854661,"user_tz":300,"elapsed":17,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["model_checkpoint = \"t5-base\""]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMAHEIXVSZYo","executionInfo":{"status":"ok","timestamp":1670445859785,"user_tz":300,"elapsed":5138,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"18fb3710-43ab-4c80-b0eb-08a0e347093a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:165: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  FutureWarning,\n"]}],"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"EXXAc4FOAw5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670445860171,"user_tz":300,"elapsed":395,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"dfb40888-13c9-49fd-bf45-0183daa4345c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[21542, 10, 3, 4605, 3, 4605, 12630, 10, 12, 63, 32, 17, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":12}],"source":["tokenizer([\"Amanda: bla bla\\r\\nGrey: toyot\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"b9hiJeaPFqEK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670446125784,"user_tz":300,"elapsed":265623,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"8000edab-752f-478e-d762-a050abc527d3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["[4269, 9491]\n","[]\n"]},{"output_type":"stream","name":"stderr","text":["Parameter 'indices'=<generator object remove_idx.<locals>.<genexpr> at 0x7f10238a0ad0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"stream","name":"stdout","text":["[]\n"]}],"source":["def check_token_length(dataset):\n","    ids=[]\n","    for i in range(len(dataset['dialogue'])):\n","        if len(tokenizer(dataset['dialogue'][i])['input_ids'])>1000:\n","            ids.append(i)\n","    print(ids)\n","    return ids\n","def remove_idx(list_idx, dataset):\n","    return dataset.select((\n","          i for i in range(len(dataset)) \n","          if i not in set(list_idx)))\n","    \n","train_ids=check_token_length(raw_datasets['train'])\n","validation_ids=check_token_length(raw_datasets['validation'])\n","test_ids = check_token_length(raw_datasets['test'])\n","changed_datasets_train=remove_idx(train_ids, raw_datasets['train'])\n","changed_datasets_val = remove_idx(validation_ids, raw_datasets['validation'])\n","changed_datasets_test = remove_idx(test_ids, raw_datasets['test'])"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"vc0BSBLIIrJQ","executionInfo":{"status":"ok","timestamp":1670446125786,"user_tz":300,"elapsed":25,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["max_input_length = 1024\n","max_target_length = 128\n","\n","def make_one_hot_sequence(input_ids, sequence_ids):\n","    changed_sequence_id=[]\n","    token_to_speaker_id={}\n","    uniq_id = 1\n","    for dic in sequence_ids:\n","        if str(input_ids[dic['spk'][0]:dic['spk'][1]]) in token_to_speaker_id:\n","            speaker_id = token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])]\n","        else:\n","            token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])] = uniq_id\n","            speaker_id = uniq_id\n","            uniq_id+=1\n","        for _ in range(dic['spk'][0], dic['spk'][1]):\n","            changed_sequence_id.append(speaker_id)\n","        for _ in range(dic['utt'][0], dic['utt'][1]):\n","            changed_sequence_id.append(-1)\n","    changed_sequence_id.append(0)\n","    return changed_sequence_id \n","\n","\n","def preprocess_function(examples): ## hit gold here. change this preprocess function to include speaker and turn information. \n","    # slash_n = tokenizer([\"\\r\\n\"])['input_ids'][0][1:-1]\n","    # slash_n_mask = tokenizer([\"\\r\\n\"])['attention_mask'][0][1:-1]\n","    inputs_list=[]\n","    masks_list=[]\n","    pos_list=[]\n","    for index in range(len(examples['dialogue'])):\n","        # breaking the dialogue for spk:utt info\n","        \n","        \n","        original = tokenizer(examples['dialogue'][index])['input_ids']\n","        \n","        \n","        broken=[]\n","        for utt in examples['dialogue'][index].split(\"\\r\\n\"):\n","            first_ind = utt.find(':')\n","            broken.append(utt[:first_ind+1])\n","            broken.append(utt[first_ind+1:])\n","        \n","        tokenized_broken = tokenizer(broken)['input_ids']\n","        attention_broken = tokenizer(broken)['attention_mask']\n","        # # adding \\r\\n tokens\n","        # for i in range(1, len(tokenized_broken)-1, 2):\n","        #     print(slash_n[0])\n","\n","        #     tokenized_broken[i].insert(-1, slash_n[0])\n","        #     tokenized_broken[i].insert(-1, slash_n[1])\n","        #     attention_broken[i].insert(-1, slash_n_mask[0])\n","        #     attention_broken[i].insert(-1, slash_n_mask[1])\n","        #     print(\"second\",tokenized_broken[i])\n","\n","        joined = tokenized_broken[0]\n","\n","        # annotating for spk_utt_pos\n","        assoc_dict={}\n","        assoc_dict['spk'] = [0, len(tokenized_broken[0])-1] # the range is actually exclusive of the last index. \n","        odd_bool = True\n","        running_length = len(tokenized_broken[0])\n","        sequence_ids=[]\n","        for inner in tokenized_broken[1:]:\n","            if odd_bool==True:\n","                assoc_dict['utt']=[running_length-1, running_length+len(inner)-2]\n","                odd_bool=False\n","                sequence_ids.append(assoc_dict)\n","                assoc_dict={}\n","            else:\n","                assoc_dict['spk']=[running_length-1, running_length+len(inner)-2]\n","                odd_bool=True\n","            joined = joined[:-1]+inner\n","            running_length += (len(inner)-1)\n","        \n","        # test for CUDA assert error\n","        if(len(joined)>1024):\n","            print(\"input tokens list length greater than 1024, skipping example\", end=' ')\n","            print(\"equal to\", len(joined))\n","            print(tokenizer.decode(joined))\n","        \n","        # creating inputs list\n","        inputs_list.append(joined)\n","        one_hot_spk_pos = make_one_hot_sequence(joined, sequence_ids)\n","        pos_list.append(one_hot_spk_pos)\n","        \n","        # creating new mask\n","        joined_mask = attention_broken[0]\n","        for inner_attention in attention_broken[1:]:\n","            joined_mask = joined_mask[:-1]+inner_attention\n","        masks_list.append(joined_mask)\n","    \n","    # overriding normal model_inputs\n","    inputs = [doc for doc in examples[\"dialogue\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","    model_inputs['input_ids'] = inputs_list\n","    model_inputs['attention_mask'] = masks_list\n","    model_inputs['spk_utt_pos'] = pos_list\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"DDtsaJeVIrJT","colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["5d8eb4ffed2c4f0f90f29eefa3552a66","cf738784725a4cd7bf0055ff6cba0497","565ced4b904d4eef916aaf382451f56a","34c429a1248f4fa69f8caf02953a4d14","a55ec348cff7404e98730e009018a836","9dbc020d6b97490381e7546d37ea1174","5468708d8644478195aa0a1e2faf9147","62ebb0fee47f426bb972a26f2850b16e","e6cef97c9e8b456a8a5753d4b36f9722","71af572901024dd9bb04d603cfbc1447","48c3750ebc5f49ee8223cd40191a8f90"]},"executionInfo":{"status":"ok","timestamp":1670446126685,"user_tz":300,"elapsed":921,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"4034a7b4-9c9e-484f-e22e-842842f8d48a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-199093c5bc04cca3.arrow\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d8eb4ffed2c4f0f90f29eefa3552a66","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3579: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-a1e6bf85a1adddd0.arrow\n"]}],"source":["tokenized_datasets_train_o = changed_datasets_train.map(preprocess_function, batched=True)\n","tokenized_datasets_val_o = changed_datasets_val.map(preprocess_function, batched=True)\n","tokenized_datasets_test_o = changed_datasets_test.map(preprocess_function, batched=True)\n","\n","# tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets_train = tokenized_datasets_train_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_val = tokenized_datasets_val_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_test = tokenized_datasets_test_o.remove_columns(['id', 'dialogue', 'summary'])"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"eXTmGGJbtFhr","executionInfo":{"status":"ok","timestamp":1670446126688,"user_tz":300,"elapsed":18,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","\n","\n","class CustomTrainer(Seq2SeqTrainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # How the loss is computed by Trainer. By default, all models return the loss in the first element.\n","        # Subclass and override for custom behavior.\n","        # print(inputs)\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","\n","        # Save past state if it exists\n","        # TODO: this needs to be fixed and mselfade cleaner later.\n","\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n","                loss = self.label_smoother(outputs, labels, shift_labels=True)\n","            else:\n","                loss = self.label_smoother(outputs, labels)\n","        else:\n","            if isinstance(outputs, dict) and \"loss\" not in outputs:\n","                raise ValueError(\n","                    \"The model did not return a loss from the inputs, only the following keys: \"\n","                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n","                )\n","            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","        return (loss, outputs) if return_outputs else loss\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"HEZHf93mtTx2","executionInfo":{"status":"ok","timestamp":1670446126689,"user_tz":300,"elapsed":15,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["from transformers import PreTrainedTokenizerBase\n","from transformers.utils import PaddingStrategy\n","from transformers import DataCollatorForSeq2Seq\n","from typing import Optional, Any, Union\n","import numpy as np\n","\n","\n","class CustomCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n","    r\"\"\"\n","    Data collator that will dynamically pad the inputs received, as well as the labels.\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        model ([`PreTrainedModel`]):\n","            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n","            prepare the *decoder_input_ids*\n","            This is useful when using *label_smoothing* to avoid calculating loss twice.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n","              is provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n","              lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","        label_pad_token_id (`int`, *optional*, defaults to -100):\n","            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    model: Optional[Any] = None\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    label_pad_token_id: int = -100\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features, return_tensors=None):\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n","        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n","        # same length to return tensors.\n","        if labels is not None:\n","            max_label_length = max(len(l) for l in labels)\n","            if self.pad_to_multiple_of is not None:\n","                max_label_length = (\n","                        (max_label_length + self.pad_to_multiple_of - 1)\n","                        // self.pad_to_multiple_of\n","                        * self.pad_to_multiple_of\n","                )\n","\n","            padding_side = self.tokenizer.padding_side\n","            for feature in features:\n","                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n","                if isinstance(feature[\"labels\"], list):\n","                    feature[\"labels\"] = (\n","                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n","                    )\n","                elif padding_side == \"right\":\n","                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n","                else:\n","                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n","        # added here\n","        spk_utt_pos = [feature[\"spk_utt_pos\"] for feature in features]\n","        max_spk_utt_pos_length = max(len(l) for l in spk_utt_pos)\n","\n","        if self.pad_to_multiple_of is not None:\n","            max_spk_utt_pos_length = (\n","                    (max_spk_utt_pos_length + self.pad_to_multiple_of - 1)\n","                    // self.pad_to_multiple_of\n","                    * self.pad_to_multiple_of\n","            )\n","\n","        padding_side = self.tokenizer.padding_side\n","        for feature in features:\n","            remainder = [0] * (max_spk_utt_pos_length - len(feature[\"spk_utt_pos\"]))\n","            if isinstance(feature[\"spk_utt_pos\"], list):\n","                feature[\"spk_utt_pos\"] = (\n","                    feature[\"spk_utt_pos\"] + remainder if padding_side == \"right\" else remainder + feature[\n","                        \"spk_utt_pos\"]\n","                )\n","            elif padding_side == \"right\":\n","                feature[\"spk_utt_pos\"] = np.concatenate([feature[\"spk_utt_pos\"], remainder]).astype(np.int64)\n","            else:\n","                feature[\"spk_utt_pos\"] = np.concatenate([remainder, feature[\"spk_utt_pos\"]]).astype(np.int64)\n","\n","        features = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=return_tensors,\n","        )\n","\n","        # prepare decoder_input_ids\n","        if (\n","                labels is not None\n","                and self.model is not None\n","                and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n","        ):\n","            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n","            features[\"decoder_input_ids\"] = decoder_input_ids\n","\n","        return features\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"nEvSIc03tW7d","executionInfo":{"status":"ok","timestamp":1670446128230,"user_tz":300,"elapsed":1554,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["from transformers.models.t5.modeling_t5 import T5ForConditionalGeneration\n","from torch import nn\n","from transformers import BartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers.modeling_outputs import BaseModelOutput\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","from torch.nn import CrossEntropyLoss\n","from transformers.models.bart.modeling_bart import BartConfig\n","import torch\n","from typing import *\n","from transformers.modeling_outputs import Seq2SeqLMOutput\n","from transformers.models.bart.modeling_bart import shift_tokens_right\n","import random\n","from tqdm import tqdm\n","import gc\n","import itertools\n","__HEAD_MASK_WARNING_MSG = \"\"\"\n","The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n","`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\n","If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\n","num_heads)`.\n","\"\"\"\n","class T5WithSCL(T5ForConditionalGeneration):\n","    def __init__(self, config: BartConfig):\n","        super().__init__(config)\n","\n","    def set_losses_list(self, SCLossesList=['token']):\n","        self.SCLossesList = SCLossesList\n","    \n","    def set_scl_coeff(self, scl_coeff=1e-1):\n","        self.scl_coeff=scl_coeff\n","    \n","    def token_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Token Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        spk_utt_states[spk].append(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]])\n","            \n","            \n","            #---------- hitesh------------------------------\n","            # positive samples\n","            # L_pos = 0\n","            # L_neg = 0 \n","\n","            # sampled_spk_utt_states = []           \n","\n","            # for spk in uniq_spks:\n","            #     utts = len(spk_utt_states[spk])\n","            #     spk_utt = []\n","            #     if utts > 1:\n","            #         # ids = random.sample(list(range(len(spk_utt_states[spk]))), random.randint(1, utts))\n","            #         ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","            #         for i in ids:\n","            #           spk_utt.append(spk_utt_states[spk][i])\n","            #     sampled_spk_utt_states.append(spk_utt)\n","\n","            # for instance in sampled_spk_utt_states:\n","            #   for i in range(len(instance)):\n","            #     for j in range(len(instance)):\n","            #       mat_mul = torch.einsum('ij, kj->ik', instance[i], instance[j])\n","            #       sigm = torch.sigmoid(mat_mul)\n","            #       log = torch.log(sigm)\n","            #       L_pos += torch.sum(-1 * log)\n","            # # print(\"L_pos\", L_pos)\n","\n","            # #negative loss\n","            # for i in range(0,len(sampled_spk_utt_states)):\n","            #   instance = sampled_spk_utt_states[i]\n","\n","            #   neg_instances = sampled_spk_utt_states[:i]+sampled_spk_utt_states[i+1:]\n","            #   neg_instances = list(itertools.chain(*neg_instances))\n","            #   # neg_instances = random.choices(neg_instances,k = random.randint(1, len(neg_instances)))\n","            #   if len(neg_instances)>0:\n","            #     # print(len(neg_instances))\n","            #     # print(\"-------------------------\")\n","            #     # print(sampled_spk_utt_states)\n","            #     neg_instances = random.choices(neg_instances,k = 2)\n","            #     for i in range(len(instance)):\n","            #       for j in range(len(neg_instances)):\n","            #         mat_mul = torch.einsum('ij, kj->ik', instance[i], neg_instances[j])\n","            #         sigm = torch.sigmoid(mat_mul)\n","            #         log = torch.log(1 - sigm+1e-5)\n","            #         L_neg += torch.sum(-1 * log)\n","            #---------- hitesh------------------------------\n","            \n","            \n","            # positive samples\n","            L_pos = 0\n","            \n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk][id1])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            \n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            # wandb.log({\"batch_scl-token\": batch_scl})\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def turn_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk][id1])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    # L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                # L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            # wandb.log({\"batch_scl-turn\": batch_scl})\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def global_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.choice(list(range(len(spk_utt_states[spk]))))\n","                    \n","                    spk_mean_exc = torch.mean(torch.vstack([spk_utt_states[spk][temp] for temp in range(len(spk_utt_states[spk])) if temp != ids]), 0)\n","                    \n","                    pos_mat_mul = torch.einsum('i, j->', spk_utt_states[spk][ids], spk_mean_exc)\n","                    pos_sigm = torch.sigmoid(pos_mat_mul)\n","                    pos_log = torch.log(pos_sigm)\n","                    L_pos += torch.sum(-1 * pos_log)\n","\n","                    # negative sample\n","\n","                    new_uniq_spks = uniq_spks.copy()\n","                    new_uniq_spks.remove(spk)\n","                    \n","                    spk2 = random.choice(new_uniq_spks)\n","                    id_neg = random.choice(list(range(len(spk_utt_states[spk2]))))\n","                    neg_mat_mul = torch.einsum('i, j->', spk_utt_states[spk2][id_neg], spk_mean_exc)\n","                    neg_sigm = torch.sigmoid(neg_mat_mul)\n","                    neg_log = torch.log(1 - neg_sigm+1e-5)\n","                    L_neg += torch.sum(-1 * neg_log)\n","                \n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            # wandb.log({\"batch_scl-global\": batch_scl})\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        spk_utt_pos: Optional[torch.Tensor] = None, ##changed here\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        decoder_head_mask: Optional[torch.FloatTensor] = None,\n","        cross_attn_head_mask: Optional[torch.Tensor] = None,\n","        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n","            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n","            labels in `[0, ..., config.vocab_size]`\n","        Returns:\n","        \"\"\"\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n","        if head_mask is not None and decoder_head_mask is None:\n","            if self.config.num_layers == self.config.num_decoder_layers:\n","                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n","                decoder_head_mask = head_mask\n","\n","        # Encode if needed (training, first prediction pass)\n","        if encoder_outputs is None:\n","            # Convert encoder inputs in embeddings if needed\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","            turn_attention_mask=None\n","            token_encoder_outputs=None\n","            tog_encoder_outputs=None\n","            \n","            if 'token' in self.SCLossesList:\n","                token_encoder_outputs = self.encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","\n","            if 'turn' in self.SCLossesList or 'global' in self.SCLossesList:\n","                tog_attention_mask = torch.where(spk_utt_pos>0, 0, attention_mask)\n","                tog_encoder_outputs = self.encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=tog_attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","            \n","        hidden_states = encoder_outputs[0]\n","\n","        if self.model_parallel:\n","            torch.cuda.set_device(self.decoder.first_device)\n","\n","        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n","            # get decoder inputs from shifting lm labels to the right\n","            decoder_input_ids = self._shift_right(labels)\n","\n","        # Set device for model parallelism\n","        if self.model_parallel:\n","            torch.cuda.set_device(self.decoder.first_device)\n","            hidden_states = hidden_states.to(self.decoder.first_device)\n","            if decoder_input_ids is not None:\n","                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n","            if attention_mask is not None:\n","                attention_mask = attention_mask.to(self.decoder.first_device)\n","            if decoder_attention_mask is not None:\n","                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n","\n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=attention_mask,\n","            head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = decoder_outputs[0]\n","\n","        # Set device for model parallelism\n","        if self.model_parallel:\n","            torch.cuda.set_device(self.encoder.first_device)\n","            self.lm_head = self.lm_head.to(self.encoder.first_device)\n","            sequence_output = sequence_output.to(self.lm_head.weight.device)\n","\n","        if self.config.tie_word_embeddings:\n","            # Rescale output before projecting on vocab\n","            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n","            sequence_output = sequence_output * (self.model_dim**-0.5)\n","\n","        lm_logits = self.lm_head(sequence_output)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            masked_lm_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n","            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n","        \n","        \n","        # added here\n","        sc_loss = 0\n","        if 'token' in self.SCLossesList and labels is not None:\n","            sc_loss += self.token_scl(last_hidden_state=token_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","            # print(sc_loss)\n","        if 'turn' in self.SCLossesList and labels is not None:\n","            sc_loss += self.turn_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if 'global' in self.SCLossesList and labels is not None:\n","            sc_loss += self.global_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if not return_dict:\n","            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n","            return ((masked_lm_loss+(self.scl_coeff*sc_loss),) + output) if loss is not None else output\n","\n","        loss = None\n","        if masked_lm_loss is None:\n","            loss = None\n","        else:\n","            loss = masked_lm_loss+(self.scl_coeff*sc_loss)        \n","\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            cross_attentions=decoder_outputs.cross_attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )\n","\n","    "]},{"cell_type":"code","execution_count":19,"metadata":{"id":"0K6zHZKL7Tle","executionInfo":{"status":"ok","timestamp":1670446132066,"user_tz":300,"elapsed":3848,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["model_checkpoint = '/content/t5-token-b4c0.1/checkpoint-18000'\n","model = T5WithSCL.from_pretrained(model_checkpoint)\n","model.set_losses_list(['global'])\n","model.set_scl_coeff(0.1)\n"]},{"cell_type":"markdown","metadata":{"id":"fZNKwHmTpjxX"},"source":["### Fine-tuning the model"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"37a4Gih6lB8t","executionInfo":{"status":"ok","timestamp":1670446132068,"user_tz":300,"elapsed":34,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["# Parameters\\\n","batch_size=4\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"t5-token-b4c0.1\",\n","    num_train_epochs=5,\n","    do_train=True,\n","    do_eval=True,\n","    evaluation_strategy = \"epoch\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    learning_rate=1e-4,\n","    warmup_steps=500,\n","    weight_decay=0.1,\n","    # label_smoothing_factor=0.1, ## causes to throw an error\n","    predict_with_generate=True,\n","    # logging_dir=\"logs\",\n","    logging_steps=10,\n","    # save_total_limit=3,\n",")\n","\n","\n","data_collator = CustomCollatorForSeq2Seq(tokenizer, model=model)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"UmvbnJ9JIrJd","executionInfo":{"status":"ok","timestamp":1670446132070,"user_tz":300,"elapsed":27,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["import nltk\n","import numpy as np\n","import torch\n","torch.cuda.empty_cache()\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    for i in range(0,50):\n","      # print(tokenized_datasets_val[\"dialogue\"][i])\n","      print(\"-----------\",i,\"--------------\")\n","      print(\"------>Predictions by Model\")\n","      print(decoded_preds[i])\n","      print(\"----->Predictions Original\")\n","      print(decoded_labels[i])\n","      print(\"**************************\")\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"fUlQNc7i7ziP","executionInfo":{"status":"ok","timestamp":1670446137896,"user_tz":300,"elapsed":5848,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets_train,\n","    eval_dataset=tokenized_datasets_val,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"2xixI4gdbuoe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670446137898,"user_tz":300,"elapsed":48,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"6f0a3ccd-812d-45ac-d2d7-0da6961880bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kXjzgbKlVAqN","executionInfo":{"status":"error","timestamp":1670456486372,"user_tz":300,"elapsed":10028762,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}},"outputId":"5c16fc5b-7ae5-4fe2-f18a-4ae851e3b7c1"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 14730\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18415\n","  Number of trainable parameters = 222903552\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15501' max='18415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15501/18415 2:47:07 < 31:25, 1.55 it/s, Epoch 4.21/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.444200</td>\n","      <td>1.646008</td>\n","      <td>48.516100</td>\n","      <td>24.951000</td>\n","      <td>40.506700</td>\n","      <td>44.845600</td>\n","      <td>17.105100</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.238700</td>\n","      <td>1.691065</td>\n","      <td>48.525800</td>\n","      <td>25.257600</td>\n","      <td>40.767600</td>\n","      <td>44.935000</td>\n","      <td>16.968200</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.071100</td>\n","      <td>1.743156</td>\n","      <td>48.639800</td>\n","      <td>24.648000</td>\n","      <td>40.424600</td>\n","      <td>44.610400</td>\n","      <td>16.971900</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.036300</td>\n","      <td>1.819814</td>\n","      <td>48.943100</td>\n","      <td>25.361500</td>\n","      <td>40.660300</td>\n","      <td>44.929700</td>\n","      <td>17.218800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-500\n","Configuration saved in t5-token-b4c0.1/checkpoint-500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-1000\n","Configuration saved in t5-token-b4c0.1/checkpoint-1000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-1500\n","Configuration saved in t5-token-b4c0.1/checkpoint-1500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-1500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-2000\n","Configuration saved in t5-token-b4c0.1/checkpoint-2000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-2000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-2500\n","Configuration saved in t5-token-b4c0.1/checkpoint-2500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-2500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-3000\n","Configuration saved in t5-token-b4c0.1/checkpoint-3000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-3000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-3500\n","Configuration saved in t5-token-b4c0.1/checkpoint-3500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-3500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='559' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [205/205 45:13]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A will take her son to the animal shelter tomorrow afternoon.\n","Tom took him there last Monday and\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to buy an advent calendar for her children.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant.\n","Iggy's friend is pregnant.\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found underwear that she had arranged with her sister.\n","Kiki thinks it\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert needs to buy guitar cable.\n","Fred sends him the address of the music shop.\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Keith will buy some milk and cereals for Megan.\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised that she is making a noise.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited Luis and Marion for a dinner at Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Jane wants to make a reservation for 6 people under her name tonight at 21:00.\n","Vegan\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy is working in Texas.\n","She's having kids who laugh at her accent.\n","Nancy is\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura needs a new printer.\n","Jamie suggests buying a second hand one.\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything she needed.\n","Haylee is in dairy section.\n","Haylee can'\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Norbert and Wendy are going to the tour.\n","Wendy is buying something.\n","Norbert is patient\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Peter took some photos of the Jandia Peninsula.\n","Cheryl and Cecil are going\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie hasn't found it.\n","Nickola is looking for pockets and handbags\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Rosie has to write an essay on bad movies.\n","Elle recommends \"Plan 9 from outer\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","James has a dream to become a voice actor.\n","He has worked in radio during college\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Poppy and Alice are going out for a beer at Nick's around 5:30.\n","Fred\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Caron is out from 12 and will be before Sash.\n","Caron needs to go out\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed with Gosia because she likes football and video games.\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Mom is calling for supper.\n","Jannette will come to Ramzi.\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer got the recipe from her grandmother.\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence will be through with the article in a few minutes.\n","The piece is needed by today\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent Brennen a photo.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie will borrow Chloe's bike again on Wednesday evening.\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Carter wants to launch a restaurant business next month.\n","Olivia is involved with a new restaurant\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kenny has another surgery scheduled for tomorrow.\n","Guy will come to the hospital with Kristine\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up after years of dating.\n","He cheated on Olivia with numerous\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda is pregnant.\n","She went dancing classes with Michael yesterday.\n","The instructor needed a partner to\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Taylor hasn't introduced his boyfriend to Isabel.\n","Isabel hasn't had any.\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is leaving on Friday.\n","Toby will join him for the weekend in the Italian\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Brandon hasn't called to say he would be late.\n","Phil will prepare a\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie is sick again and she should cancel her appointment with Olga.\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Diane is afraid of her daughter Lorelai being born.\n","Kate is happy for her.\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold.\n","Daniel will pick up some stuff on his way back from the pharmacy\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","John, Alex and Sam are watching 'Millionaires' on tv.\n","John\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Gwen and Sophie will meet at the restaurant in 15 minutes.\n","Sophie is waiting for\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way to meet with Sue.\n","She is going downstairs now.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty will buy some Asian salmon for din din.\n","George will pan fry the salmon and\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken installed Endomondo and it works fine.\n","Ken had a problem with the battery on\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ivan hasn't been to Ann's birthday party.\n","Ann will come next time\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Ashley posted some pictures of herself on her Facebook page.\n","Rowan is busy with other things.\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj has a working permission to work on his wife's papers.\n","He\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Ann is at the red table on the 3rd floor lobby.\n","Thomas and Maria will have\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim right now.\n","She will go to the Museum of the\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","She might come to Broke at\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla received 250.\n","She will let Adrian know when she has checked.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-4000\n","Configuration saved in t5-token-b4c0.1/checkpoint-4000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-4000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-4500\n","Configuration saved in t5-token-b4c0.1/checkpoint-4500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-4500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-5000\n","Configuration saved in t5-token-b4c0.1/checkpoint-5000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-5000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-5500\n","Configuration saved in t5-token-b4c0.1/checkpoint-5500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-5500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-6000\n","Configuration saved in t5-token-b4c0.1/checkpoint-6000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-6000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-6500\n","Configuration saved in t5-token-b4c0.1/checkpoint-6500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-6500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-7000\n","Configuration saved in t5-token-b4c0.1/checkpoint-7000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-7000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A will take her son to the animal shelter tomorrow afternoon.\n","He wants to get a puppy\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to buy an advent calendar for her children.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant but she doesn't want to talk about it.\n","Iggy's friend\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found some male underwear under her bed.\n","She is looking for it.\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert is looking for a music shop.\n","Fred sends him the address.\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Keith and Megan have run out of cereals and milk.\n","Megan will buy them.\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised that she is making a noise.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited Luis and Marion for a dinner at Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Jane is booking a table for 6 people under her name tonight at 21:00.\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy is a Texan and loves her job in Texas.\n","She's coming home\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura is thinking about buying a new printer.\n","Jamie suggests she buys a second hand\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything she needed for her shopping trip.\n","Haylee is in the dairy section but\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Norbert and Wendy are going on a tour.\n","Norbert is going to hurry up and\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Cecil is driving to the Jandia Peninsula.\n","He liked it a lot,\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie hasn't found it.\n","Nickola recommends checking pockets and handbags\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Rosie is writing an essay on bad movies.\n","She's going to watch \"Plan 9\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","James has a dream to become a voice actor.\n","Julia wants to listen to him as\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Poppy and Alice are going out for a beer at Nick's around 5:30.\n","Fred\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Caron is out of time.\n","Sash will be before 12.\n","Caron needs to go out\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed by Gosia because she criticizes him for playing video\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Jannette will join Ramzi for supper.\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer is preparing ravioli.\n","She got the recipe from her grandmother.\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence is going to finish the article today.\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent a photo to Brennen.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Carter is developing restaurant business.\n","He wants to launch next month.\n","Olivia is involved with a\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kenny is back from surgery.\n","He has another surgery scheduled tomorrow.\n","Kristine is scared\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up after a long time.\n","Skyler met Olivia 2 days ago\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda is pregnant.\n","She went to dancing classes with Michael yesterday.\n","She went with a guy\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Isabel hasn't introduced her boyfriend to Taylor.\n","Taylor's friends' daughters bring their\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is leaving on Friday.\n","Toby will join him for the weekend in the Italian\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Brandon hasn't called to say he'd be late.\n","Phil wants him to\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie is sick again and she cancels her scheduled appointment with Olga.\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Diane is afraid of her children's birth.\n","Kate will be her Lorelai.\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold.\n","Daniel will pick up some stuff from the pharmacy on his way back\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","Alex and Sam are watching 'Millionaires' on TV.\n","John and Alex are going\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Sophie is waiting for a client.\n","She will meet with Gwen at the restaurant in\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way.\n","Sue is going downstairs now.\n","Daniel is with the Volvo.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty will buy some Asian food for din din.\n","George will pan fry the salmon and\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken installed endomondo and wanted to track his progress.\n","It didn't work properly\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ivan hasn't been to Ann's birthday party.\n","He's bought Ann\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Ashley posted some pictures of herself on Facebook.\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj's wife's papers with the working permission were supposed to be sent\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Ann and Thomas are at the hotel.\n","They will have lunch together.\n","Peter is still at the\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim and wants to go to the Museum of the city of\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","She might come to Broke at\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla received 250.\n","She will let Adrian know when she has checked.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-7500\n","Configuration saved in t5-token-b4c0.1/checkpoint-7500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-7500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-8000\n","Configuration saved in t5-token-b4c0.1/checkpoint-8000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-8000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-8500\n","Configuration saved in t5-token-b4c0.1/checkpoint-8500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-8500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-9000\n","Configuration saved in t5-token-b4c0.1/checkpoint-9000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-9000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-9500\n","Configuration saved in t5-token-b4c0.1/checkpoint-9500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-9500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-10000\n","Configuration saved in t5-token-b4c0.1/checkpoint-10000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-10000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-10500\n","Configuration saved in t5-token-b4c0.1/checkpoint-10500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-10500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-11000\n","Configuration saved in t5-token-b4c0.1/checkpoint-11000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-11000/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A wants to get a puppy for her son.\n","Tom took him to the animal shelter last\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to get an advent calendar for her children.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant but she doesn't want to talk about it.\n","Iggy's friend\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found some male underwear under her bed.\n","Kiki and Tamara are investigating her\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert is looking for a music shop.\n","Fred sends him the address.\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Megan will buy some cereal and milk for Keith.\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised by the noise.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited Luis and Marion for dinner in Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Jane wants to book a table for six under her name tonight at 21:00.\n","Vegano\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy is a student at a Welsh school.\n","Nancy's kids laugh at her accent\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura is thinking about buying a new printer.\n","Jamie suggests buying a second hand one.\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything she needed for yoghurt.\n","Haylee is in the dairy section\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Norbert is waiting for the tour.\n","Wendy is buying something.\n","She's already there.\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Cecil is driving Lidia to the Jandia Peninsula.\n","Peter took pictures of it\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie has checked pockets and handbags twice already.\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Elle recommends \"Plan 9 from outer space\" and \"The Room\" to Rosie.\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","Julia wants to listen to James as a radio speaker.\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Alice and Poppy are going out for a beer at Nick's around 5:30.\n","Fred\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Caron is out of time.\n","Sash will come before him.\n","Caron needs to go\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed by Gosia because she criticizes him for playing video\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Jannette will join Ramzi for supper.\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer got the recipe from her grandmother and she's making ravioli.\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence will get back to Madison after finishing the article.\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent Brennen a photo.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Carter wants to launch a restaurant next month.\n","Olivia is involved with a new restaurant in\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kristine is scared she might lose her son Kenny.\n","He has another surgery scheduled tomorrow\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up after a long time.\n","Skyler met Olivia 2 days ago\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda went to dancing classes with Michael yesterday.\n","The instructor needed a partner and no one knew\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Isabel hasn't introduced Taylor to her boyfriend yet.\n","Taylor's friends' daughters bring\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is leaving on Friday before 7 am.\n","Toby will join them in the Italian\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Brandon hasn't called to say he would be late.\n","Phil wants him to come\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie cancelled her scheduled appointment with Olga because she's sick again.\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Kate will be Diane's Lorelai.\n","Diane is scared of the birth but she\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold.\n","Daniel will pick him up from the pharmacy on his way back.\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","Alex and Sam are watching 'Millionaires' on tv.\n","John has his\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Sophie is waiting for a 40-minute late client.\n","Sophie and Gwen will meet\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way.\n","Sue is going downstairs now.\n","Daniel is with the Volvo.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty and George will have Asian salmon for din din.\n","George will pan fry the salmon and\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken's phone doesn't work properly.\n","Ken had it with him while jogg\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ivan bought Ann a birthday present.\n","He will come next time.\n","They are meeting next\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Ashley has posted some pictures of herself on her facebook page.\n","Rowan doesn't like them\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj's wife's papers were supposed to be sent to him, but\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Ann and Thomas are at the hotel.\n","Peter is still at the hotel.\n","They will have lunch\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim Museum right now.\n","She will meet Terry and Kai for\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","She might come over at 10 tonight\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla received 250.\n","She will let Adrian know when she has checked.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-11500\n","Configuration saved in t5-token-b4c0.1/checkpoint-11500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-11500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-12000\n","Configuration saved in t5-token-b4c0.1/checkpoint-12000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-12000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-12500\n","Configuration saved in t5-token-b4c0.1/checkpoint-12500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-12500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-12500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-12500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-13000\n","Configuration saved in t5-token-b4c0.1/checkpoint-13000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-13000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-13000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-13000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-13500\n","Configuration saved in t5-token-b4c0.1/checkpoint-13500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-13500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-13500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-13500/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-14000\n","Configuration saved in t5-token-b4c0.1/checkpoint-14000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-14000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-14000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-14000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-14500\n","Configuration saved in t5-token-b4c0.1/checkpoint-14500/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-14500/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-14500/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-14500/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 4\n"]},{"output_type":"stream","name":"stdout","text":["----------- 0 --------------\n","------>Predictions by Model\n","A wants to get a puppy for her son.\n","He likes the one that A took\n","----->Predictions Original\n","A will go to the animal shelter tomorrow to get a puppy for her son.\n","They already visited the shelter last Monday and the son chose the puppy.\n","**************************\n","----------- 1 --------------\n","------>Predictions by Model\n","Emma wants to get an advent calendar for her children.\n","Rob used to get one every year as\n","----->Predictions Original\n","Emma and Rob love the advent calendar.\n","Lauren fits inside calendar various items, for instance, small toys and Christmas decorations.\n","Her children are excited whenever they get the calendar.\n","**************************\n","----------- 2 --------------\n","------>Predictions by Model\n","Madison is pregnant but she doesn't want to talk about it.\n","Iggy's friend\n","----->Predictions Original\n","Madison is pregnant but she doesn't want to talk about it.\n","Patricia Stevens got married and she thought she was pregnant.\n","**************************\n","----------- 3 --------------\n","------>Predictions by Model\n","Marla found some male underwear under her bed.\n","Kiki thinks it might be her\n","----->Predictions Original\n","Marla found a pair of boxers under her bed.\n","**************************\n","----------- 4 --------------\n","------>Predictions by Model\n","Robert needs to buy guitar cable.\n","Fred sends him the address of the music shop.\n","----->Predictions Original\n","Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\n","**************************\n","----------- 5 --------------\n","------>Predictions by Model\n","Keith wants Megan to buy some milk and cereals.\n","Megan didn't check in\n","----->Predictions Original\n","Megan needn't buy milk and cereals.\n","They're in the drawer next to the fridge.\n","**************************\n","----------- 6 --------------\n","------>Predictions by Model\n","Samantha and Evelyn are surprised by the noise.\n","----->Predictions Original\n","Samantha and Evelyn after watching the video cannot believe she is able to make that noise.\n","**************************\n","----------- 7 --------------\n","------>Predictions by Model\n","Tom invited Luis and Marion for a dinner in Fiesole.\n","----->Predictions Original\n","Tom's new place is in Fiesole.\n","Luis and Marion has been there.\n","**************************\n","----------- 8 --------------\n","------>Predictions by Model\n","Jane wants to book a table for six under her name tonight at 21:00.\n","Vegano\n","----->Predictions Original\n","Jane made a 9 PM reservation for 6 people tonight at Vegano Resto.\n","**************************\n","----------- 9 --------------\n","------>Predictions by Model\n","Nancy is enjoying her job in Texas.\n","She's been living in Cardiff for 21 years.\n","----->Predictions Original\n","Nancy's working in Texas, but the kids laugh at her Welsh accent.\n","She's coming home in 6 weeks.\n","Earlier than that she's going to travel with 3 other Brits.\n","**************************\n","----------- 10 --------------\n","------>Predictions by Model\n","Laura needs a new printer.\n","Jamie suggests buying a second hand one.\n","----->Predictions Original\n","Laura is going to buy a printer.\n","**************************\n","----------- 11 --------------\n","------>Predictions by Model\n","Barbara got everything she needed.\n","Haylee is in the dairy section but she can't\n","----->Predictions Original\n","Haylee can't find the coconut milk yoghurt.\n","**************************\n","----------- 12 --------------\n","------>Predictions by Model\n","Norbert and Wendy are going to catch the tour.\n","Norbert is going to be right out\n","----->Predictions Original\n","Wendy is shopping, but she needs to hurry up to catch the tour.\n","**************************\n","----------- 13 --------------\n","------>Predictions by Model\n","Cecil is driving Lidia to the Jandia Peninsula.\n","Peter took pictures of it\n","----->Predictions Original\n","Cecil, Cheryl and Peter went to the Jandia Peninsula today.\n","Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner.\n","**************************\n","----------- 14 --------------\n","------>Predictions by Model\n","Sophie has checked pockets and handbags twice already.\n","----->Predictions Original\n","Sophie still hasn't found it despite checking pockets and handbags twice.\n","**************************\n","----------- 15 --------------\n","------>Predictions by Model\n","Elle and Dennis recommend Rosie some bad movies.\n","----->Predictions Original\n","Dennis and Elle are helping Rosie think of bad movies for her essay.\n","**************************\n","----------- 16 --------------\n","------>Predictions by Model\n","Julia wants to listen to James as a radio speaker.\n","James has worked in radio during college\n","----->Predictions Original\n","James has a dream of becoming a voice actor.\n","He considers making a home radio station.\n","**************************\n","----------- 17 --------------\n","------>Predictions by Model\n","Poppy and Alice had a long day.\n","Poppy is going for drinks after work.\n","----->Predictions Original\n","Poppy and Alice are meeting for drinks after work at Nick's at 5:30.\n","Alice fancies Fred, she will invite him and a bunch of other coworkers.\n","**************************\n","----------- 18 --------------\n","------>Predictions by Model\n","Caron is out of time.\n","Sash will be there before 12.\n","Caron needs to go\n","----->Predictions Original\n","Sash needs to see Caron who'll be out from 12.\n","**************************\n","----------- 19 --------------\n","------>Predictions by Model\n","Matteo is annoyed by Gosia because she criticizes him for playing video\n","----->Predictions Original\n","Matteo is not sure about his relationship with Gosia but likes her a lot.\n","**************************\n","----------- 20 --------------\n","------>Predictions by Model\n","Jannette will join Ramzi for supper.\n","----->Predictions Original\n","Ramzi and Jannette are going for supper.\n","**************************\n","----------- 21 --------------\n","------>Predictions by Model\n","Jeniffer got the recipe from her grandmother and she's making ravioli.\n","----->Predictions Original\n","Jeniffer is preparing ravioli following her grandmothers recipe.\n","**************************\n","----------- 22 --------------\n","------>Predictions by Model\n","Lawrence will get back to Madison after finishing the article.\n","----->Predictions Original\n","Lawrence will finish writing the article soon.\n","**************************\n","----------- 23 --------------\n","------>Predictions by Model\n","Chad sent Brennen a photo.\n","----->Predictions Original\n","Chad has sent Brennen a funny photo.\n","Brennen does not find it very funny.\n","**************************\n","----------- 24 --------------\n","------>Predictions by Model\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","----->Predictions Original\n","Sadie will borrow Chloe's bike on Wednesday evening.\n","She has a dentist appointment on Thursday after work.\n","**************************\n","----------- 25 --------------\n","------>Predictions by Model\n","Carter is developing restaurant business.\n","He wants to launch next month.\n","Olivia is involved with a\n","----->Predictions Original\n","Carter is launching a restaurant business next month.\n","Olivia wants him to include a restaurant she's working for in the discount app.\n","They will meet in person to discuss it.\n","**************************\n","----------- 26 --------------\n","------>Predictions by Model\n","Kenny is back from surgery.\n","He has another surgery scheduled tomorrow.\n","Kristine is scared\n","----->Predictions Original\n","Kenny had a surgery, as Kristine reports.\n","He will have another surgery tomorrow.\n","Guy will come to St. Mark's Hospital near Asda to stay with Kristine.\n","**************************\n","----------- 27 --------------\n","------>Predictions by Model\n","Joey and Olivia broke up after a long time.\n","Skyler met Olivia 2 days ago\n","----->Predictions Original\n","Skyler and Adam are surprised that Joey and Olivia broke up.\n","**************************\n","----------- 28 --------------\n","------>Predictions by Model\n","Amanda went to dancing classes with Michael yesterday and the instructor needed a partner to teach her the\n","----->Predictions Original\n","Amanda goes to dancing classes with Michael.\n","She volunteered to show the English Waltz steps with the instructor yesterday.\n","Amanda is shy and goes to therapy.\n","**************************\n","----------- 29 --------------\n","------>Predictions by Model\n","Taylor is upset because Isabel hasn't introduced her boyfriend to her.\n","----->Predictions Original\n","Taylor wants to meet Isabel's boyfriend but she has never had any.\n","**************************\n","----------- 30 --------------\n","------>Predictions by Model\n","Theo is going to the Italian Alps before 7 am on Friday.\n","Toby will\n","----->Predictions Original\n","Theo's going to stay near Torino in the region of Italian Alpes.\n","Toby wants to join the trip.\n","Theo agrees and will pick Toby up on Friday at 7 am.\n","**************************\n","----------- 31 --------------\n","------>Predictions by Model\n","Brandon hasn't called to say he would be late.\n","Clara will tell Brandon\n","----->Predictions Original\n","Brandon is late again.\n","Clara will prepare a report on the absenteeism and lateness for Phil by Friday.\n","**************************\n","----------- 32 --------------\n","------>Predictions by Model\n","Suzie cancelled her last time because she got sick again.\n","Olga will res\n","----->Predictions Original\n","Olga and Suzie will postpone their meeting due to Suzie's sickness.\n","**************************\n","----------- 33 --------------\n","------>Predictions by Model\n","Diane is afraid of her children having a baby.\n","Kate is going to be Lorela\n","----->Predictions Original\n","Diane is pregnant and can't wait to give birth, she thinks the waiting is the worst.\n","Kate thinks she'll be an amazing mother.\n","**************************\n","----------- 34 --------------\n","------>Predictions by Model\n","Andrew has a cold.\n","Daniel will pick up some stuff from the pharmacy on his way back\n","----->Predictions Original\n","Andrew has a cold.\n","Daniel will buy him some medication.\n","**************************\n","----------- 35 --------------\n","------>Predictions by Model\n","Alex and Sam are watching 'Millionaires' on tv.\n","John has his\n","----->Predictions Original\n","Alex and Sam are watching Millionaires.\n","**************************\n","----------- 36 --------------\n","------>Predictions by Model\n","Angelica has the cinnamon cookies recipe.\n","----->Predictions Original\n","Angelica sent the cinnamon cookies recipe at Kelly's request.\n","**************************\n","----------- 37 --------------\n","------>Predictions by Model\n","Sophie is waiting for a client.\n","He's already 40 minutes late.\n","Gwen\n","----->Predictions Original\n","Sophie is waiting for a client, who is late.\n","She will meet Gwen later.\n","**************************\n","----------- 38 --------------\n","------>Predictions by Model\n","Daniel is on his way.\n","Sue is going downstairs now.\n","Daniel is with the Volvo.\n","----->Predictions Original\n","Daniel is with the Volvo on his way and will be there soon.\n","Sue is going downstairs to meet him.\n","**************************\n","----------- 39 --------------\n","------>Predictions by Model\n","Betty and George will have Asian salmon for din din.\n","George will pan fry the salmon and\n","----->Predictions Original\n","George is making salmon and stuffed squash for dinner.\n","Betty will buy a shaving cream at CVS at his request,\n","**************************\n","----------- 40 --------------\n","------>Predictions by Model\n","Ken's phone doesn't work properly.\n","Ken had it with him while jogg\n","----->Predictions Original\n","Ken has installed an app for running but it is not working properly on his phone.\n","**************************\n","----------- 41 --------------\n","------>Predictions by Model\n","Ivan bought Ann a present for her birthday.\n","He will come next time.\n","They are\n","----->Predictions Original\n","Ivan and Ann will meet next week.\n","**************************\n","----------- 42 --------------\n","------>Predictions by Model\n","Ashley has posted some pictures of herself on her Facebook page.\n","Rowan doesn't know that\n","----->Predictions Original\n","Ashley posted some nude photos on her fb page.\n","**************************\n","----------- 43 --------------\n","------>Predictions by Model\n","Mikolaj's wife's papers with the working permission are not ready.\n","The\n","----->Predictions Original\n","Mikolaj's wife needs a work permit as a foreigner.\n","Government officials missed the deadline for sending it and will need another month.\n","**************************\n","----------- 44 --------------\n","------>Predictions by Model\n","Maria, Ann and Thomas are meeting in the hotel lobby on the 3rd floor.\n","Peter\n","----->Predictions Original\n","Thomas, Ann and Maria will have lunch together at the hotel.\n","Ann is already in the 3rd floor lobby at the red table.\n","**************************\n","----------- 45 --------------\n","------>Predictions by Model\n","Sus and Val are sleepy.\n","----->Predictions Original\n","Sus and Val don't want to work and are sleepy.\n","**************************\n","----------- 46 --------------\n","------>Predictions by Model\n","Kate is at the Guggenheim Museum right now.\n","She will meet Terry and Kai for\n","----->Predictions Original\n","Kate is at the Guggenheim Museum now, but will be in the Museum of the City of New York around 2-2:30.\n","Kai may join her.\n","Ish won't.\n","Terry will join them for a coffee after they finish visiting the museum.\n","Terry has already seen the museum.\n","**************************\n","----------- 47 --------------\n","------>Predictions by Model\n","Cathy left her sunglasses at Broke's house.\n","She might come to pick them up\n","----->Predictions Original\n","Cathy left her sunglasses at Broke's house.\n","She will come collect them at 10.\n","**************************\n","----------- 48 --------------\n","------>Predictions by Model\n","Frederica will come to Bradley's birthday party tomorrow at 8 pm.\n","----->Predictions Original\n","Bradley will come to Frederica's birthday party tomorrow at 8pm.\n","**************************\n","----------- 49 --------------\n","------>Predictions by Model\n","Camilla received 250.\n","She will let Adrian know when she has checked.\n","----->Predictions Original\n","Camilla still hasn't received the 250.\n","She will check and let Adrian know.\n","Money usually takes around two days to arrive.\n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to t5-token-b4c0.1/checkpoint-15000\n","Configuration saved in t5-token-b4c0.1/checkpoint-15000/config.json\n","Model weights saved in t5-token-b4c0.1/checkpoint-15000/pytorch_model.bin\n","tokenizer config file saved in t5-token-b4c0.1/checkpoint-15000/tokenizer_config.json\n","Special tokens file saved in t5-token-b4c0.1/checkpoint-15000/special_tokens_map.json\n","Saving model checkpoint to t5-token-b4c0.1/checkpoint-15500\n","Configuration saved in t5-token-b4c0.1/checkpoint-15500/config.json\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 122337792 vs 122337680","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m         )\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1850\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2174\u001b[0m         \u001b[0mrun_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_output_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_internal_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0;31m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0;31m# Push to the Hub when `save_model` is called by the user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2705\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2707\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, **kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m                 \u001b[0msafe_save_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m                 \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system"]}]},{"cell_type":"code","execution_count":25,"metadata":{"id":"l24RBo5J75kp","executionInfo":{"status":"ok","timestamp":1670446319373,"user_tz":300,"elapsed":14,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["# trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5RY4ZQIDrmWr","executionInfo":{"status":"aborted","timestamp":1670456485698,"user_tz":300,"elapsed":25,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"outputs":[],"source":["# evaluate before training for comparison\n","trainer.evaluate()"]},{"cell_type":"code","source":["# torch.save(model.state_dict(), \"./best_model.bin\")"],"metadata":{"id":"GvJXL5br0VUZ","executionInfo":{"status":"aborted","timestamp":1670446450236,"user_tz":300,"elapsed":14,"user":{"displayName":"Sai Tanmay Reddy","userId":"13202642314045427358"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"dd2256311355469d9a2f87d8fc59787b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a12d7867bf27487d939cde6f7a3d357e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f3360edeb6344aa98206772291806d7d","IPY_MODEL_f0a749af013047eca5d70b4836ba44b5","IPY_MODEL_b3bf9230d95d4d309facc82cf6feac35"]}},"a12d7867bf27487d939cde6f7a3d357e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f3360edeb6344aa98206772291806d7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f2c2166743664ccf91a43e4d8c258e2c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1d5049a63f94b7c9eda9afa9688b151"}},"f0a749af013047eca5d70b4836ba44b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_33ce63d9a9ea4d90a6853355de364d97","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0abacb02160a46a78d077185e9473786"}},"b3bf9230d95d4d309facc82cf6feac35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa7f73136c20497e9ff26499244cafc6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/3 [00:00&lt;00:00,  7.10it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_351b2dd236f74715a447ebb8ea52e3be"}},"f2c2166743664ccf91a43e4d8c258e2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e1d5049a63f94b7c9eda9afa9688b151":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33ce63d9a9ea4d90a6853355de364d97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0abacb02160a46a78d077185e9473786":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa7f73136c20497e9ff26499244cafc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"351b2dd236f74715a447ebb8ea52e3be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d8eb4ffed2c4f0f90f29eefa3552a66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cf738784725a4cd7bf0055ff6cba0497","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_565ced4b904d4eef916aaf382451f56a","IPY_MODEL_34c429a1248f4fa69f8caf02953a4d14","IPY_MODEL_a55ec348cff7404e98730e009018a836"]}},"cf738784725a4cd7bf0055ff6cba0497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"565ced4b904d4eef916aaf382451f56a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9dbc020d6b97490381e7546d37ea1174","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5468708d8644478195aa0a1e2faf9147"}},"34c429a1248f4fa69f8caf02953a4d14":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_62ebb0fee47f426bb972a26f2850b16e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e6cef97c9e8b456a8a5753d4b36f9722"}},"a55ec348cff7404e98730e009018a836":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_71af572901024dd9bb04d603cfbc1447","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.37s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_48c3750ebc5f49ee8223cd40191a8f90"}},"9dbc020d6b97490381e7546d37ea1174":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5468708d8644478195aa0a1e2faf9147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"62ebb0fee47f426bb972a26f2850b16e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e6cef97c9e8b456a8a5753d4b36f9722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"71af572901024dd9bb04d603cfbc1447":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"48c3750ebc5f49ee8223cd40191a8f90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}