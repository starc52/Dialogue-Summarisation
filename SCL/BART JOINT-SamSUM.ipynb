{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oK_TzYwpd1LqlSQBx12p_j3JTVWrRSqB","timestamp":1670085949813},{"file_id":"1ahseFGDVwY8yEmegHemMCoQC2houCqpA","timestamp":1668264569189}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"95e1a5850b19402993a2ffce7f057d5c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_17d7269b5ba642b99d0e3c64a92a80b4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e07963cbb466412597e2b710fd47ea53","IPY_MODEL_06704d58761f470f8fd4114815110734","IPY_MODEL_6a6a04d11ffb42b48427d429de14c07a"]}},"17d7269b5ba642b99d0e3c64a92a80b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e07963cbb466412597e2b710fd47ea53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d0c6eea84fd14152bb016090d9161644","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ab1e01d4a704ee4845de03effc33642"}},"06704d58761f470f8fd4114815110734":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fd2b8009c5fd4d8882ed0fffb8e80cd5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9cb44be075b041f4817244db9a7f4d7f"}},"6a6a04d11ffb42b48427d429de14c07a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_718a1dc627d44b3292ccac10ee19a166","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/3 [00:00&lt;00:00,  8.62it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e30b047a13414769a11539fd75af0b59"}},"d0c6eea84fd14152bb016090d9161644":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ab1e01d4a704ee4845de03effc33642":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd2b8009c5fd4d8882ed0fffb8e80cd5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9cb44be075b041f4817244db9a7f4d7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"718a1dc627d44b3292ccac10ee19a166":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e30b047a13414769a11539fd75af0b59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"gMzbM4naD2g9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import random\n","\n","def set_random_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","set_random_seed(0)"],"metadata":{"id":"faaqh5xzw8pI"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOsHUjgdIrIW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7535ac3f-3918-4652-d9f2-3e271825199b","executionInfo":{"status":"ok","timestamp":1670448016960,"user_tz":300,"elapsed":3027,"user":{"displayName":"Hiteshwar singh","userId":"17502675958375875233"}}},"source":["! pip install datasets transformers rouge-score nltk py7zr"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.7.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.25.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: py7zr in /usr/local/lib/python3.7/dist-packages (0.20.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.10.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n","Requirement already satisfied: texttable in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.6.7)\n","Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from py7zr) (5.4.8)\n","Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.15.3)\n","Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.3.1)\n","Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.9)\n","Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.2.3)\n","Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from py7zr) (3.16.0)\n","Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"e-v6Mjk_JvG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %cd /content/drive/MyDrive/NLP Project with SCL"],"metadata":{"id":"z5nOd8wBL-BO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Fine-tuning a model on a summarization task"]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","metadata":{"id":"IreSlFmlIrIm","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["95e1a5850b19402993a2ffce7f057d5c","17d7269b5ba642b99d0e3c64a92a80b4","e07963cbb466412597e2b710fd47ea53","06704d58761f470f8fd4114815110734","6a6a04d11ffb42b48427d429de14c07a","d0c6eea84fd14152bb016090d9161644","2ab1e01d4a704ee4845de03effc33642","fd2b8009c5fd4d8882ed0fffb8e80cd5","9cb44be075b041f4817244db9a7f4d7f","718a1dc627d44b3292ccac10ee19a166","e30b047a13414769a11539fd75af0b59"]},"outputId":"37f264aa-12ec-4482-81fa-0b76909bec71","executionInfo":{"status":"ok","timestamp":1670448023719,"user_tz":300,"elapsed":6765,"user":{"displayName":"Hiteshwar singh","userId":"17502675958375875233"}}},"source":["from datasets import load_dataset, load_metric\n","\n","raw_datasets = load_dataset(\"samsum\")\n","\n","metric = load_metric(\"rouge\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95e1a5850b19402993a2ffce7f057d5c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  \"\"\"\n"]}]},{"cell_type":"markdown","source":["## BART"],"metadata":{"id":"Ql3_w39HR2zi"}},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["### Preprocessing the data"]},{"cell_type":"code","metadata":{"id":"TkXftRLYwOqc"},"source":["model_checkpoint = \"facebook/bart-base\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXNLu_-nIrJI"},"source":["from transformers import AutoTokenizer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_token_length(dataset):\n","    ids=[]\n","    for i in range(len(dataset['dialogue'])):\n","        if len(tokenizer(dataset['dialogue'][i])['input_ids'])>1000:\n","            ids.append(i)\n","    print(ids)\n","    return ids\n","def remove_idx(list_idx, dataset):\n","    return dataset.select((\n","          i for i in range(len(dataset)) \n","          if i not in set(list_idx)))\n","    \n","train_ids=check_token_length(raw_datasets['train'])\n","validation_ids=check_token_length(raw_datasets['validation'])\n","test_ids = check_token_length(raw_datasets['test'])\n","changed_datasets_train=remove_idx(train_ids, raw_datasets['train'])\n","changed_datasets_val = remove_idx(validation_ids, raw_datasets['validation'])\n","changed_datasets_test = remove_idx(test_ids, raw_datasets['test'])"],"metadata":{"id":"b9hiJeaPFqEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vc0BSBLIIrJQ"},"source":["max_input_length = 1024\n","max_target_length = 128\n","\n","def make_one_hot_sequence(input_ids, sequence_ids):\n","    changed_sequence_id=[0]\n","    token_to_speaker_id={}\n","    uniq_id = 1\n","    for dic in sequence_ids:\n","        if str(input_ids[dic['spk'][0]:dic['spk'][1]]) in token_to_speaker_id:\n","            speaker_id = token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])]\n","        else:\n","            token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])] = uniq_id\n","            speaker_id = uniq_id\n","            uniq_id+=1\n","        for _ in range(dic['spk'][0], dic['spk'][1]):\n","            changed_sequence_id.append(speaker_id)\n","        for _ in range(dic['utt'][0], dic['utt'][1]):\n","            changed_sequence_id.append(-1)\n","    changed_sequence_id.append(0)\n","    return changed_sequence_id \n","\n","\n","def preprocess_function(examples): ## hit gold here. change this preprocess function to include speaker and turn information. \n","    slash_n = tokenizer([\"\\r\\n\"])['input_ids'][0][1:-1]\n","    slash_n_mask = tokenizer([\"\\r\\n\"])['attention_mask'][0][1:-1]\n","    inputs_list=[]\n","    masks_list=[]\n","    pos_list=[]\n","    for index in range(len(examples['dialogue'])):\n","        # breaking the dialogue for spk:utt info\n","        broken=[]\n","        for utt in examples['dialogue'][index].split(\"\\r\\n\"):\n","            first_ind = utt.find(':')\n","            broken.append(utt[:first_ind])\n","            broken.append(utt[first_ind:])\n","        \n","        tokenized_broken = tokenizer(broken)['input_ids']\n","        attention_broken = tokenizer(broken)['attention_mask']\n","        \n","        # adding \\r\\n tokens\n","        for i in range(1, len(tokenized_broken)-1, 2):\n","            tokenized_broken[i].insert(-1, slash_n[0])\n","            tokenized_broken[i].insert(-1, slash_n[1])\n","            attention_broken[i].insert(-1, slash_n_mask[0])\n","            attention_broken[i].insert(-1, slash_n_mask[1])\n","        joined = tokenized_broken[0]\n","\n","        # annotating for spk_utt_pos\n","        assoc_dict={}\n","        assoc_dict['spk'] = [1, len(tokenized_broken[0])-1] # the range is actually exclusive of the last index. \n","        odd_bool = True\n","        running_length = len(tokenized_broken[0])\n","        sequence_ids=[]\n","        for inner in tokenized_broken[1:]:\n","            if odd_bool==True:\n","                assoc_dict['utt']=[running_length-1, running_length+len(inner)-3]\n","                odd_bool=False\n","                sequence_ids.append(assoc_dict)\n","                assoc_dict={}\n","            else:\n","                assoc_dict['spk']=[running_length-1, running_length+len(inner)-3]\n","                odd_bool=True\n","            joined = joined[:-1]+inner[1:]\n","            running_length += (len(inner)-2)\n","        \n","        # test for CUDA assert error\n","        if(len(joined)>1024):\n","            print(\"input tokens list length greater than 1024, skipping example\", end=' ')\n","            print(\"equal to\", len(joined))\n","            print(tokenizer.decode(joined))\n","        \n","        # creating inputs list\n","        inputs_list.append(joined)\n","        pos_list.append(make_one_hot_sequence(joined, sequence_ids))\n","        \n","        # creating new mask\n","        joined_mask = attention_broken[0]\n","        for inner_attention in attention_broken[1:]:\n","            joined_mask = joined_mask[:-1]+inner_attention[1:]\n","        masks_list.append(joined_mask)\n","    \n","    # overriding normal model_inputs\n","    inputs = [doc for doc in examples[\"dialogue\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","    model_inputs['input_ids'] = inputs_list\n","    model_inputs['attention_mask'] = masks_list\n","    model_inputs['spk_utt_pos'] = pos_list\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDtsaJeVIrJT"},"source":["tokenized_datasets_train_o = changed_datasets_train.map(preprocess_function, batched=True)\n","tokenized_datasets_val_o = changed_datasets_val.map(preprocess_function, batched=True)\n","tokenized_datasets_test_o = changed_datasets_test.map(preprocess_function, batched=True)\n","\n","# tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets_train = tokenized_datasets_train_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_val = tokenized_datasets_val_o.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_test = tokenized_datasets_test_o.remove_columns(['id', 'dialogue', 'summary'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","\n","\n","class CustomTrainer(Seq2SeqTrainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # How the loss is computed by Trainer. By default, all models return the loss in the first element.\n","        # Subclass and override for custom behavior.\n","        # print(inputs)\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","\n","        # Save past state if it exists\n","        # TODO: this needs to be fixed and mselfade cleaner later.\n","\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n","                loss = self.label_smoother(outputs, labels, shift_labels=True)\n","            else:\n","                loss = self.label_smoother(outputs, labels)\n","        else:\n","            if isinstance(outputs, dict) and \"loss\" not in outputs:\n","                raise ValueError(\n","                    \"The model did not return a loss from the inputs, only the following keys: \"\n","                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n","                )\n","            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","        return (loss, outputs) if return_outputs else loss\n"],"metadata":{"id":"eXTmGGJbtFhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import PreTrainedTokenizerBase\n","from transformers.utils import PaddingStrategy\n","from transformers import DataCollatorForSeq2Seq\n","from typing import Optional, Any, Union\n","import numpy as np\n","\n","\n","class CustomCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n","    r\"\"\"\n","    Data collator that will dynamically pad the inputs received, as well as the labels.\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        model ([`PreTrainedModel`]):\n","            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n","            prepare the *decoder_input_ids*\n","            This is useful when using *label_smoothing* to avoid calculating loss twice.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n","              is provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n","              lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","        label_pad_token_id (`int`, *optional*, defaults to -100):\n","            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    model: Optional[Any] = None\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    label_pad_token_id: int = -100\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features, return_tensors=None):\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n","        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n","        # same length to return tensors.\n","        if labels is not None:\n","            max_label_length = max(len(l) for l in labels)\n","            if self.pad_to_multiple_of is not None:\n","                max_label_length = (\n","                        (max_label_length + self.pad_to_multiple_of - 1)\n","                        // self.pad_to_multiple_of\n","                        * self.pad_to_multiple_of\n","                )\n","\n","            padding_side = self.tokenizer.padding_side\n","            for feature in features:\n","                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n","                if isinstance(feature[\"labels\"], list):\n","                    feature[\"labels\"] = (\n","                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n","                    )\n","                elif padding_side == \"right\":\n","                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n","                else:\n","                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n","        # added here\n","        spk_utt_pos = [feature[\"spk_utt_pos\"] for feature in features]\n","        max_spk_utt_pos_length = max(len(l) for l in spk_utt_pos)\n","\n","        if self.pad_to_multiple_of is not None:\n","            max_spk_utt_pos_length = (\n","                    (max_spk_utt_pos_length + self.pad_to_multiple_of - 1)\n","                    // self.pad_to_multiple_of\n","                    * self.pad_to_multiple_of\n","            )\n","\n","        padding_side = self.tokenizer.padding_side\n","        for feature in features:\n","            remainder = [0] * (max_spk_utt_pos_length - len(feature[\"spk_utt_pos\"]))\n","            if isinstance(feature[\"spk_utt_pos\"], list):\n","                feature[\"spk_utt_pos\"] = (\n","                    feature[\"spk_utt_pos\"] + remainder if padding_side == \"right\" else remainder + feature[\n","                        \"spk_utt_pos\"]\n","                )\n","            elif padding_side == \"right\":\n","                feature[\"spk_utt_pos\"] = np.concatenate([feature[\"spk_utt_pos\"], remainder]).astype(np.int64)\n","            else:\n","                feature[\"spk_utt_pos\"] = np.concatenate([remainder, feature[\"spk_utt_pos\"]]).astype(np.int64)\n","\n","        features = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=return_tensors,\n","        )\n","\n","        # prepare decoder_input_ids\n","        if (\n","                labels is not None\n","                and self.model is not None\n","                and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n","        ):\n","            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n","            features[\"decoder_input_ids\"] = decoder_input_ids\n","\n","        return features\n"],"metadata":{"id":"HEZHf93mtTx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","from transformers import BartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","\n","from transformers.models.bart.modeling_bart import BartConfig\n","import torch\n","from typing import *\n","from transformers.modeling_outputs import Seq2SeqLMOutput\n","from transformers.models.bart.modeling_bart import shift_tokens_right\n","import random\n","from tqdm import tqdm\n","import gc\n","import itertools\n","\n","class BartWithSCL(BartForConditionalGeneration):\n","    def __init__(self, config: BartConfig):\n","        super().__init__(config)\n","\n","    def set_losses_list(self, SCLossesList=['token']):\n","\n","        self.SCLossesList = SCLossesList\n","    def set_scl_coeff(self, scl_coeff=1e-1):\n","        self.scl_coeff=scl_coeff\n","    def token_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Token Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        \n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        spk_utt_states[spk].append(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]])\n","            \n","            \n","            #---------- hitesh------------------------------\n","            # positive samples\n","            # L_pos = 0\n","            # L_neg = 0 \n","\n","            # sampled_spk_utt_states = []           \n","\n","            # for spk in uniq_spks:\n","            #     utts = len(spk_utt_states[spk])\n","            #     spk_utt = []\n","            #     if utts > 1:\n","            #         # ids = random.sample(list(range(len(spk_utt_states[spk]))), random.randint(1, utts))\n","            #         ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","            #         for i in ids:\n","            #           spk_utt.append(spk_utt_states[spk][i])\n","            #     sampled_spk_utt_states.append(spk_utt)\n","\n","            # for instance in sampled_spk_utt_states:\n","            #   for i in range(len(instance)):\n","            #     for j in range(len(instance)):\n","            #       mat_mul = torch.einsum('ij, kj->ik', instance[i], instance[j])\n","            #       sigm = torch.sigmoid(mat_mul)\n","            #       log = torch.log(sigm)\n","            #       L_pos += torch.sum(-1 * log)\n","            # # print(\"L_pos\", L_pos)\n","\n","            # #negative loss\n","            # for i in range(0,len(sampled_spk_utt_states)):\n","            #   instance = sampled_spk_utt_states[i]\n","\n","            #   neg_instances = sampled_spk_utt_states[:i]+sampled_spk_utt_states[i+1:]\n","            #   neg_instances = list(itertools.chain(*neg_instances))\n","            #   # neg_instances = random.choices(neg_instances,k = random.randint(1, len(neg_instances)))\n","            #   if len(neg_instances)>0:\n","            #     # print(len(neg_instances))\n","            #     # print(\"-------------------------\")\n","            #     # print(sampled_spk_utt_states)\n","            #     neg_instances = random.choices(neg_instances,k = 2)\n","            #     for i in range(len(instance)):\n","            #       for j in range(len(neg_instances)):\n","            #         mat_mul = torch.einsum('ij, kj->ik', instance[i], neg_instances[j])\n","            #         sigm = torch.sigmoid(mat_mul)\n","            #         log = torch.log(1 - sigm+1e-5)\n","            #         L_neg += torch.sum(-1 * log)\n","            #---------- hitesh------------------------------\n","            \n","            \n","            # positive samples\n","            L_pos = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk][id2])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            \n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('ij, kj->ik', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","            \n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def turn_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.sample(list(range(len(spk_utt_states[spk]))), 2)\n","                    id1 = ids[0]\n","                    id2 = ids[1]\n","                    mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk][id2])\n","                    sigm = torch.sigmoid(mat_mul)\n","                    log = torch.log(sigm)\n","                    L_pos += torch.sum(-1 * log)\n","                    # L_pos = torch.nan_to_num(L_pos, posinf = 1e10, neginf = -1e10)\n","            # print(\"L_pos\", L_pos)\n","            # negative samples\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                new_uniq_spks = uniq_spks.copy()\n","                new_uniq_spks.remove(spk)\n","\n","                spk2 = random.choice(new_uniq_spks)\n","\n","                id1 = random.randint(0, len(spk_utt_states[spk])-1)\n","                id2 = random.randint(0, len(spk_utt_states[spk2])-1)\n","\n","                mat_mul = torch.einsum('i, j->', spk_utt_states[spk][id1], spk_utt_states[spk2][id2])\n","                sigm = torch.sigmoid(mat_mul)\n","                # print(1 - sigm)\n","                # print(1 - sigm+1e-5)\n","                log = torch.log(1 - sigm+1e-5)\n","                L_neg += torch.sum(-1 * log)\n","                \n","                # L_neg = torch.nan_to_num(L_neg, posinf = 1e10, neginf = -1e10)\n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","    \n","    def global_scl(self,\n","                  last_hidden_state: torch.FloatTensor,\n","                  spk_utt_pos: torch.LongTensor,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        last_hidden_state (torch.LongTensor) of shape (batch_size, sequence_length, n_dims):\n","            Output of the last layer of the encoder.\n","        spk_utt_pos (torch.LongTensor) of shape (batch_size, sequence_length,):\n","            metadata about the speaker tokens and utterance tokens\n","        Returns:\n","        Turn Level Supervised Constrastive Loss (torch.LongTensor)\n","        \"\"\"\n","        batch_scl = 0\n","        for i in range(len(spk_utt_pos)):\n","            batch_element = spk_utt_pos[i]\n","            spk_utt_list = []\n","            spk_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            utt_dict = {'start': 0, 'end': 0, 'spk_id': 0, 'bool': False}\n","            for j in range(len(batch_element)):\n","                if batch_element[j] == 0 and j > 0:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                         'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    break\n","                if batch_element[j] > 0 and spk_dict['bool'] == False:\n","                    utt_dict['end'] = j\n","                    utt_dict['bool'] = False\n","                    if j > 1:\n","                        spk_utt_list.append({'spk': [spk_dict['start'], spk_dict['end'], spk_dict['spk_id']],\n","                                             'utt': [utt_dict['start'], utt_dict['end'], utt_dict['spk_id']]})\n","                    spk_dict['start'] = j\n","                    spk_dict['bool'] = True\n","                    spk_dict['spk_id'] = batch_element[j]\n","                    \n","\n","                if batch_element[j] < 0 and spk_dict['bool'] == True:\n","                    spk_dict['end'] = j\n","                    spk_dict['bool'] = False\n","                    utt_dict['spk_id'] = spk_dict['spk_id']\n","                    utt_dict['start'] = j\n","                    utt_dict['bool'] = True\n","            # uniq spks\n","            if spk_utt_list[0]['spk'][2]==0:\n","                continue\n","            uniq_spks = list(set([int(dic['spk'][2].cpu()) for dic in spk_utt_list]))\n","            if len(uniq_spks)==1:\n","                continue\n","            # spk_utt_states\n","            spk_utt_states = {spk: [] for spk in uniq_spks}\n","\n","            for spk in uniq_spks:\n","                for dic in spk_utt_list:\n","                    if spk == dic['utt'][2]:\n","                        mean_pool = torch.mean(last_hidden_state[i, dic['utt'][0]:dic['utt'][1]], 0)\n","                        spk_utt_states[spk].append(mean_pool)\n","\n","            # positive samples\n","            L_pos = 0\n","            L_neg = 0\n","            for spk in uniq_spks:\n","                if len(spk_utt_states[spk]) > 1:\n","                    ids = random.choice(list(range(len(spk_utt_states[spk]))))\n","                    \n","                    spk_mean_exc = torch.mean(torch.vstack([spk_utt_states[spk][temp] for temp in range(len(spk_utt_states[spk])) if temp != ids]), 0)\n","                    \n","                    pos_mat_mul = torch.einsum('i, j->', spk_utt_states[spk][ids], spk_mean_exc)\n","                    pos_sigm = torch.sigmoid(pos_mat_mul)\n","                    pos_log = torch.log(pos_sigm)\n","                    L_pos += torch.sum(-1 * pos_log)\n","\n","                    # negative sample\n","\n","                    new_uniq_spks = uniq_spks.copy()\n","                    new_uniq_spks.remove(spk)\n","                    \n","                    spk2 = random.choice(new_uniq_spks)\n","                    id_neg = random.choice(list(range(len(spk_utt_states[spk2]))))\n","                    neg_mat_mul = torch.einsum('i, j->', spk_utt_states[spk2][id_neg], spk_mean_exc)\n","                    neg_sigm = torch.sigmoid(neg_mat_mul)\n","                    neg_log = torch.log(1 - neg_sigm+1e-5)\n","                    L_neg += torch.sum(-1 * neg_log)\n","                \n","\n","            # print(\"L_neg\", L_neg)\n","            \n","            batch_scl += L_pos\n","            batch_scl += L_neg\n","\n","        batch_scl /= last_hidden_state.size(0)\n","        gc.collect()\n","        return batch_scl\n","\n","    def forward(\n","            self,\n","            input_ids: torch.LongTensor = None,\n","            attention_mask: Optional[torch.Tensor] = None,\n","            spk_utt_pos: Optional[torch.Tensor] = None, ##changed here\n","            decoder_input_ids: Optional[torch.LongTensor] = None,\n","            decoder_attention_mask: Optional[torch.LongTensor] = None,\n","            head_mask: Optional[torch.Tensor] = None,\n","            decoder_head_mask: Optional[torch.Tensor] = None,\n","            cross_attn_head_mask: Optional[torch.Tensor] = None,\n","            encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n","            past_key_values: Optional[List[torch.FloatTensor]] = None,\n","            inputs_embeds: Optional[torch.FloatTensor] = None,\n","            decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","            labels: Optional[torch.LongTensor] = None,\n","            use_cache: Optional[bool] = None,\n","            output_attentions: Optional[bool] = None,\n","            output_hidden_states: Optional[bool] = None,\n","            return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, Seq2SeqLMOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n","            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n","            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n","        Returns:\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if labels is not None:\n","            if use_cache:\n","                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n","            use_cache = False\n","            if decoder_input_ids is None and decoder_inputs_embeds is None:\n","                decoder_input_ids = shift_tokens_right(\n","                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n","                )\n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            encoder_outputs=encoder_outputs,\n","            decoder_attention_mask=decoder_attention_mask,\n","            head_mask=head_mask,\n","            decoder_head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            past_key_values=past_key_values,\n","            inputs_embeds=inputs_embeds,\n","            decoder_inputs_embeds=decoder_inputs_embeds,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        if encoder_outputs is None:\n","            encoder = self.get_encoder()\n","            # TODO: mask the speaker names from the input IDs using the speaker pos info\n","            turn_attention_mask=None\n","            token_encoder_outputs=None\n","            tog_encoder_outputs=None\n","            \n","            if 'token' in self.SCLossesList:\n","                token_encoder_outputs = encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","\n","            if 'turn' in self.SCLossesList or 'global' in self.SCLossesList:\n","                tog_attention_mask = torch.where(spk_utt_pos>0, 0, attention_mask)\n","                tog_encoder_outputs = encoder(\n","                    input_ids=input_ids,\n","                    attention_mask=tog_attention_mask,\n","                    head_mask=head_mask,\n","                    inputs_embeds=inputs_embeds,\n","                    output_attentions=output_attentions,\n","                    output_hidden_states=output_hidden_states,\n","                    return_dict=return_dict,\n","                )\n","        # if 'hidden_states' in encoder_outputs:\n","        #     print(\"encoder_outputs['last_hidden_state'].size(), encoder_outputs['hidden_states'].size()\",\n","        #     encoder_outputs['last_hidden_state'].size(), encoder_outputs['hidden_states'].size())\n","        # else:\n","        #     print(\"encoder_outputs['last_hidden_state'].size()\", encoder_outputs['last_hidden_state'].size())\n","\n","        lm_logits = self.lm_head(outputs[0])\n","        lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = torch.nn.CrossEntropyLoss()\n","            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n","        # added here\n","        sc_loss = 0\n","        if 'token' in self.SCLossesList and labels is not None:\n","            sc_loss += self.token_scl(last_hidden_state=token_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","            # print(sc_loss)\n","        if 'turn' in self.SCLossesList and labels is not None:\n","            sc_loss += self.turn_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if 'global' in self.SCLossesList and labels is not None:\n","            sc_loss += self.global_scl(last_hidden_state=tog_encoder_outputs['last_hidden_state'], spk_utt_pos=spk_utt_pos)\n","        \n","        if not return_dict:\n","            output = (lm_logits,) + outputs[1:]\n","            return ((masked_lm_loss+(self.scl_coeff*sc_loss),) + output) if masked_lm_loss is not None else output\n","        loss = None\n","        if masked_lm_loss is None:\n","            loss = None\n","        else:\n","            loss = masked_lm_loss+(self.scl_coeff*sc_loss)\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=outputs.past_key_values,\n","            decoder_hidden_states=outputs.decoder_hidden_states,\n","            decoder_attentions=outputs.decoder_attentions,\n","            cross_attentions=outputs.cross_attentions,\n","            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n","            encoder_hidden_states=outputs.encoder_hidden_states,\n","            encoder_attentions=outputs.encoder_attentions,\n","        )\n"],"metadata":{"id":"nEvSIc03tW7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc \n","gc.collect()"],"metadata":{"id":"d4MxhveUbPO6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["### Fine-tuning the model"]},{"cell_type":"code","metadata":{"id":"TlqNaB8jIrJW"},"source":["# from models import BartWithSCL\n","# from datacollator import CustomCollatorForSeq2Seq\n","# from trainer import CustomTrainer\n","\n","\n","from transformers import BartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BartWithSCL.from_pretrained(model_checkpoint)\n","model.set_losses_list(['token','turn','global'])\n","model.set_scl_coeff(0.1)"],"metadata":{"id":"Khz2QvEoBYcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bliy8zgjIrJY"},"source":["batch_size = 3\n","args = Seq2SeqTrainingArguments(\n","    \"bart-tjoin-b6c0.1\",\n","    evaluation_strategy = \"epoch\",\n","    # eval_steps=5,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    gradient_accumulation_steps=2,\n","    weight_decay=0.01,\n","    # save_total_limit=2,\n","    num_train_epochs=5,\n","    logging_steps = 10, ## added\n","    predict_with_generate=True,\n","    remove_unused_columns=False, ## added\n","    fp16=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = CustomCollatorForSeq2Seq(tokenizer, model=model)"],"metadata":{"id":"fhYv33h74na-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmvbnJ9JIrJd"},"source":["import nltk\n","import numpy as np\n","import torch\n","torch.cuda.empty_cache()\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    for i in range(0,50):\n","      # print(tokenized_datasets_val[\"dialogue\"][i])\n","      print(\"-----------\",i,\"--------------\")\n","      print(\"------>Predictions by Model\")\n","      print(decoded_preds[i])\n","      print(\"----->Predictions Original\")\n","      print(decoded_labels[i])\n","      print(\"**************************\")\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"imY1oC3SIrJf"},"source":["trainer = CustomTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets_train,\n","    eval_dataset=tokenized_datasets_val,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xixI4gdbuoe"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"-lHijpxxZYA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"5ruUnR4V7-Li"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"id":"B_TUfAgR6aJd"},"execution_count":null,"outputs":[]}]}