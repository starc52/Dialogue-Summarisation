{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ahseFGDVwY8yEmegHemMCoQC2houCqpA","timestamp":1668264569189}],"collapsed_sections":["fZNKwHmTpjxX"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"d62f75dfbe4945d59d13369ecd08b93f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f189a6cf25b2471f85c51f966be3b179","IPY_MODEL_9bd4ab01f5d84463a5a647f2b64bae83","IPY_MODEL_8025d957876041798980919e1ce8921d"],"layout":"IPY_MODEL_4275f2cf8e174e5aae7ae0763357f66c"}},"f189a6cf25b2471f85c51f966be3b179":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49729ca4d1d84f5e8f026bcbcf67c800","placeholder":"​","style":"IPY_MODEL_b80023fbe9ef4995acc3d1989d7b0310","value":"100%"}},"9bd4ab01f5d84463a5a647f2b64bae83":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f479b8ddc6754c46b874eca07b0ad7aa","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f753d98929c4d19bc7654d8e84bd387","value":3}},"8025d957876041798980919e1ce8921d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbeda8ded5bc4d20b4a0b0faefec024a","placeholder":"​","style":"IPY_MODEL_0ca012cbdeba4764b9c290504fccd648","value":" 3/3 [00:00&lt;00:00, 79.91it/s]"}},"4275f2cf8e174e5aae7ae0763357f66c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49729ca4d1d84f5e8f026bcbcf67c800":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b80023fbe9ef4995acc3d1989d7b0310":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f479b8ddc6754c46b874eca07b0ad7aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f753d98929c4d19bc7654d8e84bd387":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbeda8ded5bc4d20b4a0b0faefec024a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ca012cbdeba4764b9c290504fccd648":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ab2cfead1fa4d898855a1ce9a9d1765":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04bfc223b41b4764bf25a90eed74e8b3","IPY_MODEL_dbb8c2f47dc9492e90da6b0973112b04","IPY_MODEL_c0217d603e494d2282a3f1d3e499f333"],"layout":"IPY_MODEL_3663c390a35743cc872d9b1a486017c6"}},"04bfc223b41b4764bf25a90eed74e8b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f112e7a653d34b55975b8848c4dc1b5d","placeholder":"​","style":"IPY_MODEL_a2fff1cc65bc45dfad21bb02ca0647db","value":"100%"}},"dbb8c2f47dc9492e90da6b0973112b04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0e7d8f768c84c128a8606cf1038b8f7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4bfe61b50a84c69a6e5a925866a4ef0","value":1}},"c0217d603e494d2282a3f1d3e499f333":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_173729ca1cac4fec9b2baf6cb5177828","placeholder":"​","style":"IPY_MODEL_a2dec2c4c4bd40b48646cdc7bfa0cad7","value":" 1/1 [00:01&lt;00:00,  1.20s/ba]"}},"3663c390a35743cc872d9b1a486017c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f112e7a653d34b55975b8848c4dc1b5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2fff1cc65bc45dfad21bb02ca0647db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0e7d8f768c84c128a8606cf1038b8f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4bfe61b50a84c69a6e5a925866a4ef0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"173729ca1cac4fec9b2baf6cb5177828":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2dec2c4c4bd40b48646cdc7bfa0cad7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    \n","    print('Memory Usage:',round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"],"metadata":{"id":"lv_G9EmT6ecF","executionInfo":{"status":"ok","timestamp":1670426101909,"user_tz":300,"elapsed":654,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"0e93dd9b-5625-419d-8d1f-1ce173255b83","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","Tesla T4\n","Memory Usage: 14.8 GB\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"gMzbM4naD2g9","executionInfo":{"status":"ok","timestamp":1670426101910,"user_tz":300,"elapsed":8,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import random\n","\n","def set_random_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","set_random_seed(0)"],"metadata":{"id":"faaqh5xzw8pI","executionInfo":{"status":"ok","timestamp":1670426101911,"user_tz":300,"elapsed":8,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOsHUjgdIrIW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bed09beb-0bfe-48ce-dc30-86f41d1f61b5","executionInfo":{"status":"ok","timestamp":1670426105408,"user_tz":300,"elapsed":3504,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["! pip install datasets transformers rouge-score nltk py7zr"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.8/dist-packages (0.1.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n","Requirement already satisfied: py7zr in /usr/local/lib/python3.8/dist-packages (0.20.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.3.0)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.8/dist-packages (from py7zr) (0.2.3)\n","Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from py7zr) (0.3.1)\n","Requirement already satisfied: texttable in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.6.7)\n","Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.0.1)\n","Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.0.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from py7zr) (5.4.8)\n","Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.8/dist-packages (from py7zr) (0.15.3)\n","Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.8/dist-packages (from py7zr) (1.0.9)\n","Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from py7zr) (3.16.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"e-v6Mjk_JvG3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670426108868,"user_tz":300,"elapsed":3476,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"83b2451b-81b8-4e6a-8482-09993e0155a1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP Project with SCL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5nOd8wBL-BO","executionInfo":{"status":"ok","timestamp":1670426108868,"user_tz":300,"elapsed":32,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"0304f10d-9563-495b-b31f-a70f85de0c66"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP Project with SCL\n"]}]},{"cell_type":"code","source":["! dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5JNXxHpHdOhT","executionInfo":{"status":"ok","timestamp":1670426108869,"user_tz":300,"elapsed":27,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"746bb3ba-e77b-4120-b331-fab2319ea9de"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["BART\\ Global-SamSUM.ipynb\n","BART\\ Joint-SamSUM.ipynb\n","BART\\ TOKEN-SamSUM.ipynb\n","BART\\ Turn-SamSUM.ipynb\n","datacollator.py\n","Hiteshmodels.py\n","HiteshToken\\ NLP\\ Project\\ with\\ SCL.ipynb\n","models.py\n","myTrainer.py\n","NLP\\ Project\\ with\\ SCL.ipynb\n","__pycache__\n","t5_results\n","t5-token-b4c0.1\n","T5\\ with\\ TOKEN\\ SCL.ipynb\n","test-dialogue-summarization\n","test-dialogue-summarization-hitesh\n","test-dialogue-summarization-token-batch16\n","test-global-SCL-batch16\n","test-global-SCL-batch6\n","test-jointbatch6\n","test-tokenbatch6\n","test-turn-batch6\n","Token\\ NLP\\ Project\\ with\\ SCL.ipynb\n","trainer.py\n","Turn\\ NLP\\ Project\\ with\\ SCL\\ (1).ipynb\n","Turn\\ NLP\\ Project\\ with\\ SCL.ipynb\n"]}]},{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Fine-tuning a model on a summarization task"]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","metadata":{"id":"IreSlFmlIrIm","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["d62f75dfbe4945d59d13369ecd08b93f","f189a6cf25b2471f85c51f966be3b179","9bd4ab01f5d84463a5a647f2b64bae83","8025d957876041798980919e1ce8921d","4275f2cf8e174e5aae7ae0763357f66c","49729ca4d1d84f5e8f026bcbcf67c800","b80023fbe9ef4995acc3d1989d7b0310","f479b8ddc6754c46b874eca07b0ad7aa","0f753d98929c4d19bc7654d8e84bd387","dbeda8ded5bc4d20b4a0b0faefec024a","0ca012cbdeba4764b9c290504fccd648"]},"outputId":"20a99cda-850a-4584-f0d3-cff421fe8acc","executionInfo":{"status":"ok","timestamp":1670426112047,"user_tz":300,"elapsed":3187,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["from datasets import load_dataset, load_metric\n","\n","raw_datasets = load_dataset(\"samsum\")\n","\n","metric = load_metric(\"rouge\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d62f75dfbe4945d59d13369ecd08b93f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-e81e8f9a415c>:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"rouge\")\n"]}]},{"cell_type":"markdown","source":["## BART"],"metadata":{"id":"Ql3_w39HR2zi"}},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["### Preprocessing the data"]},{"cell_type":"code","metadata":{"id":"TkXftRLYwOqc","executionInfo":{"status":"ok","timestamp":1670426112423,"user_tz":300,"elapsed":18,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["model_checkpoint = \"/content/drive/MyDrive/NLP Project with SCL/test-global-SCL-batch16/checkpoint-6000\""],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXNLu_-nIrJI","executionInfo":{"status":"ok","timestamp":1670426115230,"user_tz":300,"elapsed":2820,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["from transformers import AutoTokenizer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"],"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def check_token_length(dataset):\n","    ids=[]\n","    for i in range(len(dataset['dialogue'])):\n","        if len(tokenizer(dataset['dialogue'][i])['input_ids'])>1000:\n","            ids.append(i)\n","    print(ids)\n","    return ids\n","def remove_idx(list_idx, dataset):\n","    return dataset.select((\n","          i for i in range(len(dataset)) \n","          if i not in set(list_idx)))\n","    \n","train_ids=check_token_length(raw_datasets['train'])\n","validation_ids=check_token_length(raw_datasets['validation'])\n","test_ids = check_token_length(raw_datasets['test'])\n","changed_datasets_train=remove_idx(train_ids, raw_datasets['train'])\n","changed_datasets_val = remove_idx(validation_ids, raw_datasets['validation'])\n","changed_datasets_test = remove_idx(test_ids, raw_datasets['test'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9hiJeaPFqEK","executionInfo":{"status":"ok","timestamp":1670426371583,"user_tz":300,"elapsed":256363,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"39232940-4ea7-4c9f-9ff1-a8098d708f47"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1081 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["[4269, 8198]\n","[]\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.fingerprint:Parameter 'indices'=<generator object remove_idx.<locals>.<genexpr> at 0x7fbffb0b4dd0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"code","metadata":{"id":"vc0BSBLIIrJQ","executionInfo":{"status":"ok","timestamp":1670426371585,"user_tz":300,"elapsed":23,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["max_input_length = 1024\n","max_target_length = 128\n","\n","def make_one_hot_sequence(input_ids, sequence_ids):\n","    changed_sequence_id=[0]\n","    token_to_speaker_id={}\n","    uniq_id = 1\n","    for dic in sequence_ids:\n","        if str(input_ids[dic['spk'][0]:dic['spk'][1]]) in token_to_speaker_id:\n","            speaker_id = token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])]\n","        else:\n","            token_to_speaker_id[str(input_ids[dic['spk'][0]:dic['spk'][1]])] = uniq_id\n","            speaker_id = uniq_id\n","            uniq_id+=1\n","        for _ in range(dic['spk'][0], dic['spk'][1]):\n","            changed_sequence_id.append(speaker_id)\n","        for _ in range(dic['utt'][0], dic['utt'][1]):\n","            changed_sequence_id.append(-1)\n","    changed_sequence_id.append(0)\n","    return changed_sequence_id \n","\n","\n","def preprocess_function(examples): ## hit gold here. change this preprocess function to include speaker and turn information. \n","    slash_n = tokenizer([\"\\r\\n\"])['input_ids'][0][1:-1]\n","    slash_n_mask = tokenizer([\"\\r\\n\"])['attention_mask'][0][1:-1]\n","    inputs_list=[]\n","    masks_list=[]\n","    pos_list=[]\n","    for index in range(len(examples['dialogue'])):\n","        # breaking the dialogue for spk:utt info\n","        broken=[]\n","        for utt in examples['dialogue'][index].split(\"\\r\\n\"):\n","            first_ind = utt.find(':')\n","            broken.append(utt[:first_ind])\n","            broken.append(utt[first_ind:])\n","        \n","        tokenized_broken = tokenizer(broken)['input_ids']\n","        attention_broken = tokenizer(broken)['attention_mask']\n","        \n","        # adding \\r\\n tokens\n","        for i in range(1, len(tokenized_broken)-1, 2):\n","            tokenized_broken[i].insert(-1, slash_n[0])\n","            tokenized_broken[i].insert(-1, slash_n[1])\n","            attention_broken[i].insert(-1, slash_n_mask[0])\n","            attention_broken[i].insert(-1, slash_n_mask[1])\n","        joined = tokenized_broken[0]\n","\n","        # annotating for spk_utt_pos\n","        assoc_dict={}\n","        assoc_dict['spk'] = [1, len(tokenized_broken[0])-1] # the range is actually exclusive of the last index. \n","        odd_bool = True\n","        running_length = len(tokenized_broken[0])\n","        sequence_ids=[]\n","        for inner in tokenized_broken[1:]:\n","            if odd_bool==True:\n","                assoc_dict['utt']=[running_length-1, running_length+len(inner)-3]\n","                odd_bool=False\n","                sequence_ids.append(assoc_dict)\n","                assoc_dict={}\n","            else:\n","                assoc_dict['spk']=[running_length-1, running_length+len(inner)-3]\n","                odd_bool=True\n","            joined = joined[:-1]+inner[1:]\n","            running_length += (len(inner)-2)\n","        \n","        # test for CUDA assert error\n","        if(len(joined)>1024):\n","            print(\"input tokens list length greater than 1024, skipping example\", end=' ')\n","            print(\"equal to\", len(joined))\n","            print(tokenizer.decode(joined))\n","        \n","        # creating inputs list\n","        inputs_list.append(joined)\n","        pos_list.append(make_one_hot_sequence(joined, sequence_ids))\n","        \n","        # creating new mask\n","        joined_mask = attention_broken[0]\n","        for inner_attention in attention_broken[1:]:\n","            joined_mask = joined_mask[:-1]+inner_attention[1:]\n","        masks_list.append(joined_mask)\n","    \n","    # overriding normal model_inputs\n","    inputs = [doc for doc in examples[\"dialogue\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","    model_inputs['input_ids'] = inputs_list\n","    model_inputs['attention_mask'] = masks_list\n","    model_inputs['spk_utt_pos'] = pos_list\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDtsaJeVIrJT","executionInfo":{"status":"ok","timestamp":1670426372880,"user_tz":300,"elapsed":1315,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["5ab2cfead1fa4d898855a1ce9a9d1765","04bfc223b41b4764bf25a90eed74e8b3","dbb8c2f47dc9492e90da6b0973112b04","c0217d603e494d2282a3f1d3e499f333","3663c390a35743cc872d9b1a486017c6","f112e7a653d34b55975b8848c4dc1b5d","a2fff1cc65bc45dfad21bb02ca0647db","c0e7d8f768c84c128a8606cf1038b8f7","b4bfe61b50a84c69a6e5a925866a4ef0","173729ca1cac4fec9b2baf6cb5177828","a2dec2c4c4bd40b48646cdc7bfa0cad7"]},"outputId":"17a912fc-41d7-4cb8-8438-b44de1d611e8"},"source":["tokenized_datasets_train = changed_datasets_train.map(preprocess_function, batched=True)\n","tokenized_datasets_val = changed_datasets_val.map(preprocess_function, batched=True)\n","tokenized_datasets_test = changed_datasets_test.map(preprocess_function, batched=True)\n","\n","# tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets_train = tokenized_datasets_train.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_val = tokenized_datasets_val.remove_columns(['id', 'dialogue', 'summary'])\n","tokenized_datasets_test = tokenized_datasets_test.remove_columns(['id', 'dialogue', 'summary'])"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-ddce6797c79a3be8.arrow\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ab2cfead1fa4d898855a1ce9a9d1765"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3578: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-153eb072cf9e73a9.arrow\n"]}]},{"cell_type":"code","source":["# tokenized_datasets_train = tokenized_datasets_train.select(range(2500))\n","# tokenized_datasets_val = tokenized_datasets_val.select(range(500))\n","# tokenized_datasets_train = tokenized_datasets_train\n","# tokenized_datasets_val = tokenized_datasets_val"],"metadata":{"id":"MGY5ubEK-CIW","executionInfo":{"status":"ok","timestamp":1670426372884,"user_tz":300,"elapsed":18,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["### Fine-tuning the model"]},{"cell_type":"code","metadata":{"id":"TlqNaB8jIrJW","executionInfo":{"status":"ok","timestamp":1670426373180,"user_tz":300,"elapsed":310,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["from models import BartWithSCL\n","from datacollator import CustomCollatorForSeq2Seq\n","from trainer import CustomTrainer\n","\n","\n","from transformers import BartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers.modeling_utils import unwrap_model\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES"],"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = BartWithSCL.from_pretrained(model_checkpoint)\n","model.set_losses_list(['global'])\n","model.set_scl_coeff()"],"metadata":{"id":"Khz2QvEoBYcH","executionInfo":{"status":"ok","timestamp":1670426376137,"user_tz":300,"elapsed":2967,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bliy8zgjIrJY","executionInfo":{"status":"ok","timestamp":1670426376139,"user_tz":300,"elapsed":27,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["batch_size = 6\n","args = Seq2SeqTrainingArguments(\n","    \"test-global-SCL-batch6\",\n","    evaluation_strategy = \"epoch\",\n","    # eval_steps=5,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    gradient_accumulation_steps=2,\n","    weight_decay=0.01,\n","    # save_total_limit=2,\n","    num_train_epochs=5,\n","    logging_steps = 10, ## added\n","    predict_with_generate=True,\n","    remove_unused_columns=False, ## added\n","    fp16=True,\n",")"],"execution_count":17,"outputs":[]},{"cell_type":"code","source":["data_collator = CustomCollatorForSeq2Seq(tokenizer, model=model)"],"metadata":{"id":"fhYv33h74na-","executionInfo":{"status":"ok","timestamp":1670426376141,"user_tz":300,"elapsed":23,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmvbnJ9JIrJd","executionInfo":{"status":"ok","timestamp":1670426376143,"user_tz":300,"elapsed":23,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["import nltk\n","import numpy as np\n","import torch\n","torch.cuda.empty_cache()\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    for i in range(0,50):\n","      # print(tokenized_datasets_val[\"dialogue\"][i])\n","      print(\"----------\",i,\"---------------\")\n","      print(\"------>Predictions by Model\")\n","      print(decoded_preds[i])\n","      print(\"----->Predictions Original\")\n","      print(decoded_labels[i])\n","      print(\"**************************\")\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"imY1oC3SIrJf","executionInfo":{"status":"ok","timestamp":1670426380086,"user_tz":300,"elapsed":3963,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"db3f5430-5a49-4c91-cbc1-b6ae267923f0"},"source":["\n","trainer = CustomTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets_train,\n","    eval_dataset=tokenized_datasets_val,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cuda_amp half precision backend\n"]}]},{"cell_type":"code","metadata":{"id":"2xixI4gdbuoe","executionInfo":{"status":"ok","timestamp":1670426380088,"user_tz":300,"elapsed":49,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eea0b739-cdaf-4f76-b845-64d236ba66cc"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["print()"],"metadata":{"id":"74MMDi56lh4Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670426380089,"user_tz":300,"elapsed":34,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"6f0f2358-e135-4245-f43b-9be1b7d6a32e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"ceEjkJM0b0FS","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1670426402782,"user_tz":300,"elapsed":4154,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}},"outputId":"064096b1-11da-46ee-c691-047167de69c1"},"source":["trainer.evaluate()"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 818\n","  Batch size = 6\n"]},{"output_type":"stream","name":"stdout","text":["spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward tensor([[ 0,  1, -1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ..., -1, -1,  0],\n","        [ 0,  1, -1,  ...,  0,  0,  0],\n","        [ 0,  1, -1,  ...,  0,  0,  0]], device='cuda:0')\n","torch.Size([6, 321, 768])\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='22' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 22/137 00:21 < 01:55, 1.00 it/s]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward tensor([[ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1, -1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ..., -1, -1,  0],\n","        [ 0,  1, -1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0]], device='cuda:0')\n","torch.Size([6, 266, 768])\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward tensor([[ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ..., -1, -1,  0]], device='cuda:0')\n","torch.Size([6, 357, 768])\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward tensor([[ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ..., -1, -1,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1, -1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0]], device='cuda:0')\n","torch.Size([6, 186, 768])\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward tensor([[ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1, -1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ...,  0,  0,  0],\n","        [ 0,  1,  1,  ..., -1, -1,  0],\n","        [ 0,  1, -1,  ...,  0,  0,  0]], device='cuda:0')\n","torch.Size([6, 322, 768])\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n","spk_utt_pos in forward None\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-f245b31d31e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     def predict(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2811\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2812\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2989\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2990\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mgeneration_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         generated_tokens = self.model.generate(\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mgeneration_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             )\n\u001b[1;32m   1607\u001b[0m             \u001b[0;31m# 12. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1609\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2797\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2799\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2800\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2801\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/NLP Project with SCL/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, spk_utt_pos, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    346\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 )\n\u001b[0;32m--> 348\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    446\u001b[0m             )\n\u001b[1;32m    447\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"uNx5pyRlIrJh","scrolled":false,"executionInfo":{"status":"aborted","timestamp":1670426395893,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"id":"B_TUfAgR6aJd","executionInfo":{"status":"aborted","timestamp":1670426395894,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## T5"],"metadata":{"id":"X23VZ_cqSQJ5"}},{"cell_type":"markdown","metadata":{"id":"EGgU3K1eSZYo"},"source":["### Preprocessing the data"]},{"cell_type":"code","metadata":{"id":"y1It_TQ_SZYo","executionInfo":{"status":"aborted","timestamp":1670426395896,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["model_checkpoint = \"t5-base\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMAHEIXVSZYo","executionInfo":{"status":"aborted","timestamp":1670426395897,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["from transformers import AutoTokenizer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhVWRDbLSZYo","executionInfo":{"status":"aborted","timestamp":1670426395898,"user_tz":300,"elapsed":30,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["max_input_length = 512\n","max_target_length = 128\n","\n","def preprocess_function(examples):\n","    task_prefix = \"summarize: \"\n","    inputs = examples[\"dialogue\"]\n","    model_inputs = tokenizer([task_prefix + dialogue for dialogue in inputs], \n","                             padding=\"max_length\",\n","                             max_length=max_input_length, \n","                             truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    labels = tokenizer(examples[\"summary\"], \n","                        padding=\"max_length\",\n","                        max_length=max_target_length, \n","                        truncation=True)\n","\n","    model_inputs[\"labels\"] = [\n","        [-100 if token == tokenizer.pad_token_id else token for token in l]\n","        for l in labels[\"input_ids\"]]\n","        \n","    return model_inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIi8rOGDSZYo","executionInfo":{"status":"aborted","timestamp":1670426395898,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"source":["tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample a small set for development\n","# tokenized_datasets_train = tokenized_datasets['train'].select(range(100))\n","# tokenized_datasets_val = tokenized_datasets['validation'].select(range(70))\n","\n","\n","tokenized_datasets_train = tokenized_datasets['train']\n","tokenized_datasets_val = tokenized_datasets['validation']"],"metadata":{"id":"jqcT42_MovSR","executionInfo":{"status":"aborted","timestamp":1670426395899,"user_tz":300,"elapsed":30,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZNKwHmTpjxX"},"source":["### Fine-tuning the model"]},{"cell_type":"code","source":["# Parameters\\\n","batch_size=8\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"t5_results\",\n","    num_train_epochs=5,\n","    do_train=True,\n","    do_eval=True,\n","    evaluation_strategy = \"epoch\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    learning_rate=1e-4,\n","    warmup_steps=500,\n","    weight_decay=0.1,\n","    # label_smoothing_factor=0.1, ## causes to throw an error\n","    predict_with_generate=True,\n","    # logging_dir=\"logs\",\n","    logging_steps=10,\n","    save_total_limit=3,\n",")\n","\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets_train,\n","    eval_dataset=tokenized_datasets_val,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"37a4Gih6lB8t","executionInfo":{"status":"aborted","timestamp":1670426395900,"user_tz":300,"elapsed":30,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate before training for comparison\n","trainer.evaluate()"],"metadata":{"id":"5RY4ZQIDrmWr","executionInfo":{"status":"aborted","timestamp":1670426395901,"user_tz":300,"elapsed":27,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","trainer.train()"],"metadata":{"id":"ntN6_k_wpga6","executionInfo":{"status":"aborted","timestamp":1670426395901,"user_tz":300,"elapsed":27,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"id":"FqF-4NrJrl0s","executionInfo":{"status":"aborted","timestamp":1670426395903,"user_tz":300,"elapsed":29,"user":{"displayName":"Sai Chakkera","userId":"14509669684842661857"}}},"execution_count":null,"outputs":[]}]}